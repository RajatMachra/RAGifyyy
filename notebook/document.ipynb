{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7b2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1275d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'page': 1, 'author': 'Rajat Machra', 'date_created': '2025-11-11'}, page_content='this is a main text context for RAG')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is a main text context for RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"page\":1,\n",
    "        \"author\":\"Rajat Machra\",\n",
    "        \"date_created\":\"2025-11-11\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6750a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fadb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text files created\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/hadoop.txt\":\n",
    "    \"\"\" When I first joined Orkut, I was happy. With Orkut, I had a new platform enabling me get \n",
    "to know the people around me, including their thoughts, their views, their purchases, \n",
    "and the places they visited. We were all gaining more knowledge than ever before and \n",
    "felt more connected to the people around us. Uploading pictures helped us share good \n",
    "ideas of places to visit. I was becoming more and more addicted to understanding \n",
    "and expressing sentiments. After a few years, I joined Facebook. And day by day, I was \n",
    "introduced to what became an infinite amount of information from all over world. Next, \n",
    "I started purchasing items online, and I liked it more than shopping offline. I could easily \n",
    "get a lot of information about products, and I could compare prices and features. And I \n",
    "wasnt the only one; millions of people were feeling the same way about the Web.\n",
    " More and more data was flooding in from every corner of the world to the Web. And \n",
    "thanks to all those inventions relate\"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"sample text files created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691ea3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/hadoop.txt'}, page_content=' When I first joined Orkut, I was happy. With Orkut, I had a new platform enabling me get \\nto know the people around me, including their thoughts, their views, their purchases, \\nand the places they visited. We were all gaining more knowledge than ever before and \\nfelt more connected to the people around us. Uploading pictures helped us share good \\nideas of places to visit. I was becoming more and more addicted to understanding \\nand expressing sentiments. After a few years, I joined Facebook. And day by day, I was \\nintroduced to what became an infinite amount of information from all over world. Next, \\nI started purchasing items online, and I liked it more than shopping offline. I could easily \\nget a lot of information about products, and I could compare prices and features. And I \\nwasnt the only one; millions of people were feeling the same way about the Web.\\n More and more data was flooding in from every corner of the world to the Web. And \\nthanks to all those inventions relate')]\n"
     ]
    }
   ],
   "source": [
    "###text loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/hadoop.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c8607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\hadoop.txt'}, page_content=' When I first joined Orkut, I was happy. With Orkut, I had a new platform enabling me get \\nto know the people around me, including their thoughts, their views, their purchases, \\nand the places they visited. We were all gaining more knowledge than ever before and \\nfelt more connected to the people around us. Uploading pictures helped us share good \\nideas of places to visit. I was becoming more and more addicted to understanding \\nand expressing sentiments. After a few years, I joined Facebook. And day by day, I was \\nintroduced to what became an infinite amount of information from all over world. Next, \\nI started purchasing items online, and I liked it more than shopping offline. I could easily \\nget a lot of information about products, and I could compare prices and features. And I \\nwasnt the only one; millions of people were feeling the same way about the Web.\\n More and more data was flooding in from every corner of the world to the Web. And \\nthanks to all those inventions relate')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###directory loader for text files\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\",###To match files\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaaf66da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 0}, page_content=''),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 1}, page_content='Building Machine Learning Powered \\nApplications \\nGoing from Idea to Product \\nEmmanuel Ameisen'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 2}, page_content='Building Machine Learning Powered \\nApplications \\nby Emmanuel Ameisen \\nCopyright © 2020 Emmanuel Ameisen. All rights reserved. \\nPrinted in the United States of America. \\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, \\nSebastopol, CA 95472. \\nO’Reilly books may be purchased for educational, business, or sales \\npromotional use. Online editions are also available for most titles \\n(http://oreilly.com). For more information, contact our \\ncorporate/institutional sales department: 800-998-9938 \\nor corporate@oreilly.com. \\n\\uf0b7 Acquisitions Editor: Jonathan Hassell \\n\\uf0b7 Development Editor: Melissa Potter \\n\\uf0b7 Production Editor: Deborah Baker \\n\\uf0b7 Copyeditor: Kim Wimpsett \\n\\uf0b7 Proofreader: Christina Edwards \\n\\uf0b7 Indexer: Judith McConville \\n\\uf0b7 Interior Designer: David Futato \\n\\uf0b7 Cover Designer: Karen Montgomery \\n\\uf0b7 Illustrator: Rebecca Demarest \\n\\uf0b7 February 2020: First Edition \\nRevision History for the First Edition \\n\\uf0b7 2020-01-17: First Release \\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492045113 for release \\ndetails. \\nThe O’Reilly logo is a registered trademark of O’Reilly Media, \\nInc. Building Machine Learning Powered Applications, the cover image, \\nand related trade dress are trademarks of O’Reilly Media, Inc. \\nThe views expressed in this work are those of the author, and do not \\nrepresent the publisher’s views. While the publisher and the author have'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 3}, page_content='used good faith efforts to ensure that the information and instructions \\ncontained in this work are accurate, the publisher and the author disclaim \\nall responsibility for errors or omissions, including without limitation \\nresponsibility for damages resulting from the use of or reliance on this \\nwork. Use of the information and instructions contained in this work is at \\nyour own risk. If any code samples or other technology this work contains \\nor describes is subject to open source licenses or the intellectual property \\nrights of others, it is your responsibility to ensure that your use thereof \\ncomplies with such licenses and/or rights. \\n978-1-492-04511-3 \\n[LSI]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 4}, page_content='Preface \\nThe Goal of Using Machine Learning Powered \\nApplications \\nOver the past decade, machine learning (ML) has increasingly been used \\nto power a variety of products such as automated support systems, \\ntranslation services, recommendation engines, fraud detection models, and \\nmany, many more. \\nSurprisingly, there aren’t many resources available to teach engineers and \\nscientists how to build such products. Many books and classes will teach \\nhow to train ML models or how to build software projects, but few blend \\nboth worlds to teach how to build practical applications that are powered \\nby ML. \\nDeploying ML as part of an application requires a blend of creativity, \\nstrong engineering practices, and an analytical mindset. ML products are \\nnotoriously challenging to build because they require much more than \\nsimply training a model on a dataset. Choosing the right ML approach for \\na given feature, analyzing model errors and data quality issues, and \\nvalidating model results to guarantee product quality are all challenging \\nproblems that are at the core of the ML building process. \\nThis book goes through every step of this process and aims to help you \\naccomplish each of them by sharing a mix of methods, code examples, and \\nadvice from me and other experienced practitioners. We’ll cover the \\npractical skills required to design, build, and deploy ML–powered \\napplications. The goal of this book is to help you succeed at every part of \\nthe ML process. \\nUse ML to Build Practical Applications \\nIf you regularly read ML papers and corporate engineering blogs, you may \\nfeel overwhelmed by the combination of linear algebra equations and \\nengineering terms. The hybrid nature of the field leads many engineers and \\nscientists who could contribute their diverse expertise to feel intimidated \\nby the field of ML. Similarly, entrepreneurs and product leaders often'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 5}, page_content='struggle to tie together their ideas for a business with what is possible with \\nML today (and what may be possible tomorrow). \\nThis book covers the lessons I have learned working on data teams at \\nmultiple companies and helping hundreds of data scientists, software \\nengineers, and product managers build applied ML projects through my \\nwork leading the artificial intelligence program at Insight Data Science. \\nThe goal of this book is to share a step-by-step practical guide to building \\nML–powered applications. It is practical and focuses on concrete tips and \\nmethods to help you prototype, iterate, and deploy models. Because it \\nspans a wide range of topics, we will go into only as much detail as is \\nneeded at each step. Whenever possible, I will provide resources to help \\nyou dive deeper into the topics covered if you so desire. \\nImportant concepts are illustrated with practical examples, including a \\ncase study that will go from idea to deployed model by the end of the \\nbook. Most examples will be accompanied by illustrations, and many will \\ncontain code. All of the code used in this book can be found in the book’s \\ncompanion GitHub repository. \\nBecause this book focuses on describing the process of ML, each chapter \\nbuilds upon concepts defined in earlier ones. For this reason, I recommend \\nreading it in order so that you can understand how each successive step fits \\ninto the entire process. If you are looking to explore a subset of the process \\nof ML, you might be better served with a more specialized book. If that is \\nthe case, I’ve shared a few recommendations. \\nAdditional Resources \\n\\uf0b7 If you’d like to know ML well enough to write your own algorithms \\nfrom scratch, I recommend Data Science from Scratch, by Joel Grus. \\nIf the theory of deep learning is what you are after, the \\ntextbook Deep Learning (MIT Press), by Ian Goodfellow, Yoshua \\nBengio, and Aaron Courville, is a comprehensive resource. \\n\\uf0b7 If you are wondering how to train models efficiently and accurately \\non specific datasets, Kaggle and fast.ai are great places to look. \\n\\uf0b7 If you’d like to learn how to build scalable applications that need to \\nprocess a lot of data, I recommend looking at Designing Data-\\nIntensive Applications (O’Reilly), by Martin Kleppmann.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 6}, page_content='If you have coding experience and some basic ML knowledge and want to \\nbuild ML–driven products, this book will guide you through the entire \\nprocess from product idea to shipped prototype. If you already work as a \\ndata scientist or ML engineer, this book will add new techniques to your \\nML development tool. If you do not know how to code but collaborate \\nwith data scientists, this book can help you understand the process of ML, \\nas long as you are willing to skip some of the in-depth code examples. \\nLet’s start by diving deeper into the meaning of practical ML. \\nPractical ML \\nFor the purpose of this introduction, think of ML as the process of \\nleveraging patterns in data to automatically tune algorithms. This is a \\ngeneral definition, so you will not be surprised to hear that many \\napplications, tools, and services are starting to integrate ML at the core of \\nthe way they function. \\nSome of these tasks are user-facing, such as search engines, \\nrecommendations on social platforms, translation services, or systems that \\nautomatically detect familiar faces in photographs, follow instructions \\nfrom voice commands, or attempt to provide useful suggestions to finish a \\nsentence in an email. \\nSome work in less visible ways, silently filtering spam emails and \\nfraudulent accounts, serving ads, predicting future usage patterns to \\nefficiently allocate resources, or experimenting with personalizing website \\nexperiences for each user. \\nMany products currently leverage ML, and even more could do so. \\nPractical ML refers to the task of identifying practical problems that could \\nbenefit from ML and delivering a successful solution to these problems. \\nGoing from a high-level product goal to ML–powered results is a \\nchallenging task that this book tries to help you to accomplish. \\nSome ML courses will teach students about ML methods by providing a \\ndataset and having them train a model on them, but training an algorithm \\non a dataset is a small part of the ML process. Compelling ML–powered \\nproducts rely on more than an aggregate accuracy score and are the results \\nof a long process. This book will start from ideation and continue all the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 7}, page_content='way through to production, illustrating every step on an example \\napplication. We will share tools, best practices, and common pitfalls \\nlearned from working with applied teams that are deploying these kinds of \\nsystems every day. \\nWhat This Book Covers \\nTo cover the topic of building applications powered by ML, the focus of \\nthis book is concrete and practical. In particular, this book aims to \\nillustrate the whole process of building ML–powered applications. \\nTo do so, I will first describe methods to tackle each step in the process. \\nThen, I will illustrate these methods using an example project as a case \\nstudy. The book also contains many practical examples of ML in industry \\nand features interviews with professionals who have built and maintained \\nproduction ML models. \\nTHE ENTIRE PROCESS OF ML \\nTo successfully serve an ML product to users, you need to do more than \\nsimply train a model. You need to thoughtfully translate your product \\nneed to an ML problem, gather adequate data, efficiently iterate in \\nbetween models, validate your results, and deploy them in a robust \\nmanner. \\nBuilding a model often represents only a tenth of the total workload of an \\nML project. Mastering the entire ML pipeline is crucial to successfully \\nbuild projects, succeed at ML interviews, and be a top contributor on ML \\nteams. \\nA TECHNICAL, PRACTICAL CASE STUDY \\nWhile we won’t be re-implementing algorithms from scratch in C, we will \\nstay practical and technical by using libraries and tools providing higher-\\nlevel abstractions. We will go through this book building an example ML \\napplication together, from the initial idea to the deployed product. \\nI will illustrate key concepts with code snippets when applicable, as well \\nas figures describing our application. The best way to learn ML is by \\npracticing it, so I encourage you to go through the book reproducing the \\nexamples and adapting them to build your own ML–powered application.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 8}, page_content='REAL BUSINESS APPLICATIONS \\nThroughout this book, I will include conversations and advice from ML \\nleaders who have worked on data teams at tech companies such as \\nStitchFix, Jawbone, and FigureEight. These discussions will cover \\npractical advice garnered after building ML applications with millions of \\nusers and will correct some popular misconceptions about what makes \\ndata scientists and data science teams successful. \\nPrerequisites \\nThis book assumes some familiarity with programming. I will mainly be \\nusing Python for technical examples and assume that the reader is familiar \\nwith the syntax. If you’d like to refresh your Python knowledge, I \\nrecommend The Hitchhiker’s Guide to Python (O’Reilly), by Kenneth \\nReitz and Tanya Schlusser. \\nIn addition, while I will define most ML concepts referred to in the book, I \\nwill not cover the inner workings of all ML algorithms used. Most of these \\nalgorithms are standard ML methods that are covered in introductory-level \\nML resources, such as the ones mentioned in “Additional Resources”. \\nOur Case Study: ML–Assisted Writing \\nTo concretely illustrate this idea, we will build an ML application together \\nas we go through this book. \\nAs a case study, I chose an application that can accurately illustrate the \\ncomplexity of iterating and deploying ML models. I also wanted to cover a \\nproduct that could produce value. This is why we will be implementing \\na machine learning–powered writing assistant. \\nOur goal is to build a system that will help users write better. In particular, \\nwe will aim to help people write better questions. This may seem like a \\nvery vague objective, and I will define it more clearly as we scope out the \\nproject, but it is a good example for a few key reasons. \\nText data is everywhere \\nText data is abundantly available for most use cases you can think of \\nand is core to many practical ML applications. Whether we are \\ntrying to better understand the reviews of our product, accurately'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 9}, page_content='categorize incoming support requests, or tailor our promotional \\nmessages to potential audiences, we will consume and produce text \\ndata. \\nWriting assistants are useful \\nFrom Gmail’s text prediction feature to Grammarly’s smart \\nspellchecker, ML–powered editors have proven that they can deliver \\nvalue to users in a variety of ways. This makes it particularly \\ninteresting for us to explore how to build them from scratch. \\nML–assisted writing is self-standing \\nMany ML applications can function only when tightly integrated \\ninto a broader ecosystem, such as ETA prediction for ride-hailing \\ncompanies, search and recommendation systems for online retailers, \\nand ad bidding models. A text editor, however, even though it could \\nbenefit from being integrated into a document editing ecosystem, \\ncan prove valuable on its own and be exposed through a simple \\nwebsite. \\nThroughout the book, this project will allow us to highlight the challenges \\nand associated solutions we suggest to build ML–powered applications. \\nThe ML Process \\nThe road from an idea to a deployed ML application is long and winding. \\nAfter seeing many companies and individuals build such projects, I’ve \\nidentified four key successive stages, which will each be covered in a \\nsection of this book. \\n1. Identifying the right ML approach: The field of ML is broad and \\noften proposes a multitude of ways to tackle a given product goal. \\nThe best approach for a given problem will depend on many factors \\nsuch as success criteria, data availability, and task complexity. The \\ngoals of this stage are to set the right success criteria and to identify \\nan adequate initial dataset and model choice. \\n2. Building an initial prototype: Start by building an end-to-end \\nprototype before working on a model. This prototype should aim to \\ntackle the product goal with no ML involved and will allow you to \\ndetermine how to best apply ML. Once a prototype is built, you \\nshould have an idea of whether you need ML, and you should be \\nable to start gathering a dataset to train a model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 10}, page_content='3. Iterating on models: Now that you have a dataset, you can train a \\nmodel and evaluate its shortcomings. The goal of this stage is to \\nrepeatedly alternate between error analysis and implementation. \\nIncreasing the speed at which this iteration loop happens is the best \\nway to increase ML development speed. \\n4. Deployment and monitoring: Once a model shows good \\nperformance, you should pick an adequate deployment option. Once \\ndeployed, models often fail in unexpected ways. The last two \\nchapters of this book will cover methods to mitigate and monitor \\nmodel errors. \\nThere is a lot of ground to cover, so let’s dive right in and start \\nwith Chapter 1! \\nConventions Used in This Book \\nThe following typographical conventions are used in this book: \\nItalic \\nIndicates new terms, URLs, email addresses, filenames, and file \\nextensions. \\nConstant width \\nUsed for program listings, as well as within paragraphs to refer to \\nprogram elements such as variable or function names, databases, \\ndata types, environment variables, statements, and keywords. \\nConstant width bold \\nShows commands or other text that should be typed literally by the \\nuser. \\nConstant width italic \\nShows text that should be replaced with user-supplied values or by \\nvalues determined by context. \\nTIP \\nThis element signifies a tip or suggestion. \\nNOTE \\nThis element signifies a general note.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 11}, page_content='WARNING \\nThis element indicates a warning or caution. \\nUsing Code Examples \\nSupplemental code examples for this book are available for download \\nat https://oreil.ly/ml-powered-applications. \\nIf you have a technical question or a problem using the code examples, \\nplease send email to bookquestions@oreilly.com. \\nThis book is here to help you get your job done. In general, if example \\ncode is offered with this book, you may use it in your programs and \\ndocumentation. You do not need to contact us for permission unless you’re \\nreproducing a significant portion of the code. For example, writing a \\nprogram that uses several chunks of code from this book does not require \\npermission. Selling or distributing examples from O’Reilly books does \\nrequire permission. Answering a question by citing this book and quoting \\nexample code does not require permission. Incorporating a significant \\namount of example code from this book into your product’s \\ndocumentation does require permission. \\nWe appreciate, but generally do not require, attribution. An attribution \\nusually includes the title, author, publisher, and ISBN. For \\nexample: Building Machine Learning Powered Applications by Emmanuel \\nAmeisen (O’Reilly). Copyright 2020 Emmanuel Ameisen, 978-1-492-\\n04511-3.” \\nIf you feel your use of code examples falls outside fair use or the \\npermission given here, feel free to contact us at permissions@oreilly.com. \\nO’Reilly Online Learning \\nNOTE \\nFor more than 40 years, O’Reilly Media has provided technology and \\nbusiness training, knowledge, and insight to help companies succeed.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 12}, page_content='Our unique network of experts and innovators share their knowledge and \\nexpertise through books, articles, conferences, and our online learning \\nplatform. O’Reilly’s online learning platform gives you on-demand access \\nto live training courses, in-depth learning paths, interactive coding \\nenvironments, and a vast collection of text and video from O’Reilly and \\n200+ other publishers. For more information, please \\nvisit http://oreilly.com. \\nHow to Contact Us \\nPlease address comments and questions concerning this book to the \\npublisher: \\n\\uf0b7 O’Reilly Media, Inc. \\n\\uf0b7 1005 Gravenstein Highway North \\n\\uf0b7 Sebastopol, CA 95472 \\n\\uf0b7 800-998-9938 (in the United States or Canada) \\n\\uf0b7 707-829-0515 (international or local) \\n\\uf0b7 707-829-0104 (fax) \\nYou can access the web page for this book, where we list errata, examples, \\nand any additional information, \\nat https://oreil.ly/Building_ML_Powered_Applications. \\nEmail bookquestions@oreilly.com to comment or ask technical questions \\nabout this book. \\nFor more information about our books, courses, conferences, and news, \\nsee our website at http://www.oreilly.com. \\nFind us on Facebook: http://facebook.com/oreilly \\nFollow us on Twitter: http://twitter.com/oreillymedia \\nWatch us on YouTube: http://www.youtube.com/oreillymedia \\nAcknowledgments'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 13}, page_content='The project of writing this book started as a consequence of my work \\nmentoring Fellows and overseeing ML projects at Insight Data Science. \\nFor giving me the opportunity to lead this program and for encouraging \\nme to write about the lessons learned doing so, I’d like to thank Jake \\nKlamka and Jeremy Karnowski, respectively. I’d also like to thank the \\nhundreds of Fellows I’ve worked with at Insight for allowing me to help \\nthem push the limits of what an ML project can look like. \\nWriting a book is a daunting task, and the O’Reilly staff helped make it \\nmore manageable every step of the way. In particular, I would like to \\nthank my editor, Melissa Potter, who tirelessly provided guidance, \\nsuggestions, and moral support throughout the journey that is writing a \\nbook. Thank you to Mike Loukides for somehow convincing me that \\nwriting a book was a reasonable endeavor. \\nThank you to the tech reviewers who combed through early drafts of this \\nbook, pointing out errors and offering suggestions for improvement. Thank \\nyou Alex Gude, Jon Krohn, Kristen McIntyre, and Douwe Osinga for \\ntaking the time out of your busy schedules to help make this book the best \\nversion of itself that it could be. To data practitioners whom I asked about \\nthe challenges of practical ML they felt needed the most attention, thank \\nyou for your time and insights, and I hope you’ll find that this book covers \\nthem adequately. \\nFinally, for their unwavering support during the series of busy weekends \\nand late nights that came with writing this book, I’d like to thank my \\nunwavering partner Mari, my sarcastic sidekick Eliott, my wise and patient \\nfamily, and my friends who refrained from reporting me as missing. You \\nmade this book a reality.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 14}, page_content='Part I. Find the Correct ML Approach \\nMost individuals or companies have a good grasp of which problems they \\nare interested in solving—for example, predicting which customers will \\nleave an online platform or building a drone that will follow a user as they \\nski down a mountain. Similarly, most people can quickly learn how to \\ntrain a model to classify customers or detect objects to reasonable accuracy \\ngiven a dataset. \\nWhat is much rarer, however, is the ability to take a problem, estimate \\nhow best to solve it, build a plan to tackle it with ML, and confidently \\nexecute on said plan. This is often a skill that has to be learned through \\nexperience, after multiple overly ambitious projects and missed deadlines. \\nFor a given product, there are many potential ML solutions. In Figure I-1, \\nyou can see a mock-up of a potential writing assistant tool on the left, \\nwhich includes a suggestion and an opportunity for the user to provide \\nfeedback. On the right of the image is a diagram of a potential ML \\napproach to provide such recommendations. \\n \\nFigure I-1. From product to ML \\nThis section starts by covering these different potential approaches, as well \\nas methods to choose one over the others. It then dives into methods to \\nreconcile a model’s performance metrics with product requirements. \\nTo do this, we will tackle two successive topics:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 15}, page_content='Chapter 1 \\nBy the end of this chapter, you will be able to take an idea for an \\napplication, estimate whether it is possible to solve, determine \\nwhether you would need ML to do so, and figure out which kind of \\nmodel would make the most sense to start with. \\nChapter 2 \\nIn this chapter, we will cover how to accurately evaluate your \\nmodel’s performance within the context of your application’s goals \\nand how to use this measure to make regular progress.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 16}, page_content='Chapter 1. From Product Goal to ML \\nFraming \\nML allows machines to learn from data and behave in a probabilistic way \\nto solve problems by optimizing for a given objective. This stands in \\nopposition to traditional programming, in which a programmer writes step-\\nby-step instructions describing how to solve a problem. This makes ML \\nparticularly useful to build systems for which we are unable to define a \\nheuristic solution. \\nFigure 1-1 describes two ways to write a system to detect cats. On the left, \\na program consists of a procedure that has been manually written out. On \\nthe right, an ML approach leverages a dataset of photos of cats and dogs \\nlabeled with the corresponding animal to allow a model to learn the \\nmapping from image to category. In the ML approach, there is no \\nspecification of how the result should be achieved, only a set of example \\ninputs and outputs. \\n \\nFigure 1-1. From defining procedures to showing examples \\nML is powerful and can unlock entirely new products, but since it is based \\non pattern recognition, it introduces a level of uncertainty. It is important \\nto identify which parts of a product would benefit from ML and how to \\nframe a learning goal in a way that minimizes the risks of users having a \\npoor experience. \\nFor example, it is close to impossible (and extremely time-consuming to \\nattempt) for humans to write step-by-step instructions to automatically \\ndetect which animal is in an image based on pixel values. By feeding'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 17}, page_content='thousands of images of different animals to a convolutional neural network \\n(CNN), however, we can build a model that performs this classification \\nmore accurately than a human. This makes it an attractive task to tackle \\nwith ML. \\nOn the other side, an application that calculates your taxes automatically \\nshould rely on guidelines provided by the government. As you may have \\nheard, having errors on your tax return is generally frowned upon. This \\nmakes the use of ML for automatically generating tax returns a dubious \\nproposition. \\nYou never want to use ML when you can solve your problem with a \\nmanageable set of deterministic rules. By manageable, I mean a set of \\nrules that you could confidently write and that would not be too complex \\nto maintain. \\nSo while ML opens up a world of different applications, it is important to \\nthink about which tasks can and should be solved by ML. When building \\nproducts, you should start from a concrete business problem, determine \\nwhether it requires ML, and then work on finding the ML approach that \\nwill allow you to iterate as rapidly as possible. \\nWe will cover this process in this chapter, starting with methods to \\nestimate what tasks are able to be solved by ML, which ML approaches \\nare appropriate for which product goals, and how to approach data \\nrequirements. I will illustrate these methods with the ML Editor case study \\nthat we mentioned in “Our Case Study: ML–Assisted Writing”, and an \\ninterview with Monica Rogati. \\nEstimate What Is Possible \\nSince ML models can tackle tasks without humans needing to give them \\nstep-by-step instructions, that means they are able to perform some tasks \\nbetter than human experts (such as detecting tumors from radiology \\nimages or playing Go) and some that are entirely inaccessible to humans \\n(such as recommending articles out of a pool of millions or changing the \\nvoice of a speaker to sound like someone else). \\nThe ability of ML to learn directly from data makes it useful in a broad \\nrange of applications but makes it harder for humans to accurately'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 18}, page_content='distinguish which problems are solvable by ML. For each successful result \\npublished in a research paper or a corporate blog, there are hundreds of \\nreasonable-sounding ideas that have entirely failed. \\nWhile there is currently no surefire way to predict ML success, there are \\nguidelines that can help you reduce the risk associated with tackling an \\nML project. Most importantly, you should always start with a product goal \\nto then decide how best to solve it. At this stage, be open to any approach \\nwhether it requires ML or not. When considering ML approaches, make \\nsure to evaluate those approaches based on how appropriate they are for \\nthe product, not simply on how interesting the methods are in a vacuum. \\nThe best way to do this is by following two successive steps: (1) framing \\nyour product goal in an ML paradigm, and (2) evaluating the feasibility of \\nthat ML task. Depending on your evaluation, you can readjust your \\nframing until we are satisfied. Let’s explore what these steps really mean. \\n1. Framing a product goal in an ML paradigm: When we build a \\nproduct, we start by thinking of what service we want to deliver to \\nusers. As we mentioned in the introduction, we’ll illustrate concepts \\nin this book using the case study of an editor that helps users write \\nbetter questions. The goal of this product is clear: we want users to \\nreceive actionable and useful advice on the content they write. ML \\nproblems, however, are framed in an entirely different way. An ML \\nproblem concerns itself with learning a function from data. An \\nexample is learning to take in a sentence in one language and output \\nit in another. For one product goal, there are usually many different \\nML formulations, with varying levels of implementation difficulty. \\n2. Evaluating ML feasibility: All ML problems are not created equal! \\nAs our understanding of ML has evolved, problems such as building \\na model to correctly classify photos of cats and dogs have become \\nsolvable in a matter of hours, while others, such as creating a system \\ncapable of carrying out a conversation, remain open research \\nproblems. To efficiently build ML applications, it is important to \\nconsider multiple potential ML framings and start with the ones we \\njudge as the simplest. One of the best ways to evaluate the difficulty \\nof an ML problem is by looking at both the kind of data it requires \\nand at the existing models that could leverage said data. \\nTo suggest different framings and evaluate their feasibility, we should \\nexamine two core aspects of an ML problem: data and models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 19}, page_content='We will start with models. \\nModels \\nThere are many commonly used models in ML, and we will abstain from \\ngiving an overview of all of them here. Feel free to refer to the books \\nlisted in “Additional Resources” for a more thorough overview. In addition \\nto common models, many model variations, novel architectures, and \\noptimization strategies are published on a weekly basis. In May 2019 \\nalone, more than 13,000 papers were submitted to ArXiv, a popular \\nelectronic archive of research where papers about new models are \\nfrequently submitted. \\nIt is useful, however, to share an overview of different categories of \\nmodels and how they can be applied to different problems. To this end, I \\npropose here a simple taxonomy of models based on how they approach a \\nproblem. You can use it as a guide for selecting an approach to tackle a \\nparticular ML problem. Because models and data are closely coupled in \\nML, you will notice some overlap between this section and “Data types”. \\nML algorithms can be categorized based on whether they require labels. \\nHere, a label refers to the presence in the data of an ideal output that a \\nmodel should produce for a given example. Supervised \\nalgorithms leverage datasets that contain labels for inputs, and they aim to \\nlearn a mapping from inputs to labels. Unsupervised algorithms, on the \\nother hand, do not require labels. Finally, weakly supervised \\nalgorithms leverage labels that aren’t exactly the desired output but that \\nresemble it in some way. \\nMany product goals can be tackled by both supervised and unsupervised \\nalgorithms. A fraud detection system can be built by training a model to \\ndetect transactions that differ from the average one, requiring no labels. \\nSuch a system could also be built by manually labeling transactions as \\nfraudulent or legitimate, and training a model to learn from said labels. \\nFor most applications, supervised approaches are easier to validate since \\nwe have access to labels to assess the quality of a model’s prediction. This \\nalso makes it easier to train models since we have access to desired \\noutputs. While creating a labeled dataset can sometimes be time-\\nconsuming initially, it makes it much easier to build and validate models. \\nFor this reason, this book will mostly cover supervised approaches.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 20}, page_content='With that being said, determining which kind of inputs your model will \\ntake in and which outputs it will produce will help you narrow down \\npotential approaches significantly. Based on these types, any of the \\nfollowing categories of ML approaches could be a good fit: \\n\\uf0b7 Classification and regression \\n\\uf0b7 Knowledge extraction \\n\\uf0b7 Catalog organization \\n\\uf0b7 Generative models \\nI’ll expand on these further in the following section. As we explore these \\ndifferent modeling approaches, I recommend thinking about which kind of \\ndata you have available to you or could gather. Oftentimes, data \\navailability ends up being the limiting factor in model selection. \\nCLASSIFICATION AND REGRESSION \\nSome projects are focused on effectively classifying data points between \\ntwo or more categories or attributing them a value on a continuous scale \\n(referred to as regression instead of classification). Regression and \\nclassification are technically different, but oftentimes methods to tackle \\nthem have significant overlap, so we lump them together here. \\nOne of the reasons classification and regression are similar is because most \\nclassification models output a probability score for a model to belong to a \\ncategory. The classification aspect then boils down to deciding how to \\nattribute an object to a category based on said scores. At a high level, a \\nclassification model can thus be seen as a regression on probability values. \\nCommonly, we classify or score individual examples, such as spam \\nfilters that classify each email as valid or junk, fraud detection systems that \\nclassify users as fraudulent or legitimate, or computer vision radiology \\nmodels that classify bones as fractured or healthy. \\nIn Figure 1-2, you can see an example of classifying a sentence according \\nto its sentiment, and the topic it covers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 21}, page_content='Figure 1-2. Classifying a sentence in multiple categories \\nIn regression projects, instead of attributing a class to each example, we \\ngive them a value. Predicting the sale price of a home based on attributes \\nsuch as how many rooms it has and where it is located is an example of a \\nregression problem. \\nIn some cases, we have access to a series of past data points (instead of \\none) to predict an event in the future. This type of data is often \\ncalled a time series, and making predictions from a series of data points is \\nreferred to as forecasting. Time-series data could represent a patient’s \\nmedical history or a series of attendance measurements from national \\nparks. These projects often benefit from models and features that can \\nleverage this added temporal dimension. \\nIn other cases, we attempt to detect unusual events from a dataset. This is \\ncalled anomaly detection. When a classification problem is trying to detect \\nevents that represent a small minority of the data and thus are hard to \\ndetect accurately, a different set of methods is often required. Picking a \\nneedle out of a haystack is a good analogy here. \\nGood classification and regression work most often requires significant \\nfeature selection and feature engineering work. Feature selection consists \\nof identifying a subset of features that have the most predictive value. \\nFeature generation is the task of identifying and generating good \\npredictors of a target by modifying and combining existing features of a \\ndataset. We will cover both of these topics in more depth in Part III. \\nRecently, deep learning has shown a promising ability to automatically \\ngenerate useful features from images, text, and audio. In the future, it may'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 22}, page_content='play a larger part in simplifying feature generation and selection, but for \\nnow, they remain integral parts of the ML workflow. \\nFinally, we can often build on top of the classification or score described \\nearlier to provide useful advice. This requires building an interpretable \\nclassification model and using its feature to generate actionable advice. \\nMore on this later! \\nNot all problems aim to attribute a set of categories or values to an \\nexample. In some cases, we’d like to operate at a more granular level and \\nextract information from parts of an input, such as knowing where an \\nobject is in a picture, for example. \\nKNOWLEDGE EXTRACTION FROM UNSTRUCTURED \\nDATA \\nStructured data is data that is stored in a tabular format. Database tables \\nand Excel sheets are good examples of structured data. Unstructured \\ndata refers to datasets that are not in a tabular format. This includes text \\n(from articles, reviews, Wikipedia, and so on), music, videos, and songs. \\nIn Figure 1-3, you can see an example of structured data on the left and \\nunstructured data on the right. Knowledge extraction models focus on \\ntaking a source of unstructured data and extracting structure out of it using \\nML. \\nIn the case of text, knowledge extraction can be used to add structure to \\nreviews, for example. A model can be trained to extract aspects such as \\ncleanliness, service quality, and price from reviews. Users could then \\neasily access reviews that mention topics they are interested in.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 23}, page_content='Figure 1-3. Example types of structured and unstructured data \\nIn the medical domain, a knowledge extraction model could be built to \\ntake raw text from medical papers as input, and extract information such as \\nthe disease that is discussed in the paper, as well as the associated \\ndiagnosis and its performance. In Figure 1-4, a model takes a sentence as \\nan input and extracts which words refer to a type of media and which \\nwords refer to the title of a media. Using such a model on comments in a \\nfan forum, for example, would allow us to generate summaries of which \\nmovies frequently get discussed. \\n \\nFigure 1-4. Extracting media type and title from a sentence \\nFor images, knowledge extraction tasks often consist of finding areas of \\ninterest in an image and categorizing them. Two common approaches are \\ndepicted in Figure 1-5: object detection is a coarser approach that consists \\nof drawing rectangles (referred to as bounding boxes) around areas of \\ninterest, while segmentation precisely attributes each pixel of an image to \\na given category.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 24}, page_content='Figure 1-5. Bounding boxes and segmentation masks \\nSometimes, this extracted information can be used as an input to another \\nmodel. An example is using a pose detection model to extract key points \\nfrom a video of a yogi, and feeding those key points to a second model \\nthat classifies the pose as correct or not based on labeled data. Figure 1-\\n6 shows an example of a series of two models that could do just this. The \\nfirst model extracts structured information (the coordinates of joints) from \\nunstructured data (a photo), and the second one takes these coordinates \\nand classifies them as a yoga pose. \\n \\nFigure 1-6. Yoga pose detection \\nThe models we’ve seen so far focus on generating outputs conditioned on \\na given input. In some cases such as search engines or recommendation \\nsystems, the product goal is about surfacing relevant items. This is what \\nwe will cover in the following category.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 25}, page_content='CATALOG ORGANIZATION \\nCatalog organization models most often produce a set of results to present \\nto users. These results can be conditioned on an input string typed into a \\nsearch bar, an uploaded image, or a phrase spoken to a home assistant. In \\nmany cases such as streaming services, this set of results can also be \\nproactively presented to the user as content they may like without them \\nmaking a request at all. \\nFigure 1-7 shows an example of such a system that volunteers potential \\ncandidate movies to watch based on a movie the user just viewed, but \\nwithout having the user perform any form of search. \\n \\nFigure 1-7. Movie recommendations \\nThese models thus either recommend items that are related to an item the \\nuser already expressed interest in (similar Medium articles or Amazon \\nproducts) or provide a useful way to search through a catalog (allowing \\nusers to search for items by typing text or submitting their own photos). \\nThese recommendations are most often based on learning from previous \\nuser patterns, in which case they are called collaborative recommendation \\nsystems. Sometimes, they are based on particular attributes of items, in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 26}, page_content='which case they are called content-based recommendation systems. Some \\nsystems leverage both collaborative and content-based approaches. \\nFinally, ML can also be used for creative purposes. Models can learn to \\ngenerate aesthetically pleasing images, audio, and even amusing text. Such \\nmodels are referred to as generative models. \\nGENERATIVE MODELS \\nGenerative models focus on generating data, potentially dependent on user \\ninput. Because these models focus on generating data rather than \\nclassifying it in categories, scoring it, extracting information from it, or \\norganizing it, they usually have a wide range of outputs. This means that \\ngenerative models are uniquely fit for tasks such as translation, where \\noutputs are immensely varied. \\nOn the other side, generative models are often used to train and have \\noutputs that are less constrained, making them a riskier choice for \\nproduction. For that reason, unless they are necessary to attain your goal, I \\nrecommend starting with other models first. For readers who would like to \\ndive deeper into generative models, however, I recommend the \\nbook Generative Deep Learning, by David Foster. \\nPractical examples include translation, which maps sentences in one \\nlanguage to another; summarization; subtitle generation, which maps \\nvideos and audio tracks to transcripts; and neural style transfer (see Gatys \\net al., “A Neural Algorithm of Artistic Style”), which maps images to \\nstylized renditions. \\nFigure 1-8 shows an example of a generative model transforming a \\nphotograph on the left by giving it a style similar to a painting shown in \\nthe vignette on the right side.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 27}, page_content='Figure 1-8. Style transfer example from Gatys et al., “A Neural Algorithm of Artistic Style” \\nAs you can tell by now, each type of model requires a different type of \\ndata to be trained on. Commonly, a choice of a model will strongly depend \\non the data you are able to obtain—data availability often drives model \\nselection. \\nLet’s cover a few common data scenarios and associated models. \\nData \\nSupervised ML models leverage patterns in data to learn useful mappings \\nbetween inputs and outputs. If a dataset contains features that are \\npredictive of the target output, it should be possible for an appropriate \\nmodel to learn from it. Most often, however, we do not initially have the \\nright data to train a model to solve a product use case from end-to-end. \\nFor example, say we are training a speech recognition system that will \\nlisten for requests from customers, understand their intent, and perform \\nactions depending on said intent. When we start working on this project, \\nwe may define a set of intents we would want to understand, such as \\n“playing a movie on the television.” \\nTo train an ML model to accomplish this task, we would need to have a \\ndataset containing audio clips of users of diverse backgrounds asking in \\ntheir own terms for the system to play a movie. Having a representative set \\nof inputs is crucial, as any model will only be able to learn from the data \\nthat we present to it. If a dataset contains examples from only a subset of \\nthe population, a product will be useful to only that subset. With that in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 28}, page_content='mind, because of the specialized domain we have selected, it is extremely \\nunlikely that a dataset of such examples already exists. \\nFor most applications we would want to tackle, we will need to search for, \\ncurate, and collect additional data. The data acquisition process can vary \\nwidely in scope and complexity depending on the specifics of a project, \\nand estimating the challenge ahead of time is crucial in order to succeed. \\nTo start, let’s define a few different situations you can find yourself in \\nwhen searching for a dataset. This initial situation should be a key factor in \\ndeciding how to proceed. \\nDATA TYPES \\nOnce we’ve defined a problem as mapping inputs to outputs, we can \\nsearch for data sources that follow this mapping. \\nFor fraud detection, these could be examples of fraudulent and innocent \\nusers, along with features of their account that we could use to predict \\ntheir behavior. For translation, this would be a corpus of sentence pairs in \\nthe source and target domains. For content organization and search, this \\ncould be a history of past searches and clicks. \\nWe will rarely be able to find the exact mapping we are looking for. For \\nthis reason, it is useful to consider a few different cases. Think of this as a \\nhierarchy of needs for data. \\nDATA AVAILABILITY \\nThere are roughly three levels of data availability, from best-case scenario \\nto most challenging. Unfortunately, as with most other tasks, you can \\ngenerally assume that the most useful type of data will be the hardest to \\nfind. Let’s go through them. \\nLabeled data exists \\nThis is the leftmost category in Figure 1-9. When working on a \\nsupervised model, finding a labeled dataset is every practitioner’s \\ndream. Labeled here means that many data points contain the target \\nvalue that the model is trying to predict. This makes training and \\njudging model quality much easier, as labels provide ground truth \\nanswers. Finding a labeled dataset that fits your needs and is freely'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 29}, page_content='available on the web is rare in practice. It is common, however, to \\nmistake the dataset that you find for the dataset that you need. \\nWeakly labeled data exists \\nThis is the middle category in Figure 1-9. Some datasets contain \\nlabels that are not exactly a modeling target, but somewhat \\ncorrelated with it. Playback and skip history for a music streaming \\nservice are examples of a weakly labeled dataset for predicting \\nwhether a user dislikes a song. While a listener may have not marked \\na song as disliked, if they skipped it as it was playing, it is an \\nindication that they may have not been fond of it. Weak labels are \\nless precise by definition but often easier to find than perfect labels. \\nUnlabeled data exists \\nThis category is on the right side of Figure 1-9. In some cases, while \\nwe do not have a labeled dataset mapping desired inputs to outputs, \\nwe at least have access to a dataset containing relevant examples. \\nFor the text translation example, we might have access to large \\ncollections of text in both languages, but with no direct mapping \\nbetween them. This means we need to label the dataset, find a model \\nthat can learn from unlabeled data, or do a little bit of both. \\nWe need to acquire data \\nIn some cases, we are one step away from unlabeled data, as we \\nneed to first acquire it. In many cases, we do not have a dataset for \\nwhat we need and thus will need to find a way to acquire such data. \\nThis is often seen as an insurmountable task, but many methods now \\nexist to rapidly gather and label data. This will be the focus \\nof Chapter 4. \\nFor our case study, an ideal dataset would be a set of user-typed questions, \\nalong with a set of better worded questions. A weakly labeled dataset \\nwould be a dataset of many questions with some weak labels indicative of \\ntheir quality such as “likes” or “upvotes.” This would help a model learn \\nwhat makes for good and bad questions but would not provide side-by-side \\nexamples for the same question. You can see both of these examples \\nin Figure 1-9.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 30}, page_content='Figure 1-9. Data availability versus data usefulness \\nIn general in ML, a weakly labeled dataset refers to a dataset that contains \\ninformation that will help a model learn, but not the exact ground truth. In \\npractice, most datasets that we can gather are weakly labeled. \\nHaving an imperfect dataset is entirely fine and shouldn’t stop you. The \\nML process is iterative in nature, so starting with a dataset and getting \\nsome initial results is the best way forward, regardless of the data quality. \\nDATASETS ARE ITERATIVE \\nIn many cases, since you will not be able to immediately find a dataset \\ncontaining a direct mapping from inputs to your desired output, I suggest \\nprogressively iterating on the way you formulate the problem, making it \\neasier to find an adequate dataset to start with. Each dataset you explore \\nand use will provide you with valuable information that you can use to \\ncurate the next version of your dataset and generate useful features for \\nyour models. \\nLet’s now dive into the case study and see how we can use what we’ve \\nlearned to identify different models and datasets we could use, and choose \\nthe most appropriate. \\nFraming the ML Editor \\nLet’s see how we could iterate through a product use case to find the right \\nML framing. We’ll get through this process by outlining a method to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 31}, page_content='progress from a product goal (helping users write better questions) to an \\nML paradigm. \\nWe would like to build an editor that accepts questions by users and \\nimproves them to be better written, but what does “better” mean in this \\ncase? Let’s start by defining the writing assistant’s product goal a little \\nmore clearly. \\nMany people use forums, social networks, and websites such as Stack \\nOverflow to find answers to their questions. However, the way that people \\nask questions has a dramatic impact on whether they receive a useful \\nanswer. This is unfortunate both for the user looking to get their question \\nanswered and for future users that may have the same problem and could \\nhave found an existing answer useful. To that end, our goal will be to build \\nan assistant that can help users write better questions. \\nWe have a product goal and now need to decide which modeling approach \\nto use. To make this decision, we will go through the iteration loop of \\nmodel selection and data validation mentioned earlier. \\nTrying to Do It All with ML: An End-to-End Framework \\nIn this context, end-to-end means using a single model to go from input to \\noutput with no intermediary steps. Since most product goals are very \\nspecific, attempting to solve an entire use case by learning it from end-to-\\nend often requires custom-built cutting-edge ML models. This may be the \\nright solution for teams that have the resources to develop and maintain \\nsuch models, but it is often worth it to start with more well-understood \\nmodels first. \\nIn our case, we could attempt to gather a dataset of poorly formulated \\nquestions, as well as their professionally edited versions. We could then \\nuse a generative model to go straight from one text to the other. \\nFigure 1-10 depicts what this would look like in practice. It shows a \\nsimple diagram with user input on the left, the desired output on the right, \\nand a model in between.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 32}, page_content='Figure 1-10. End-to-end approach \\nAs you’ll see, this approach comes with significant challenges: \\nData \\nTo acquire such a dataset, we would need to find pairs of questions \\nwith the same intent but of different wording quality. This is quite a \\nrare dataset to find as is. Building it ourselves would be costly as \\nwell, as we would need to be assisted by professional editors to \\ngenerate this data. \\nModel \\nModels going from one sequence of text to another, seen in the \\ngenerative models category discussed earlier, have progressed \\ntremendously in recent years. Sequence-to-sequence models (as \\ndescribed in the paper by I. Sutskever et al., “Sequence to Sequence \\nLearning with Neural Networks”) were originally proposed in 2014 \\nfor translation tasks and are closing the gap between machine and \\nhuman translation. The success of these models, however, has \\nmostly been on sentence-level tasks, and they have not been \\nfrequently used to process text longer than a paragraph. This is \\nbecause so far, they have not been able to capture long-term context \\nfrom one paragraph to another. Additionally, because they usually \\nhave a large number of parameters, they are some of the slowest \\nmodels to train. If a model is trained only once, this is not \\nnecessarily an issue. If it needs to be retrained hourly or daily, \\ntraining time can become an important factor. \\nLatency \\nSequence-to-sequence models are often autoregressive models, \\nmeaning they require the model’s output of the previous word to \\nstart working on the next. This allows them to leverage information \\nfrom neighboring words but causes them to be slower to train and \\nslower to run inference on than the simpler models. Such models can \\ntake a few seconds to produce an answer at inference time, as \\nopposed to subsecond latency for simpler models. While it is \\npossible to optimize such a model to run quickly enough, it will \\nrequire additional engineering work. \\nEase of implementation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 33}, page_content='Training complex end-to-end models is a very delicate and error-\\nprone process, as they have many moving parts. This means that we \\nneed to consider the tradeoff between a model’s potential \\nperformance and the complexity it adds to a pipeline. This \\ncomplexity will slow us down when building a pipeline, but it also \\nintroduces a maintenance burden. If we anticipate that other \\nteammates may need to iterate on and improve on your model, it \\nmay be worthwhile to choose a set of simpler, more well-understood \\nmodels. \\nThis end-to-end approach could work, but it will require a lot of upfront \\ndata gathering and engineering effort, with no success guarantee, so it \\nwould be worthwhile to explore other alternatives, as we will cover next. \\nThe Simplest Approach: Being the Algorithm \\nAs you’ll see in the interview at the end of this section, it is often a great \\nidea for data scientists to be the algorithm before they implement it. In \\nother words, to understand how to best automate a problem, start by \\nattempting to solve it manually. So, if we were editing questions ourselves \\nto improve readability and the odds of getting an answer, how would we \\ngo about it? \\nA first approach would be to not use data at all but leverage prior art to \\ndefine what makes a question or a body of text well written. For general \\nwriting tips, we could reach out to a professional editor or research \\nnewspapers’ style guides to learn more. \\nIn addition, we should dive into a dataset to look at individual examples \\nand trends and let those inform our modeling strategy. We will skip this \\nfor now as we will cover how to do this in more depth in Chapter 4. \\nTo start, we could look at existing research to identify a few attributes we \\nmight use to help people write more clearly. These features could include \\nfactors such as: \\nProse simplicity \\nWe often give new writers the advice to use simpler words and \\nsentence structures. We could thus establish a set of criteria on the \\nappropriate sentence and word length, and recommend changes as \\nneeded.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 34}, page_content='Tone \\nWe could measure the use of adverbs, superlatives, and punctuation \\nto measure the polarity of the text. Depending on the context, more \\nopinionated questions may receive fewer answers. \\nStructural features \\nFinally, we could try to extract the presence of important structural \\nattributes such as the use of greetings or question marks. \\nOnce we have identified and generated useful features, we can build a \\nsimple solution that uses them to provide recommendations. There is no \\nML involved here, but this phase is crucial for two reasons: it provides a \\nbaseline that is very quick to implement and will serve as a yardstick to \\nmeasure models against. \\nTo validate our intuition about how to detect good writing, we can gather a \\ndataset of “good” and “bad” text and see if we can tell the good from the \\nbad using these features. \\nMiddle Ground: Learning from Our Experience \\nNow that we have a baseline set of features, we can attempt to use them \\nto learn a model of style from a body of data. To do this we can gather a \\ndataset, extract the features we described earlier from it, and train a \\nclassifier on it to separate good and bad examples. \\nOnce we have a model that can classify written text, we can inspect it to \\nidentify which features are highly predictive and use those as \\nrecommendations. We will see how to do this in practice in Chapter 7. \\nFigure 1-11 describes this approach. On the left side, a model is trained to \\nclassify a question as good or bad. On the right side, the trained model is \\ngiven a question and scores candidate reformulations of this question that \\nwill lead to it receiving a better score. The reformulation with the highest \\nscore is recommended to the user.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 35}, page_content='Figure 1-11. A middle ground between manual and end-to-end \\nLet’s examine the challenges we outlined in “Trying to Do It All with ML: \\nAn End-to-End Framework” and see whether the classifier approach \\nmakes them any easier: \\nDataset \\nWe could obtain a dataset of good and bad examples by gathering \\nquestions from an online forum along with some measure of their \\nquality, such as the number of views or upvotes. As opposed to the \\nend-to-end approach, this does not require us to have access to \\nrevisions of the same questions. We simply need a set of good and \\nbad examples we can hope to learn aggregate features from, which is \\nan easier dataset to find. \\nModel \\nWe need to consider two things here: how predictive a model is (can \\nit efficiently separate good and bad articles?) and how easily features \\ncan be extracted from it (can we see which attributes were used to \\nclassify an example?). There are many potential models that we \\ncould use here, along with different features we could extract from \\ntext to make it more explainable. \\nLatency \\nMost text classifiers are quite quick. We could start with a simple \\nmodel such as a random forest, which can return results in less than \\na tenth of a second on regular hardware, and move on to more \\ncomplex architectures if needed. \\nEase of implementation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 36}, page_content='Compared to text generation, text classification is relatively well \\nunderstood, meaning building such a model should be relatively \\nquick. Many examples of working text classification pipelines exist \\nonline, and many such models have already been deployed to \\nproduction. \\nIf we start with a human heuristic and then build this simple model, we \\nwill quickly be able to have an initial baseline, and the first step toward a \\nsolution. Moreover, the initial model will be a great way to inform what to \\nbuild next (more on this in Part III). \\nFor more on the importance of starting with simple baselines, I sat down \\nwith Monica Rogati, who shares some of the lessons she has learned \\nhelping data teams deliver products. \\nMonica Rogati: How to Choose and Prioritize ML \\nProjects \\nAfter getting her Ph.D. in computer science, Monica Rogati started her \\ncareer at LinkedIn where she worked on core products such as integrating \\nML into the People You May Know algorithm and built the first version of \\njob-to-candidate matching. She then became VP of data at Jawbone where \\nshe built and led the entire data team. Monica is now an adviser to dozens \\nof companies whose number of employees ranges from 5 to 8,000. She has \\nkindly agreed to share some of the advice she often gives to teams when it \\ncomes to designing and executing on ML products. \\nQ: How do you scope out an ML product? \\nA: You have to remember that you are trying to use the best tools to solve \\na problem, and only use ML if it makes sense. \\nLet’s say you wanted to predict what a user of an application will do and \\nshow it to them as a suggestion. You should start by combining \\ndiscussions on modeling and product. Among other things, this includes \\ndesigning the product around handling ML failures gracefully. \\nYou could start by taking into account the confidence our model has in its \\nprediction. We could then formulate our suggestions differently based on \\nthe confidence score. If the confidence is above 90%, we present the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 37}, page_content='suggestion prominently; if it is over 50%, we still display it but with less \\nemphasis, and we do not display anything if the confidence is below this \\nscore. \\nQ: How do you decide what to focus on in an ML project? \\nA: You have to find the impact bottleneck, meaning the piece of your \\npipeline that could provide the most value if you improve on it. When \\nworking with companies, I often find that they may not be working on the \\nright problem or not be at the right growth stage for this. \\nThere are often problems around the model. The best way to find this out \\nis to replace the model with something simple and debug the whole \\npipeline. Frequently, the issues will not be with the accuracy of your \\nmodel. Frequently, your product is dead even if your model is successful. \\nQ: Why do you usually recommend starting with a simple model? \\nA: The goal of our plan should be to derisk our model somehow. The best \\nway to do this is to start with a “strawman baseline” to evaluate worst-case \\nperformance. For our earlier example, this could be simply suggesting \\nwhichever action the user previously took. \\nIf we did this, how often would our prediction be correct, and how \\nannoying would our model be to the user if we were wrong? Assuming \\nthat our model was not much better than this baseline, would our product \\nstill be valuable? \\nThis applies well to examples in natural language understanding and \\ngeneration such as chatbots, translation, Q&A, and summarization. \\nOftentimes in summarization, for example, simply extracting the top \\nkeywords and categories covered by an article is enough to serve most \\nusers’ needs. \\nQ: Once you have your whole pipeline, how do you identify the impact \\nbottleneck? \\nA: You should start with imagining that the impact bottleneck is solved, \\nand ask yourself whether it was worth the effort you estimated it would \\ntake. I encourage data scientists to compose a tweet and companies to \\nwrite a press release before they even start on a project. That helps them'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 38}, page_content='avoid working on something just because they thought it was cool and puts \\nthe impact of the results into context based on the effort. \\nThe ideal case is that you can pitch the results regardless of the outcome: if \\nyou do not get the best outcome, is this still impactful? Have you learned \\nsomething or validated some assumptions? A way to help with this is to \\nbuild infrastructure to help lower the required effort for deployment. \\nAt LinkedIn, we had access to a very useful design element, a little \\nwindow with a few rows of text and hyperlinks, that we could customize \\nwith our data. This made it easier to launch experiments for projects such \\nas job recommendations, as the design was already approved. Because the \\nresource investment was low, the impact did not have to be as large, which \\nallowed for a faster iteration cycle. The barrier then becomes about \\nnonengineering concerns, such as ethics, fairness, and branding. \\nQ: How do you decide which modeling techniques to use? \\nA: The first line of defense is looking at the data yourself. Let’s say we \\nwant to build a model to recommend groups to LinkedIn users. A naive \\nway would be to recommend the most popular group containing their \\ncompany’s name in the group title. After looking at a few examples, we \\nfound out one of the popular groups for the company Oracle was “Oracle \\nsucks!” which would be a terrible group to recommend to Oracle \\nemployees. \\nIt is always valuable to spend the manual effort to look at inputs and \\noutputs of your model. Scroll past a bunch of examples to see if anything \\nlooks weird. The head of my department at IBM had this mantra of doing \\nsomething manually for an hour before putting in any work. \\nLooking at your data helps you think of good heuristics, models, and ways \\nto reframe the product. If you rank examples in your dataset by frequency, \\nyou might even be able to quickly identify and label 80% of your use \\ncases. \\nAt Jawbone, for example, people entered “phrases” to log the content of \\ntheir meals. By the time we labeled the top 100 by hand, we had covered \\n80% of phrases and had strong ideas of what the main problems we would \\nhave to handle, such as varieties of text encoding and languages.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 39}, page_content='The last line of defense is to have a diverse workforce that looks at the \\nresults. This will allow you to catch instances where a model is exhibiting \\ndiscriminative behavior, such as tagging your friends as a gorilla, or is \\ninsensitive by surfacing painful past experiences with its smart “this time \\nlast year” retrospective. \\nConclusion \\nAs we’ve seen, building an ML-powered application starts with judging \\nfeasibility and picking an approach. Most often, picking a supervised \\napproach is the simplest way to get started. Among those, classification, \\nknowledge extraction, catalog organization, or generative models are the \\nmost common paradigms in practice. \\nAs you are picking an approach, you should identify how easily you’ll be \\nable to access strongly or weakly labeled data, or any data at all. You \\nshould then compare potential models and datasets by defining a product \\ngoal and choosing the modeling approach that best allows you to \\naccomplish this goal. \\nWe illustrated these steps for the ML Editor, opting to start with simple \\nheuristics and a classification-based approach. And finally, we covered \\nhow leaders such as Monica Rogati have been applying these practices to \\nsuccessfully ship ML models to users. \\nNow that we have chosen an initial approach, it is time to define success \\nmetrics and create an action plan to make regular progress. This will \\ninvolve setting minimal performance requirements, doing a deep dive into \\navailable modeling and data resources, and building a simple prototype. \\nWe will cover all of those in Chapter 2.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 40}, page_content='Chapter 2. Create a Plan \\nIn the previous chapter, we covered how to estimate if ML is necessary, \\nfind where it could be most appropriately used, and convert a product goal \\nto the most appropriate ML framing. In this chapter, we will cover the use \\nof metrics to track ML and product progress and compare different ML \\nimplementations. Then, we will identify methods to build a baseline and \\nplan modeling iterations. \\nI have had the unfortunate opportunity to see many ML projects be \\ndoomed from the start due to a misalignment between product metrics and \\nmodel metrics. More projects fail by producing good models that aren’t \\nhelpful for a product rather than due to modeling difficulties. This is why I \\nwanted to dedicate a chapter to metrics and planning. \\nWe will cover tips to leverage existing resources and the constraints of \\nyour problem to build an actionable plan, which will dramatically simplify \\nany ML project. \\nLet’s start with defining performance metrics in more detail. \\nMeasuring Success \\nWhen it comes to ML, the first model we build should be the simplest \\nmodel that could address a product’s needs, because generating and \\nanalyzing results is the fastest way to make progress in ML. In the \\nprevious chapter, we covered three potential approaches of increasing \\ncomplexity for the ML Editor. Here they are as a reminder: \\nBaseline; designing heuristics based on domain knowledge \\nWe could start with simply defining rules ourselves, based on prior \\nknowledge of what makes for well-written content. We will test \\nthese rules by seeing if they help differentiate between well-written \\ntext and poorly written text. \\nSimple model; classifying text as good or bad, and using the classifier to \\ngenerate recommendations \\nWe could then train a simple model to differentiate between good \\nand bad questions. Provided that the model performs well, we can'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 41}, page_content='then inspect it to see which features it found to be highly predictive \\nof a good question, and use those features as recommendations. \\nComplex model; training an end-to-end model that goes from bad text to \\ngood text \\nThis is the most complex approach, both in terms of model and data, \\nbut if we had the resources to gather the training data and build and \\nmaintain a complex model, we could solve the product requirements \\ndirectly. \\nAll of these approaches are different and may evolve as we learn more \\nfrom prototypes along the way, but when working on ML, you should \\ndefine a common set of metrics to compare the success of modeling \\npipelines. \\nNOTE \\nYou Don’t Always Need ML \\nYou may have noticed that the baseline approach does not rely on ML at \\nall. As we discussed in Chapter 1, some features do not require ML. It is \\nimportant to also realize that even features that could benefit from ML can \\noften simply use a heuristic for their first version. Once the heuristic is \\nbeing used, you may even realize that you do not need ML at all. \\nBuilding a heuristic is also often the fastest way to build a feature. Once \\nthe feature is built and used, you’ll have a clearer view of your user’s \\nneeds. This will allow you to evaluate whether you need ML, and select a \\nmodeling approach. \\nIn most cases, starting without ML is the fastest way to build an ML \\nproduct. \\nTo that end, we will cover four categories of performance that have a large \\nimpact on the usefulness of any ML product: business metrics, model \\nmetrics, freshness, and speed. Clearly defining these metrics will allow us \\nto accurately measure the performance of each iteration. \\nBusiness Performance \\nWe’ve talked about the importance of starting with a clear product or \\nfeature goal. Once this goal is clear, a metric should be defined to judge its'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 42}, page_content='success. This metric should be separate from any model metrics and only \\nbe a reflection of the product’s success. Product metrics may be as simple \\nas the number of users a feature attracts or more nuanced such as the click-\\nthrough rate (CTR) of the recommendations we provide. \\nProduct metrics are ultimately the only ones that matter, as they represent \\nthe goals of your product or feature. All other metrics should be used as \\ntools to improve product metrics. Product metrics, however, do not need to \\nbe unique. While most projects tend to focus on improving one product \\nmetric, their impact is often measured in terms of multiple metrics, \\nincluding guardrail metrics, metrics that shouldn’t decline below a given \\npoint. For example, an ML project can aim to increase a given metric such \\nas CTR, while also holding other metrics steady, such as the average user \\nsession length, for example. \\nFor the ML Editor, we will pick a metric that measures the usefulness of a \\nrecommendation. For example, we could use the proportion of times that \\nusers follow suggestions. In order to compute such a metric, the interface \\nof the ML editor should capture whether a user approves of a suggestion, \\nby overlaying it above the input and making it clickable, for example. \\nWe’ve seen that each product lends itself to many potential ML \\napproaches. To measure the effectiveness of an ML approach, you should \\ntrack model performance. \\nModel Performance \\nFor most online products, the ultimate product metric that determines the \\nsuccess of a model is the proportion of visitors who use the output of a \\nmodel out of all the visitors who could benefit from it. In the case of \\na recommendation system, for example, performance is often judged by \\nmeasuring how many people click on recommended products \\n(see Chapter 8 for potential pitfalls of this approach). \\nWhen a product is still being built and not deployed yet, it is not possible \\nto measure usage metrics. To still measure progress, it is important to \\ndefine a separate success metric called an offline metric or a model metric. \\nA good offline metric should be possible to evaluate without exposing a \\nmodel to users, and be as correlated as possible with product metrics and \\ngoals.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 43}, page_content='Different modeling approaches use different model metrics, and changing \\napproaches can make it much easier to reach a level of modeling \\nperformance that is sufficient to accomplish product goals. \\nFor example, let’s say you are trying to offer helpful suggestions to users \\nas they type a search query on an online retail website. You’ll measure the \\nsuccess of this feature by measuring CTR, how often users click on the \\nsuggestions you make. \\nTo generate the suggestions, you could build a model that attempts to \\nguess the words a user will type and present the predicted completed \\nsentence to them as they write. You could measure the performance of this \\nmodel by computing its word-level accuracy, calculating how often it \\npredicts the correct next set of words. Such a model would need to reach \\nextremely high accuracy to help increase the product’s CTR, because a \\nprediction error of one word would be enough to render a suggestion \\nuseless. This approach is sketched out on the left side of Figure 2-1. \\nAnother approach would be to train a model that classifies user input into \\ncategories in your catalog and suggests the three most likely predicted \\ncategories. You’d measure the performance of your model using accuracy \\nover all categories rather than accuracy over every English word. Since the \\nnumber of categories in a catalog is much smaller than the English \\nvocabulary, this would be a much easier modeling metric to optimize. In \\naddition, the model only needs to predict one category correctly to \\ngenerate a click. It is much easier for this model to increase the product’s \\nCTR. You can see a mock-up of how this approach would work in practice \\non the right side of Figure 2-1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 44}, page_content='Figure 2-1. Slightly altering a product can make modeling tasks much easier \\nAs you can see, small changes to the interaction between the model and \\nproduct can make it possible to use a more straightforward modeling \\napproach and deliver results more reliably. Here are a few other examples \\nof updating an application to make a modeling task easier: \\n\\uf0b7 Changing an interface so that a model’s results can be omitted if \\nthey are below a confidence threshold. When building a model to \\nautocomplete a sentence typed by the user, for example, the model \\nmay perform well only on a subset of sentences. We can implement \\nlogic to only show a suggestion to users if the model’s confidence \\nscore exceeds 90%. \\n\\uf0b7 Presenting a few other predictions or heuristics in addition to a \\nmodel’s top prediction. For example, most websites display more \\nthan one recommendation suggested by a model. Displaying five \\ncandidate items instead of just one makes it more likely that \\nsuggestions will be useful to users, even if the model is the same. \\n\\uf0b7 Communicating to users that a model is still in an experimental \\nphase and giving them opportunities to provide feedback. When \\nautomatically detecting a language that is not in the user’s native \\ntongue and translating it for them, websites often add a button for a \\nuser to let them know whether the translation was accurate and \\nuseful.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 45}, page_content='Even when a modeling approach is appropriate for a problem, it can \\nsometimes be worthwhile to generate additional model metrics that \\ncorrelate better with product performance. \\nI once worked with a data scientist who built a model to generate HTML \\nfrom hand-drawn sketches of simple websites (see his post, “Automated \\nFront-End Development Using Deep Learning”). The model’s \\noptimization metric compares each predicted HTML token to the correct \\none using cross-entropy loss. The goal of the product, however, is for the \\ngenerated HTML to produce a website that looks like the input sketch, \\nregardless of the order of tokens. \\nCross-entropy does not account for alignment: if a model generates a \\ncorrect HTML sequence except for one extra token at the start, all of the \\ntokens will be shifted by one compared to the target. Such an output would \\nlead to a very high loss value, despite producing an almost ideal result. \\nThis means that when trying to evaluate the usefulness of the model, we \\nshould look beyond its optimization metric. In this example, using a BLEU \\nScore provides a better measurement of the similarity between the \\ngenerated HTML and the ideal output. \\nFinally, a product should be designed with reasonable assumptions of \\nmodel performance in mind. If a product relies on a model being perfect to \\nbe useful, it is very likely to produce inaccurate or even dangerous results. \\nFor example, if you are building a model that lets you take a picture of a \\npill and tells patients its type and dosage, what is the worst accuracy a \\nmodel could have and still be useful? If this accuracy requirement is hard \\nto attain with current methods, could you redesign your product to make \\nsure users are well served by it and not put at risk by prediction errors it \\ncould make? \\nIn our case, the product we want to build will provide writing advice. Most \\nML models have certain input they excel at and certain inputs they will \\nstruggle with. From a product standpoint, if we are not able to help—we \\nneed to make sure we are not going to hurt—we would like to limit the \\namount of time that we output a result that is worse than the input. How \\ncould we express this in model metrics? \\nLet’s say we build a classification model that attempts to predict whether a \\nquestion is good as measured by the number of upvotes it received. The'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 46}, page_content='classifier’s precision would be defined as the proportion of questions that \\nare truly good out of the ones it predicts as good. Its recall on the other \\nside is the proportion of questions that it predicts as good out of all the \\ngood questions in the dataset. \\nIf we want to always have advice that is relevant, we would want to \\nprioritize the model’s precision because when a high-precision model \\nclassifies a question as good (and thus makes a recommendation), there is \\na high chance that this question is actually good. High precision means \\nthat when we do make a recommendation, it will tend to be correct. For \\nmore about why high-precision models are more useful for writing \\nrecommendations, feel free to refer to “Chris Harland: Shipping \\nExperiments”. \\nWe measure such metrics by looking through the outputs of a model on a \\nrepresentative validation set. We will dive into what this means \\nin “Evaluate Your Model: Look Beyond Accuracy”, but for now, think of \\na validation set as a set of data that is held out from training and used to \\nestimate how your model performs on unseen data. \\nInitial model performance is important, but so is the ability of a model to \\nstay useful in the face of changing user behavior. A model trained on a \\ngiven dataset will perform well on similar data, but how do we know \\nwhether we need to update a dataset? \\nFreshness and Distribution Shift \\nSupervised models draw their predictive power from learning correlations \\nbetween input features and prediction targets. This means that most \\nmodels need to have been exposed to training data that is similar to a given \\ninput to perform well on it. A model that was trained to predict the age of a \\nuser from a photo using only photos of men will not perform well on \\nphotos of women. But even if a model is trained on an adequate dataset, \\nmany problems have a distribution of data that changes as time goes on. \\nWhen the distribution of the data shifts, the model often needs to change as \\nwell in order to maintain the same level of performance. \\nLet’s imagine that after noticing the impact of rain on traffic in San \\nFrancisco, you built a model to predict traffic conditions based on the \\namount of rain in the past week. If you built your model in October using \\ndata from the past 3 months, your model was likely trained on data with'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 47}, page_content='daily precipitation lower than an inch. See Figure 2-2 for an example of \\nhow such a distribution might look. As winter approaches, the average \\nprecipitation will become closer to 3 inches, which is higher than anything \\nthe model was exposed to during training, as you can see in Figure 2-2. If \\nthe model isn’t trained on more recent data, it will struggle to keep \\nperforming well. \\n \\nFigure 2-2. Changing distributions \\nIn general, a model can perform well on data it hasn’t seen before as long \\nas it is similar enough to the data it was exposed to during training. \\nNot all problems have the same freshness requirements. Translation \\nservices for ancient languages can expect the data they operate to remain \\nrelatively constant, while search engines need to be built with the \\nassumption that they will need to evolve as fast as users change their \\nsearch habits. \\nDepending on your business problem, you should consider how hard it \\nwill be to keep models fresh. How often will you need to retrain models, \\nand how much will it cost you each time we do so? \\nFor the ML editor, we imagine that the cadence at which the definition of \\n“well-formulated English prose” changes is relatively low, perhaps in the \\norder of a year. Freshness requirements would change if we targeted \\nspecific domains, however. For example, the right way to ask a question \\nabout mathematics will change much more slowly than the best phrasing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 48}, page_content='of questions concerning music trends. Since we estimate that models will \\nneed to be retrained every year, we will require fresh data to train on \\nyearly. \\nOur baseline and simple model can both learn from unpaired data, which \\nmakes the data gathering process simpler (we’d simply need to find new \\nquestions from the last year). The complex model requires paired data, \\nmeaning we will have to find examples of the same sentences, said in a \\n“good” and “bad” way, every year. This means satisfying the freshness \\nrequirement we’ve defined will be much harder for a model requiring \\npaired data, since it is more time-consuming to acquire an updated dataset. \\nFor most applications, popularity can help alleviate data gathering \\nrequirements. If our question phrasing service goes viral, we could add a \\nbutton for users to rate the quality of outputs. We could then gather past \\ninputs from users along with the model’s predictions and associated user \\nratings and use them as a training set. \\nFor an application to be popular, however, it should be useful. Oftentimes, \\nthis requires responding to user requests in a timely manner. The speed at \\nwhich a model can deliver predictions is thus an important factor to take \\ninto account. \\nSpeed \\nIdeally, a model should deliver a prediction quickly. This allows users to \\ninteract with it more easily and makes it easier to serve a model to many \\nconcurrent users. So how fast does a model need to be? For some use \\ncases, such as translating a short sentence, users will expect an answer \\nimmediately. For others, such as a medical diagnosis, patients would be \\nhappy to wait 24 hours if it meant that they would get the most accurate \\nresults. \\nIn our case, we will consider two potential ways we could deliver \\nsuggestions: through a submission box where the user writes, clicks \\nSubmit, and gets a result or by dynamically updating each time the user \\nenters a new letter. While we may want to favor the latter because we \\nwould be able to make the tool much more interactive, it would require \\nmodels to perform much faster.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 49}, page_content='We could conceivably imagine a user waiting a couple seconds for a result \\nonce they click a submission button, but for a model to run as a user is \\nediting text, it would need to run significantly under a second. The most \\npowerful models take longer to process data, so as we iterate through \\nmodels, we will keep this requirement in mind. Any model we use should \\nbe able to process an example through its whole pipeline in under two \\nseconds. \\nModel inference runtimes increase as models get more complex. The \\ndifference is significant even in a domain where each individual data point \\ncan be relatively small data such as NLP (as opposed to tasks on live \\nvideos, for example). On the text data used for the case study in this book, \\nfor example, an LSTM is about three times slower than a random forest \\n(around 22 ms for an LSTM, while the random forest takes only 7 ms). On \\nan individual datapoint, this difference are small, but they can quickly add \\nup when needing to run inference on tens of thousands of examples at a \\ntime. \\nFor complex applications where an inference call will be associated with \\nmultiple network calls or database queries, model execution time can \\nbecome short compared to the rest of the application logic. In those cases, \\nthe speed of said models becomes less of an issue. \\nDepending on your problem, there are other categories you could consider, \\nsuch as hardware constraints, development time, and maintainability. It is \\nimportant to develop an understanding of your needs before choosing a \\nmodel so that you make sure to pick said model in an informed way. \\nOnce you identify requirements and associated metrics, it’s time to start \\nmaking a plan. This requires estimating the challenges that lie ahead. In \\nthe next section, I’ll cover ways to leverage prior work and explore a \\ndataset to decide what to build next. \\nEstimate Scope and Challenges \\nAs we’ve seen, ML performance is often reported in terms of model \\nmetrics. While these metrics are useful, they should be used to improve the \\nproduct metrics we defined, which represent the actual task we are trying \\nto solve. As we iterate on a pipeline, we should keep in mind product \\nmetrics and aim to improve them.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 50}, page_content='The tools we have covered so far will help us determine whether a project \\nis worth tackling at all, and measure how well we are currently doing. A \\nlogical next step is to sketch out a plan of attack to estimate the scope and \\nduration of a project and anticipate potential roadblocks. \\nIn ML, success generally requires understanding the context of the task \\nwell, acquiring a good dataset, and building an appropriate model. \\nWe will cover each of these categories in the following section. \\nLeverage Domain Expertise \\nThe simplest model we can start with is a heuristic: a good rule of thumb \\nbased on knowledge of the problem and the data. The best way to devise \\nheuristics is to see what experts are currently doing. Most practical \\napplications are not entirely novel. How do people currently solve the \\nproblem you are trying to solve? \\nThe second best way to devise heuristics is to look at your data. Based on \\nyour dataset, how would you solve this task if you were doing it manually? \\nTo identify good heuristics, I recommend either learning from experts in \\nthe field or getting familiar with the data. I’ll describe both in a little more \\ndetail next. \\nLEARNING FROM EXPERTS \\nFor many domains we might want to automate, learning from experts in \\nthe domain can save us dozens of hours of work. If we are attempting to \\nbuild a predictive maintenance system for factory equipment, for example, \\nwe should start by reaching out to a factory manager to understand which \\nassumptions we can reasonably make. This could include understanding \\nhow often maintenance is currently performed, which symptoms usually \\nindicate that a machine will require maintenance soon, and the legal \\nrequirements concerning maintenance. \\nThere are, of course, examples where finding domain experts is likely to \\nbe difficult—such as proprietary data for a novel use case like predicting \\nusage of a unique website feature. In these cases, however, we can often \\nfind professionals who have had to tackle similar problems and learn from \\ntheir experiences.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 51}, page_content='This will allow us to learn about useful features we can leverage, find \\npitfalls we should avoid, and most importantly prevent us from reinventing \\nthe wheel that many data scientists get a bad rep for. \\nEXAMINING THE DATA \\nAs both Monica Rogati in “Monica Rogati: How to Choose and Prioritize \\nML Projects” and Robert Munro in “Robert Munro: How Do You Find, \\nLabel, and Leverage Data?” mention, it’s key to look at the data before we \\nstart modeling. \\nExploratory data analysis (EDA) is the process of visualizing and \\nexploring a dataset, often to get an intuition to a given business problem. \\nEDA is a crucial part of building any data product. In addition to EDA, it \\nis crucial to individually label examples in the way you hope a model \\nwould. Doing so helps validate assumptions and confirms that you chose \\nmodels that can appropriately leverage your dataset. \\nThe EDA process will allow you to get an understanding of the trends in \\nyour data, and labeling it yourself will force you to build a set of heuristics \\nto solve your problem. After having done both previous steps, you should \\nhave a clearer idea of which kind of models will serve you best, as well as \\nany additional data gathering and labeling strategies we may require. \\nThe next logical step is to see how others have tackled similar modeling \\nproblems. \\nStand on the Shoulders of Giants \\nHave people solved similar problems? If so, the best way to get started is \\nto understand and reproduce existing results. Look for public \\nimplementations either with similar models or similar datasets, or both. \\nIdeally, this would involve finding open source code and an available \\ndataset, but these aren’t always easy to come by, especially for very \\nspecific products. Nevertheless, the fastest way to get started on an ML \\nproject is to reproduce existing results and then build on top of them. \\nIn a domain with as many moving pieces as ML, it is crucial to stand on \\nthe shoulders of giants.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 52}, page_content='NOTE \\nIf you’re planning to use open source code or datasets in your work, please \\nmake sure that you are allowed to do so. Most repositories and datasets \\nwill include a license that defines acceptable usages. In addition, credit any \\nsource you end up using, ideally with a reference to their original work. \\nIt can often be a good idea to build a convincing proof of concept before \\ncommitting significant resources to a project. Before using time and \\nmoney to label data, for example, we need to convince ourselves that we \\ncan build a model that will learn from said data. \\nSo, how do we find an efficient way to start? Like most topics we will \\ncover in this book, this includes two main parts: data and code. \\nOPEN DATA \\nYou might not always be able to find a dataset that matches your desires, \\nbut you can often find a dataset that is similar enough in nature to be \\nhelpful. What does a similar dataset mean in this context? Thinking about \\nML models as mapping an input to an output is helpful here. With this in \\nmind, a similar dataset simply means a dataset with similar input and \\noutput types (but not necessarily domains). \\nFrequently, models using similar inputs and outputs can be applied to \\nentirely different contexts. On the left side of Figure 2-3 are two models \\nthat both predict a text sequence from an image input. One is used to \\ndescribe photos, while the other generates HTML code for a website from \\na screenshot of said website. Similarly, the right side of Figure 2-3 shows a \\nmodel that predicts a type of food from a text description in English, and \\nanother that predicts a music genre from a sheet music transcription.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 53}, page_content='Figure 2-3. Different models with similar inputs and outputs \\nFor example, let’s say we are trying to build a model to predict viewership \\nof news articles but are struggling to find a dataset of news articles and \\nassociated view counts. We could start with the openly accessible dataset \\nof Wikipedia page traffic statistics and train a predictive model on it. If we \\nare happy with its performance, it is reasonable to believe that given a \\ndataset of views for a news article, our model could perform reasonably \\nwell. Finding a similar dataset can help prove the validity of an approach \\nand makes it more reasonable to spend resources to acquire data. \\nThis method also works when working on proprietary data. Oftentimes, \\nthe dataset you need for a prediction task may not be easy to access. In \\nsome cases, the data you would need is not being currently collected. In \\nsuch cases, building a model that performs well on a similar dataset can \\noften be the best way to convince stakeholders to build a novel data \\ncollection pipeline or facilitate access to an existing one. \\nWhen it comes to publicly accessible data, new data sources and \\ncollections appear regularly. The following are a few that I’ve found \\nuseful: \\n\\uf0b7 The Internet archive maintains a set of datasets including website \\ndata, videos, and books. \\n\\uf0b7 The subreddit r/datasets is dedicated to sharing datasets. \\n\\uf0b7 Kaggle’s Datasets page offers a large selection in a variety of \\ndomains. \\n\\uf0b7 The UCI Machine Learning Repository is a vast resource of ML \\ndatasets.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 54}, page_content='\\uf0b7 Google’s dataset search covers a large searchable index of accessible \\ndatasets. \\n\\uf0b7 Common Crawl crawls and archives data from across the web and \\nmakes results publicly available. \\n\\uf0b7 Wikipedia also has a great evolving list of ML research datasets. \\nFor most use cases, one of these sources will provide you with a dataset \\nsufficiently similar to the one you would need. \\nTraining a model on this tangential dataset will allow you to prototype \\nand validate your results rapidly. In some cases, you can even train a \\nmodel on a tangential dataset and transfer some of its performance to your \\nfinal dataset (more on this in Chapter 4). \\nOnce you have an idea of which dataset you’ll start with, it is time to turn \\nyour attention to models. While it can be tempting to simply start building \\nyour own pipeline from scratch, it can often be worthwhile to at least \\nobserve what others have done. \\nOPEN SOURCE CODE \\nSearching for existing code can achieve two high-level goals. It lets us see \\nwhich challenges others have faced when doing similar modeling and \\nsurfaces potential issues with the given dataset. For this reason, I \\nrecommend looking for both pipelines tackling your product goal and code \\nworking with the dataset you have chosen. If you find an example, the first \\nstep would be to reproduce its results yourself. \\nI have seen many data scientists attempt to leverage ML code they found \\nonline only to find that they are unable to train the given models to a \\nsimilar level of accuracy claimed by the authors. Because new approaches \\nare not always accompanied with well-documented and functioning code, \\nML results are often hard to reproduce and thus should always be verified. \\nSimilar to your search for data, a good way to find similar codebases is to \\nabstract your problem to its input and output types and find codebases \\ntackling problems with similar types. \\nFor example, when attempting to generate HTML code from screenshots \\nof a website, Tony Beltramelli, the author of the paper, “pix2code: \\nGenerating Code from a Graphical User Interface Screenshot”, realized'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 55}, page_content='that his problem boiled down to translating an image into a sequence. He \\nleveraged existing architectures and best practices from a field that was \\nmore mature and also generated sequences from images, meaning image \\ncaptioning! This allowed him to get excellent results on an entirely new \\ntask and leverage years of work in an adjacent application. \\nOnce you’ve looked at data and at code, you’re ready to move forward. \\nIdeally, this process has given you a few pointers to start your work and \\nacquire a more nuanced perspective on your problem. Let’s sum up the \\nsituations you can find yourself in after looking for prior work. \\nBRING BOTH TOGETHER \\nAs we just discussed, leveraging existing open code and datasets can help \\nmake implementation faster. In the worst case, if none of the existing \\nmodels performs well on an open dataset, you now at least know that this \\nproject will require significant modeling and/or data collection work. \\nIf you have found an existing model that solves a similar task and \\nmanaged to train it on the dataset it was originally trained on, all that is left \\nis to adapt it to your domain. To do so, I recommend going through the \\nfollowing successive steps: \\n1. Find a similar open source model, ideally paired with a dataset it \\nwas trained on, and attempt to reproduce the training results \\nyourself. \\n2. Once you have reproduced the results, find a dataset that is closer to \\nyour use case, and attempt to train the previous model on that \\ndataset. \\n3. Once you have integrated the dataset to the training code, it is time \\nto judge how your model is doing using the metrics you defined and \\nstart iterating. \\nWe’ll explore the pitfalls of each of these steps and how to overcome them \\nstarting in Part II. For now, let’s go back to the case study and review the \\nprocess we just described. \\nML Editor Planning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 56}, page_content='Let’s examine common writing advice and search for candidate datasets \\nand models for the ML editor. \\nInitial Plan for an Editor \\nWe should start by implementing heuristics based on common writing \\nguidelines. We’ll gather these rules by searching existing guides for \\nwriting and editing, like those described in “The Simplest Approach: \\nBeing the Algorithm”. \\nOur perfect dataset would consist of questions and their associated quality. \\nFirst, we should quickly find a similar dataset that is easier to acquire. \\nBased on observed performance on this dataset, we will then expand and \\ndeepen our search if needed. \\nSocial media posts and online forums are good examples of text associated \\nwith a quality metric. Since most of these metrics exist to favor useful \\ncontent, they often include quality metrics such as “likes” or “upvotes.” \\nStack Exchange, a network of Q&A communities, is a popular site for \\nquestions and answers. There’s also an entire anonymized data dump of \\nStack Exchange on the Internet Archive, one of the data sources we \\nmentioned earlier. This is a great dataset to start with. \\nWe can build an initial model by using Stack Exchange questions and \\ntrying to predict a question’s upvotes score from its content. We will also \\nuse this opportunity to look through the dataset and label it, trying to find \\npatterns. \\nThe model we want to build attempts to classify text quality accurately, to \\nthen provide writing recommendations. Many open source models exist \\nfor text classification; check out this popular Python ML library scikit-\\nlearn tutorial on the topic. \\nOnce we have a working classifier, we will cover how to leverage it to \\nmake recommendations in Chapter 7. \\nNow that we have a potential initial dataset, let’s transition to models and \\ndecide what we should start with.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 57}, page_content='Always Start with a Simple Model \\nAn important takeaway of this chapter is that the purpose of building an \\ninitial model and dataset is to produce informative results that will guide \\nfurther modeling and data gathering work toward a more useful product. \\nBy starting with a simple model and extracting trends of what makes a \\nStack Overflow question successful, we can quickly measure performance \\nand iterate on it. \\nThe opposite approach of trying to build a perfect model from scratch does \\nnot work in practice. This is because ML is an iterative process where the \\nfastest way to make progress is to see how a model fails. The faster your \\nmodel fails, the more progress you will make. We will dive into this \\niterative process in much more detail in Part III. \\nWe should keep caveats of each approach in mind, however. For example, \\nthe engagement that a question receives depends on many more factors \\nthan just the quality of its formulation. The context of the post, the \\ncommunity it was posted in, the popularity of the poster, the time at which \\nit was posted, and many other details that the initial model will ignore also \\nmatter very much. To take such factors into account, we will restrict our \\ndataset to a subset of communities. Our first model will ignore all \\nmetadata related to a post, but we will consider incorporating it if it seems \\nnecessary. \\nAs such, our model uses what is often referred to as a weak label, one that \\nis only slightly correlated with the desired output. As we analyze how the \\nmodel performs, we will determine whether this label contains enough \\ninformation for it to be useful. \\nWe have a starting point, and we can now decide how we will progress. \\nMaking regular progress in ML can often seem hard due to the \\nunpredictable aspect of modeling. It is hard to know ahead of time to \\nwhich extent a given modeling approach will succeed. Because of this, I’d \\nlike to share a few tips to make steady progress. \\nTo Make Regular Progress: Start Simple'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 58}, page_content='It is worth repeating that much of the challenge in ML is similar to one of \\nthe biggest challenges in software—resisting the urge to build pieces that \\nare not needed yet. Many ML projects fail because they rely on an initial \\ndata acquisition and model building plan and do not regularly evaluate and \\nupdate this plan. Because of the stochastic nature of ML, it is extremely \\nhard to predict how far a given dataset or model will get us. \\nFor that reason, it is vital to start with the simplest model that could \\naddress your requirements, build an end-to-end prototype including this \\nmodel, and judge its performance not simply in terms of optimization \\nmetrics but in terms of your product goal. \\nStart with a Simple Pipeline \\nIn the vast majority of cases, looking at the performance of a simple model \\non an initial dataset is the best way to decide what task should be tackled \\nnext. The goal is then to repeat this approach for each of the following \\nsteps, making small incremental improvements that are easy to track, \\nrather than attempting to build the perfect model in one go. \\nTo do this, we will need to build a pipeline that can take data in and return \\nresults. For most ML problems, there are actually two separate pipelines to \\nconsider. \\nTRAINING \\nFor your model to be able to make accurate predictions, you first need to \\ntrain it. \\nA training pipeline ingests all of the labeled data you would like to train on \\n(for some tasks, datasets can be so large that they cannot fit on a single \\nmachine) and passes it to a model. It then trains said model on the dataset \\nuntil it reaches satisfactory performance. Most often, a training pipeline is \\nused to train multiple models and compare their performance on a held-out \\nvalidation set. \\nINFERENCE \\nThis is your pipeline in production. It serves the results of a trained model \\nto your user.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 59}, page_content='At a high level, an inference pipeline starts by accepting input data and \\npreprocessing it. The preprocessing phase usually consists of multiple \\nsteps. Most commonly, these steps will include cleaning and validating the \\ninput, generating features a model needs, and formatting the data to a \\nnumerical representation appropriate for an ML model. Pipelines in more \\ncomplex systems also often need to fetch additional information the model \\nneeds such as user features stored in a database, for example. The pipeline \\nthen runs the example through the model, applies any postprocessing \\nlogic, and returns a result. \\nFigure 2-4 shows a flowchart of a typical inference and training pipeline. \\nIdeally, the cleaning and preprocessing steps should be the same for both \\ntraining and inference pipelines to ensure that a trained model receives \\ndata with the same format and characteristics at inference time. \\n \\nFigure 2-4. Training and inference pipelines are complementary \\nPipelines for different models will be built with different concerns in \\nmind, but generally, the high-level infrastructure remains relatively stable. \\nThis is why it is valuable to start by building both your training and \\ninference pipeline end-to-end to quickly evaluate the impact bottleneck \\nMonica Rogati mentioned in “Monica Rogati: How to Choose and \\nPrioritize ML Projects”. \\nMost pipelines have a similar high-level structure, but because of \\ndifferences in the structure of datasets, the functions themselves often have \\nnothing in common. Let’s illustrate this by looking at the pipeline for the \\neditor.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 60}, page_content='Pipeline for the ML Editor \\nFor the editor, we will be building both training and inference pipelines \\nusing Python, which is a common language of choice in ML. The goal in \\nthis first prototype is to build an end-to-end pipeline without being too \\nconcerned with its perfection. \\nAs it should be done in any work that takes time, we can, and will, revisit \\nparts of it to improve them. For training, we will write a pretty standard \\npipeline, one broadly applicable to many ML problems and which has a \\nfew functions, mainly ones that: \\n\\uf0b7 Load records of data. \\n\\uf0b7 Clean data by removing incomplete records and input missing values \\nwhen necessary. \\n\\uf0b7 Preprocess and format data in a way that can be understood by a \\nmodel. \\n\\uf0b7 Remove a set of data that will not be trained on but used to validate \\nmodel results (a validation set). \\n\\uf0b7 Train a model on a given subset of data and return a trained model \\nand summary statistics. \\nFor inference, we will leverage some functions from the training pipeline, \\nas well as writing a few custom ones. Ideally, we would need functions \\nthat: \\n\\uf0b7 Load a trained model and keep it in memory (to provide faster \\nresults) \\n\\uf0b7 Will preprocess (same as training) \\n\\uf0b7 Gather any relevant outside information \\n\\uf0b7 Will pass one example through a model (an inference function) \\n\\uf0b7 Will postprocess, to clean up results before serving them to users \\nIt is often easiest to visualize a pipeline as a flowchart, such as the one \\ndepicted for Figure 2-5.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 61}, page_content='Figure 2-5. Pipelines for the editor \\nIn addition, we will write various analysis and exploration functions to \\nhelp us diagnose problems, such as: \\n\\uf0b7 A function that visualizes examples the model performs the best and \\nworst on \\n\\uf0b7 Functions to explore data \\n\\uf0b7 A function to explore model results \\nMany pipelines contain steps that validate inputs to the model and check \\nits final outputs. Such checks help with debugging, as you’ll see \\nin Chapter 10, and help guarantee a standard of quality for an application \\nby catching any poor results before displaying them to a user. \\nRemember that when using ML, the outputs of models on unseen data can \\noften be unpredictable and will not always be satisfactory. For this reason, \\nit is important to acknowledge that models will not always work and to \\narchitect systems around this potential for mistakes. \\nConclusion \\nWe have now seen how to define core metrics that allow us to compare \\nentirely different models and understand the trade-offs between each of \\nthem. We covered resources and methods to use to speed up the building'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 62}, page_content='process of your first few pipelines. We then outlined an overview of what \\nwe’ll need to build for each pipeline to get a first set of results. \\nWe now have an idea framed as an ML problem, a way to measure \\nprogress, and an initial plan. It is time to dive into the implementation. \\nIn Part II, we will dive into how to build a first pipeline and explore and \\nvisualize an initial dataset.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 63}, page_content='Part II. Build a Working Pipeline \\nSince researching, training, and evaluating models is a time-consuming \\nprocess, going in the wrong direction can be very costly in ML. This is \\nwhy this book focuses on reducing risk and identifying the highest priority \\nto work on. \\nWhile Part I focused on planning in order to maximize our speed and \\nchances of success, this chapter will dive into implementation. \\nAs Figure II-1 shows, in ML like in much of software engineering, you \\nshould get to a minimum viable product (MVP) as soon as possible. This \\nsection will cover just that: the quickest way to get a pipeline in place and \\nevaluating it. \\nImproving said model will be the focus of Part III of this book. \\n \\nFigure II-1. The right way to build your first pipeline (reproduced with permission from Henrik Kniberg) \\nWe will build our initial model in two steps: \\nChapter 3 \\nIn this chapter, we will build the structure and scaffolding of our \\napplication. This will involve building a pipeline to take user input \\nand return suggestions, and a separate pipeline to train our models \\nbefore we use them. \\nChapter 4'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 64}, page_content='In this chapter, we will focus on gathering and inspecting an initial \\ndataset. The goal here is to quickly identify patterns in our data and \\npredict which of these patterns will be predictive and useful for our \\nmodel.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 65}, page_content='Chapter 3. Build Your First End-to-End \\nPipeline \\nIn Part I, we started by covering how to go from product requirements to \\ncandidate modeling approaches. Then, we moved on to the planning stage \\nand described how to find relevant resources and leverage them to make \\nan initial plan of what to build. Finally, we discussed how building an \\ninitial prototype of a functioning system was the best way to make \\nprogress. This is what we will cover in this chapter. \\nThis first iteration will be lackluster by design. Its goal is to allow us to \\nhave all the pieces of a pipeline in place so that we can prioritize which \\nones to improve next. Having a full prototype is the easiest way to identify \\nthe impact bottleneck that Monica Rogati described in “Monica Rogati: \\nHow to Choose and Prioritize ML Projects”. \\nLet’s start by building the simplest pipeline that could produce predictions \\nfrom an input. \\nThe Simplest Scaffolding \\nIn “Start with a Simple Pipeline”, we described how most ML models \\nconsist of two pipelines, training and inference. Training allows us to \\ngenerate a high-quality model, and inference is about serving results to \\nusers. See “Start with a Simple Pipeline” for more about the difference \\nbetween training and inference. \\nFor the first prototype of an application, we will focus on being able to \\ndeliver results to users. This means that out of the two pipelines we \\ndescribed in Chapter 2, we will start with the inference pipeline. This will \\nallow us to quickly examine how users may interact with the output of a \\nmodel, therefore gathering useful information to make training a model \\neasier. \\nIf we are only focusing on inference, we will ignore training for now. And \\nsince we are not training a model, we can instead write some simple rules. \\nWriting such rules or heuristics is often a great way to get started. It is the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 66}, page_content='quickest way to build a prototype and allows us to see a simplified version \\nof the full application right away. \\nWhile this may seem superfluous if we are aiming to implement an ML \\nsolution anyway (as we will later in the book), it is a critical forcing \\nfunction to make us confront our problem and devise an initial set of \\nhypotheses about how best to solve it. \\nBuilding, validating, and updating hypotheses about the best way to model \\ndata are core parts of the iterative model building process, which starts \\nbefore we even build our first model! \\nNOTE \\nHere are a couple of examples of great heuristics from projects I have seen \\nused by Fellows I mentored at Insight Data Science. \\n\\uf0b7 Code quality estimation: When building a model aiming to predict \\nwhether a coder performed well on HackerRank (a competitive \\ncoding website) from a sample of code, Daniel started by counting \\nthe number of open and closed parentheses, brackets, and curly \\nbraces. \\nIn the majority of proper working code, the counts of opening and \\nclosing brackets match, so this rule proved to be quite a strong \\nbaseline. Furthermore, it gave him the intuition to focus his \\nmodeling on using an abstract syntax tree to capture even more \\nstructural information about code. \\n\\uf0b7 Tree counting: When trying to count trees in a city from satellite \\nimagery, after looking at some data, Mike started by devising a rule \\nestimating tree density based on counting the proportion of green \\npixels in a given image. \\nIt turns out that this approach worked for trees that were spread apart \\nbut failed when it came to groves of trees. Again, this helped define \\nthe next modeling steps, which focused on building a pipeline that \\ncan handle densely grouped trees. \\nThe vast majority of ML projects should start with a similar heuristic. The \\nkey is to remember to devise it based on expert knowledge and data \\nexploration and to use it to confirm initial assumptions and speed up \\niteration.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 67}, page_content='Once you have a heuristic, it is time to create a pipeline that can gather \\ninput, pre-process it, apply your rules to it, and serve results. This could be \\nas simple as a Python script you could call from the terminal or a web \\napplication that gathers a user’s camera feed to then serve live results. \\nThe point here is to do for your product the same thing we did for your \\nML approach, simplify it as much as possible, and build it so you have a \\nsimple functional version. This is often referred to as an MVP (minimum \\nviable product) and is a battle-tested method for getting useful results as \\nfast as possible. \\nPrototype of an ML Editor \\nFor our ML editor, we will leverage common editing recommendations to \\ncraft a few rules about what makes for good or bad questions and display \\nthe results of those rules to users. \\nFor a minimal version of our project that takes user input from the \\ncommand line and returns suggestions, we only need to write four \\nfunctions, shown here: \\ninput_text = parse_arguments() \\nprocessed = clean_input(input_text) \\ntokenized_sentences = preprocess_input(processed) \\nsuggestions = get_suggestions(tokenized_sentences) \\nLet’s dive into each of them! We will keep the argument parser simple and \\nstart with taking a string of text from the user, with no options. You can \\nfind the source code for the example and all other code examples in this \\nbook’s GitHub repository. \\nParse and Clean Data \\nFirst, we simply parse incoming data coming from the command line. This \\nis relatively straightforward to write in Python. \\ndef parse_arguments(): \\n    \"\"\" \\n \\n    :return: The text to be edited \\n    \"\"\" \\n    parser = argparse.ArgumentParser( \\n        description=\"Receive text to be edited\" \\n    )'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 68}, page_content='parser.add_argument( \\n        \\'text\\', \\n        metavar=\\'input text\\', \\n        type=str \\n    ) \\n    args = parser.parse_args() \\n    return args.text \\nWhenever a model runs on user input, you should start by validating and \\nverifying it! In our case, users will type in data, so we will make sure that \\ntheir input contains characters we can parse. To clean our input, we will \\nremove non-ASCII characters. This shouldn’t restrict our users’ creativity \\ntoo much and allow us to make reasonable assumptions about what is in \\nthe text. \\ndef clean_input(text): \\n    \"\"\" \\n \\n    :param text: User input text \\n    :return: Sanitized text, without non ascii characters \\n    \"\"\" \\n    # To keep things simple at the start, let\\'s only keep ASCII \\ncharacters \\n    return str(text.encode().decode(\\'ascii\\', errors=\\'ignore\\')) \\nNow, we need to preprocess our input and provide recommendations. To \\nget us started, we will lean on some of the existing research about \\nclassifying text we mentioned in “The Simplest Approach: Being the \\nAlgorithm”. This will involve counting words such as “told” and “said” \\nand computing summary statistics of syllables, words, and sentences to \\nestimate sentence complexity. \\nTo compute word-level statistics, we need to be able to identify words \\nfrom sentences. In the world of natural language processing, this is known \\nas tokenization. \\nTokenizing Text \\nTokenization is not straightforward, and most simple methods you can \\nthink of, such as splitting our input into words based on spaces or periods, \\nwill fail on realistic text due to the diversity of ways words can be \\nseparated. Consider this sentence, provided as an example by \\nStanford’s NLP class, for example: \\n“Mr. O’Neill thinks that the boys’ stories about Chile’s capital aren’t \\namusing.”'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 69}, page_content='Most simple methods will fail on this sentence due to the presence of \\nperiods and apostrophes that carry various meanings. Instead of building \\nour own tokenizer, we will leverage nltk, a popular open source library, \\nwhich allows us to do this in two easy steps, as follows: \\ndef preprocess_input(text): \\n    \"\"\" \\n \\n    :param text: Sanitized text \\n    :return: Text ready to be fed to analysis, by having sentences and \\n    words tokenized \\n    \"\"\" \\n    sentences = nltk.sent_tokenize(text) \\n    tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \\n    return tokens \\nOnce our output is preprocessed, we can use it to generate features that \\nwill help judge the quality of a question. \\nGenerating Features \\nThe last step is to write a few rules we could use to give advice to our \\nusers. For this simple prototype, we will start by computing the frequency \\nof a few common verbs and connectors and then count adverb usage and \\ndetermine the Flesch readability score. We will then return a report of \\nthese metrics to our users: \\ndef get_suggestions(sentence_list): \\n    \"\"\" \\n    Returns a string containing our suggestions \\n    :param sentence_list: a list of sentences, each being a list of \\nwords \\n    :return: suggestions to improve the input \\n    \"\"\" \\n    told_said_usage = sum( \\n        (count_word_usage(tokens, [\"told\", \"said\"]) for tokens in \\nsentence_list) \\n    ) \\n    but_and_usage = sum( \\n        (count_word_usage(tokens, [\"but\", \"and\"]) for tokens in \\nsentence_list) \\n    ) \\n    wh_adverbs_usage = sum( \\n        ( \\n            count_word_usage( \\n                tokens, \\n                [ \\n                    \"when\", \\n                    \"where\", \\n                    \"why\", \\n                    \"whence\", \\n                    \"whereby\",'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 70}, page_content='\"wherein\", \\n                    \"whereupon\", \\n                ], \\n            ) \\n            for tokens in sentence_list \\n        ) \\n    ) \\n    result_str = \"\" \\n    adverb_usage = \"Adverb usage: %s told/said, %s but/and, %s wh \\nadverbs\" % ( \\n        told_said_usage, \\n        but_and_usage, \\n        wh_adverbs_usage, \\n    ) \\n    result_str += adverb_usage \\n    average_word_length = \\ncompute_total_average_word_length(sentence_list) \\n    unique_words_fraction = \\ncompute_total_unique_words_fraction(sentence_list) \\n \\n    word_stats = \"Average word length %.2f, fraction of unique words \\n%.2f\" % ( \\n        average_word_length, \\n        unique_words_fraction, \\n    ) \\n    # Using HTML break to later display on a webapp \\n    result_str += \"<br/>\" \\n    result_str += word_stats \\n \\n    number_of_syllables = count_total_syllables(sentence_list) \\n    number_of_words = count_total_words(sentence_list) \\n    number_of_sentences = len(sentence_list) \\n \\n    syllable_counts = \"%d syllables, %d words, %d sentences\" % ( \\n        number_of_syllables, \\n        number_of_words, \\n        number_of_sentences, \\n    ) \\n    result_str += \"<br/>\" \\n    result_str += syllable_counts \\n \\n    flesch_score = compute_flesch_reading_ease( \\n        number_of_syllables, number_of_words, number_of_sentences \\n    ) \\n \\n    flesch = \"%d syllables, %.2f flesch score: %s\" % ( \\n        number_of_syllables, \\n        flesch_score, \\n        get_reading_level_from_flesch(flesch_score), \\n    ) \\n \\n    result_str += \"<br/>\" \\n    result_str += flesch \\n \\n    return result_str'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 71}, page_content='Voilà, we can now call our application from the command line and see its \\nresults live. It is not very useful yet, but we have a starting point we can \\ntest and iterate from, which we’ll do next. \\nTest Your Workflow \\nNow that we’ve built this prototype, we can test our assumptions about the \\nway we’ve framed our problem and how useful our proposed solution is. \\nIn this section, we will take a look both at the objective quality of our \\ninitial rules and examine whether we are presenting our output in a useful \\nmanner. \\nAs Monica Rogati shared earlier, “Frequently, your product is dead even if \\nyour model is successful.” If the method we have chosen excels at \\nmeasuring question quality but our product does not provide any advice to \\nusers to improve their writing, our product will not be useful despite the \\nquality of our method. Looking at our complete pipeline, let’s evaluate \\nboth the usefulness of the current user experience and the results of our \\nhandcrafted model. \\nUser Experience \\nLet’s first examine how satisfying our product is to use, independently of \\nthe quality of our model. In other words, if we imagine that we will \\neventually get a model that performs well enough, is this the most useful \\nway to present results to our users? \\nIf we are building a tree census, for example, we may want to present our \\nresults as a summary of a long-running analysis of an entire city. We may \\nwant to include the number of reported trees, as well as broken-down \\nstatistics per neighborhood, and a measure of the error on a gold standard \\ntest set. \\nIn other words, we would want to make sure that the results we present are \\nuseful (or will be if we improve our model). On the flip side, of course, \\nwe’d also like our model to perform well. That is the next aspect we’ll \\nevaluate.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 72}, page_content='Modeling Results \\nWe mentioned the value of focusing on the right metric in “Measuring \\nSuccess”. Having a working prototype early on will allow us to identify \\nand iterate on our chosen metrics to make sure they represent product \\nsuccess. \\nAs an example, if we were building a system to help users search for rental \\ncars nearby, we may use a metric such as discounted cumulative gain \\n(DCG). DCG measures ranking quality by outputting a score that is \\nhighest when the most relevant items are returned earlier than others \\n(see the Wikipedia article on DCG for more information about ranking \\nmetrics). When initially building our tool, we may have assumed that we \\nwanted at least one useful suggestion to appear in our first five results. We \\nthus used DCG at 5 to score our model. However, when having users try \\nthe tool, we may notice that users only ever consider the first three results \\ndisplayed. In that case, we should change our metric of success from DCG \\nat 5 to 3. \\nThe goal of considering both user experience and model performance is to \\nmake sure we are working on the most impactful aspect. If your user \\nexperience is poor, improving your model is not helpful. In fact, you may \\nrealize you would be better served with an entirely different model! Let’s \\nlook at two examples. \\nFINDING THE IMPACT BOTTLENECK \\nThe goal of looking both at modeling results and at the current \\npresentation of the product is to identify which challenge to tackle next. \\nMost of the time, this will mean iterating on the way we present results to \\nour users (which could mean changing the way we train our models) or \\nimproving model performance by identifying key failure points. \\nWhile we will dive into error analysis more in Part III, we should identify \\nfailure modes and appropriate ways to resolve them. It is important to \\ndetermine whether the most impactful task to work on is in the modeling \\nor product domain, as they each require different remediations. Let’s see \\nan example of each: \\nOn the product side'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 73}, page_content='Let’s say you have built a model that looks at images of research \\npapers and predicts whether they will be accepted to top conferences \\n(see Jia-Bin Huang’s paper “Deep Paper Gestalt,” which tackles this \\nissue). However, you’ve noticed that returning only a probability of \\nrejection to a user is not the most satisfying of outputs. In this case, \\nimproving your model would not be helpful. It would make sense to \\nfocus on extracting advice from the model so that we can help our \\nusers improve their papers and increase their chances of being \\naccepted. \\nOn the model side \\nYou’ve built a credit scoring model and are noticing that with all \\nother factors being equal, it assigns higher risks of defaulting to a \\ncertain ethnic group. This is likely due to a bias in the training data \\nyou have been using, so you should gather more representative data \\nand build a new cleaning and augmentation pipeline to attempt to \\naddress this. In this case, regardless of the manner in which you \\npresent results, the model needs to be fixed. Examples like this are \\ncommon and a reason why you should always dive deeper than an \\naggregate metric and look at the impact of your model on different \\nslices of your data. This is what we will do in Chapter 5. \\nTo illustrate this further, let’s go through this exercise for our ML Editor. \\nML Editor Prototype Evaluation \\nLet’s see how our initial pipeline fares both in terms of user experience \\nand model performance. Let’s start by throwing in a few inputs to our \\napplication. We will start by testing a simple question, a convoluted \\nquestion, and a full paragraph. \\nSince we are using a reading ease score, we would ideally like our \\nworkflow to return a high score for the simple sentence, a low score for the \\nconvoluted one, and suggestions for improving our paragraph. Let’s \\nactually run a few examples through our prototype. \\nSimple question: \\n$ python ml_editor.py  \"Is this workflow any good?\" \\nAdverb usage: 0 told/said, 0 but/and, 0 wh adverbs \\nAverage word length 3.67, fraction of unique words 1.00 \\n6 syllables, 5 words, 1 sentences'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 74}, page_content='6 syllables, 100.26 flesch score: Very easy to read \\nConvoluted question: \\n$ python ml_editor.py  \"Here is a needlessly obscure question, \\nthat\"\\\\ \\n\"does not provide clearly which information it would\"\\\\ \\n\"like to acquire, does it?\" \\n \\nAdverb usage: 0 told/said, 0 but/and, 0 wh adverbs \\nAverage word length 4.86, fraction of unique words 0.90 \\n30 syllables, 18 words, 1 sentences \\n30 syllables, 47.58 flesch score: Difficult to read \\nEntire paragraph (that you’ll recognize from earlier): \\n$ python ml_editor.py \"Ideally, we would like our workflow to return \\na positive\"\\\\ \\n\" score for the simple sentence, a negative score for the convoluted \\none, and \"\\\\ \\n\"suggestions for improving our paragraph. Is that the case already?\" \\nAdverb usage: 0 told/said, 1 but/and, 0 wh adverbs \\nAverage word length 4.03, fraction of unique words 0.76 \\n52 syllables, 33 words, 2 sentences \\n52 syllables, 56.79 flesch score: Fairly difficult to read \\nLet’s examine these results using both of the aspects we’ve just defined. \\nModel \\nIt is unclear whether our results align well with what we would consider \\nquality writing. The convoluted sentence and the entire paragraph receive \\na similar readability score. Now, I will be the first to admit that my prose \\ncan sometimes be difficult to read, but the earlier paragraph is more \\ncomprehensible than the convoluted sentence we tested before it. \\nThe attributes we are extracting from the text are not necessarily the most \\ncorrelated with “good writing.” This is usually due to not having defined \\nsuccess clearly enough: given two questions, how can we say one is better \\nthan the other? When we build our dataset in the next chapter, we will \\ndefine this more clearly. \\nAs expected, we have some modeling work to do, but are we even \\npresenting results in a useful manner?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 75}, page_content='User Experience \\nFrom the results shown earlier, two issues are immediately apparent. The \\ninformation we return is both overwhelming and irrelevant. The goal of \\nour product is to provide actionable recommendations to our users. The \\nfeatures and readability score are a quality metric but will not help a user \\ndecide how to improve their submission. We may want to boil down our \\nrecommendations to a single score, along with actionable \\nrecommendations to improve it. \\nFor example, we could suggest general changes such as using fewer \\nadverbs, or work at a more granular level by suggesting word- and \\nsentence-level changes. Ideally, we could present results by highlighting or \\nunderlining the parts of the input that require users’ attention. I’ve added a \\nmock-up of how this could look in Figure 3-1. \\n \\nFigure 3-1. More actionable writing suggestions \\nEven if we were not able to directly highlight recommendations in the \\ninput string, our product could benefit from providing recommendations \\nsimilar to the ones on the right side of Figure 3-1, which are more \\nactionable than a list of scores. \\nConclusion \\nWe have built an initial inference prototype and used it to evaluate the \\nquality of our heuristics and the workflow of our product. This allowed us'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 76}, page_content='to narrow down our performance criteria and iterate on the way we would \\nlike to present results to our users. \\nFor the ML Editor, we’ve learned that we should both focus on providing \\na better user experience by providing actionable recommendations and \\nimprove our modeling approach by looking at data to more clearly define \\nwhat makes for a good question. \\nIn the first three chapters, we’ve used our product goals to define which \\ninitial approach to take, explored existing resources to make a plan for our \\napproach, and built an initial prototype to validate our plan and \\nassumptions. \\nNow, it is time to dive into what is often the most overlooked part of an \\nML project—exploring our dataset. In Chapter 4, we will see how to \\ngather an initial dataset, assess its quality, and iteratively label subsets of it \\nto help guide our feature generation and modeling decisions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 77}, page_content='Chapter 4. Acquire an Initial Dataset \\nOnce you have a plan to solve your product needs and have built an initial \\nprototype to validate that your proposed workflow and model are sound, it \\nis time to take a deeper dive into your dataset. We will use what we find to \\ninform our modeling decisions. Oftentimes, understanding your data well \\nleads to the biggest performance improvements. \\nIn this chapter, we will start by looking at ways to efficiently judge the \\nquality of a dataset. Then, we will cover ways to vectorize your data and \\nhow to use said vectorized representation to label and inspect a dataset \\nmore efficiently. Finally, we’ll cover how this inspection should guide \\nfeature generation strategies. \\nLet’s start by discovering a dataset and judging its quality. \\nIterate on Datasets \\nThe fastest way to build an ML product is to rapidly build, evaluate, and \\niterate on models. Datasets themselves are a core part of that success of \\nmodels. This is why data gathering, preparation, and labeling should be \\nseen as an iterative process, just like modeling. Start with a simple dataset \\nthat you can gather immediately, and be open to improving it based on \\nwhat you learn. \\nThis iterative approach to data can seem confusing at first. In ML research, \\nperformance is often reported on standard datasets that the community \\nuses as benchmarks and are thus immutable. In traditional software \\nengineering, we write deterministic rules for our programs, so we treat \\ndata as something to receive, process, and store. \\nML engineering combines engineering and ML in order to build products. \\nOur dataset is thus just another tool to allow us to build products. In ML \\nengineering, choosing an initial dataset, regularly updating it, and \\naugmenting it is often the majority of the work. This difference in \\nworkflow between research and industry is illustrated in Figure 4-1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 78}, page_content='Figure 4-1. Datasets are fixed in research, but part of the product in industry  \\nTreating data as part of your product that you can (and should) iterate on, \\nchange, and improve is often a big paradigm shift for newcomers to the \\nindustry. Once you get used to it, however, data will become your best \\nsource of inspiration to develop new models and the first place you look \\nfor answers when things go wrong. \\nDo Data Science \\nI’ve seen the process of curating a dataset be the main roadblock to \\nbuilding ML products more times than I can count. This is partly because \\nof the relative lack of education on the topic (most online courses provide \\nthe dataset and focus on the models), which leads to many practitioners \\nfearing this part of the work. \\nIt is easy to think of working with data as a chore to tackle before playing \\nwith fun models, but models only serve as a way to extract trends and \\npatterns from existing data. Making sure that the data we use exhibits \\npatterns that are predictive enough for a model to leverage (and checking \\nwhether it contains clear bias) is thus a fundamental part of the work of a \\ndata scientist (in fact, you may have noticed the name of the role is not \\nmodel scientist).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 79}, page_content='This chapter will focus on this process, from gathering an initial dataset to \\ninspecting and validating its applicability for ML. Let’s start with \\nexploring a dataset efficiently to judge its quality. \\nExplore Your First Dataset \\nSo how do we go about exploring an initial dataset? The first step of \\ncourse is to gather a dataset. This is where I see practitioners get stuck the \\nmost often as they search for a perfect dataset. Remember, our goal is to \\nget a simple dataset to extract preliminary results from. As with other \\nthings in ML, start simple, and build from there. \\nBe Efficient, Start Small \\nFor most ML problems, more data can lead to a better model, but this does \\nnot mean that you should start with the largest possible dataset. When \\nstarting on a project, a small dataset allows you to easily inspect and \\nunderstand your data and how to model it better. You should aim for an \\ninitial dataset that is easy to work with. Only once you’ve settled on a \\nstrategy does it make sense to scale it up to a larger size. \\nIf you are working at a company with terabytes of data stored in a cluster, \\nyou can start by extracting a uniformly sampled subset that fits in memory \\non your local machine. If you would like to start working on a side project \\ntrying to identify the brands of cars that drive in front of your house, for \\nexample, start with a few dozens of images of cars on streets. \\nOnce you have seen how your initial model performs and where it \\nstruggles, you will be able to iterate on your dataset in an informed \\nmanner! \\nYou can find many existing datasets online on platforms such \\nas Kaggle or Reddit or gather a few examples yourself, either by scraping \\nthe web, leveraging large open datasets such as found on the Common \\nCrawl site, or generating data! For more information, see “Open data”. \\nGathering and analyzing data is not only necessary, it will speed you up, \\nespecially early on in a project’s development. Looking at your dataset and \\nlearning about its features is the easiest way to come up with a good \\nmodeling and feature generation pipeline.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 80}, page_content='Most practitioners overestimate the impact of working on the model and \\nunderestimate the value of working on the data, so I recommend always \\nmaking an effort to correct this trend and bias yourself toward looking at \\ndata. \\nWhen examining data, it is good to identify trends in an exploratory \\nfashion, but you shouldn’t stop there. If your aim is to build ML products, \\nyou should ask yourself what the best way to leverage these trends in an \\nautomated fashion is. How can these trends help you power an automated \\nproduct? \\nInsights Versus Products \\nOnce you have a dataset, it is time to dive into it and explore its content. \\nAs we do so, let’s keep in mind the distinction between data exploration \\nfor analysis purposes and data exploration for product building purposes. \\nWhile both aim to extract and understand trends in data, the former \\nconcerns itself with creating insights from trends (learning that most \\nfraudulent logins to a website happen on Thursdays and are from the \\nSeattle area, for example), while the latter is about using trends to build \\nfeatures (using the time of a login attempt and its IP address to build a \\nservice that prevents fraudulent accounts logins). \\nWhile the difference may seem subtle, it leads to an extra layer of \\ncomplexity in the product building case. We need to have confidence that \\nthe patterns we see will apply to data we receive in the future and quantify \\nthe differences between the data we are training on and the data we expect \\nto receive in production. \\nFor fraud prediction, noticing a seasonality aspect to fraudulent logins is \\nthe first step. We should then use this observed seasonal trend to estimate \\nhow often we need to train our models on recently gathered data. We will \\ndive into more examples as we explore our data more deeply later in this \\nchapter. \\nBefore noticing predictive trends, we should start by examining quality. If \\nour chosen dataset does not meet quality standards, we should improve it \\nbefore moving on to modeling.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 81}, page_content='A Data Quality Rubric \\nIn this section, we will cover some aspects to examine when first working \\nwith a new dataset. Each dataset comes with its own biases and oddities, \\nwhich require different tools to be understood, so writing a comprehensive \\nrubric covering anything you may want to look for in a dataset is beyond \\nthe scope of this book. Yet, there are a few categories that are valuable to \\npay attention to when first approaching a dataset. Let’s start with \\nformatting. \\nDATA FORMAT \\nIs the dataset already formatted in such a way that you have clear inputs \\nand outputs, or does it require additional preprocessing and labeling? \\nWhen building a model that attempts to predict whether a user will click \\non an ad, for example, a common dataset will consist of a historical log of \\nall clicks for a given time period. You would need to transform this dataset \\nso that it contains multiple instances of an ad being presented to a user and \\nwhether the user clicked. You’d also want to include any features of the \\nuser or the ad that you think your model could leverage. \\nIf you are given a dataset that has already been processed or aggregated for \\nyou, you should validate that you understand the way in which the data \\nwas processed. If one of the columns you were given contains an average \\nconversion rate, for example, can you calculate this rate yourself and \\nverify that it matches with the provided value? \\nIn some cases, you will not have access to the required information to \\nreproduce and validate preprocessing steps. In those cases, looking at the \\nquality of the data will help you determine which features of it you trust \\nand which ones would be best left ignored. \\nDATA QUALITY \\nExamining the quality of a dataset is crucial before you start modeling it. \\nIf you know that half of the values for a crucial feature are missing, you \\nwon’t spend hours debugging a model to try to understand why it isn’t \\nperforming well.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 82}, page_content='There are many ways in which data can be of poor quality. It can be \\nmissing, it can be imprecise, or it can even be corrupted. Getting an \\naccurate picture of its quality will not only allow you to estimate which \\nlevel of performance is reasonable, it will make it easier to select potential \\nfeatures and models to use. \\nIf you are working with logs of user activity to predict usage of an online \\nproduct, can you estimate how many logged events are missing? For the \\nevents you do have, how many contain only a subset of information about \\nthe user? \\nIf you are working on natural language text, how would you rate the \\nquality of the text? For example, are there many incomprehensible \\ncharacters? Is the spelling very erroneous or inconsistent? \\nIf you are working on images, are they clear enough that you could \\nperform the task yourself? If it is hard for you to detect an object in an \\nimage, do you think your model will struggle to do so? \\nIn general, which proportion of your data seems noisy or incorrect? How \\nmany inputs are hard for you to interpret or understand? If the data has \\nlabels, do you tend to agree with them, or do you often find yourself \\nquestioning their accuracy? \\nI’ve worked on a few projects aiming to extract information from satellite \\nimagery, for example. In the best cases, these projects have access to a \\ndataset of images with corresponding annotations denoting objects of \\ninterest such as fields or planes. In some cases, however, these annotations \\ncan be inaccurate or even missing. Such errors have a significant impact \\non any modeling approach, so it is vital to find out about them early. We \\ncan work with missing labels by either labeling an initial dataset ourselves \\nor finding a weak label we can use, but we can do so only if we notice the \\nquality ahead of time. \\nAfter verifying the format and quality of the data, one additional step can \\nhelp proactively surface issues: examining data quantity and feature \\ndistribution.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 83}, page_content='DATA QUANTITY AND DISTRIBUTION \\nLet’s estimate whether we have enough data and whether feature values \\nseem within a reasonable range. \\nHow much data do we have? If we have a large dataset, we should select a \\nsubset to start our analysis on. On the other hand, if our dataset is too small \\nor some classes are underrepresented, models we train would risk being \\njust as biased as our data. The best way to avoid such bias is to increase \\nthe diversity of our data through data gathering and augmentation. The \\nways in which you measure the quality of your data depend on your \\ndataset, but Table 4-1 covers a few questions to get you started. \\nQuality \\nFormat \\nQuantity and distribution \\nAre any relevant fields ever \\nempty? \\nHow many preprocessing steps \\ndoes your data require? \\nHow many examples do you \\nhave? \\nAre there potential errors of \\nmeasurement? \\nWill you be able to preprocess it \\nin the same way in production? \\nHow many examples per \\nclass? Are any absent? \\nTable 4-1. A data quality rubric \\nFor a practical example, when building a model to automatically \\ncategorize customer support emails into different areas of expertise, a data \\nscientist I was working with, Alex Wahl, was given nine distinct \\ncategories, with only one example per category. Such a dataset is too small \\nfor a model to learn from, so he focused most of his effort on a data \\ngeneration strategy. He used templates of common formulations for each \\nof the nine categories to produce thousands more examples that a model \\ncould then learn from. Using this strategy, he managed to get a pipeline to \\na much higher level of accuracy than he would have had by trying to build \\na model complex enough to learn from only nine examples. \\nLet’s apply this exploration process to the dataset we chose for our ML \\neditor and estimate its quality!'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 84}, page_content='ML EDITOR DATA INSPECTION \\nFor our ML editor, we initially settled on using the anonymized Stack \\nExchange Data Dump as a dataset. Stack Exchange is a network of \\nquestion-and-answer websites, each focused on a theme such as \\nphilosophy or gaming. The data dump contains many archives, one for \\neach of the websites in the Stack Exchange network. \\nFor our initial dataset, we’ll choose a website that seems like it would \\ncontain broad enough questions to build useful heuristics from. At first \\nglance, the Writing community seems like a good fit. \\nEach website archive is provided as an XML file. We need to build a \\npipeline to ingest those files and transform them into text we can then \\nextract features from. The following example shows the Posts.xml file \\nfor datascience.stackexchange.com: \\n<?xml version=\"1.0\" encoding=\"utf-8\"?> \\n<posts> \\n  <row Id=\"5\" PostTypeId=\"1\" CreationDate=\"2014-05-13T23:58:30.457\" \\nScore=\"9\" ViewCount=\"516\" Body=\"&lt;p&gt; &quot;Hello World&quot; \\nexample? \" \\nOwnerUserId=\"5\" LastActivityDate=\"2014-05-14T00:36:31.077\" \\nTitle=\"How can I do simple machine learning without hard-coding \\nbehavior?\" \\nTags=\"&lt;machine-learning&gt;\" AnswerCount=\"1\" CommentCount=\"1\" /> \\n  <row Id=\"7\" PostTypeId=\"1\" AcceptedAnswerId=\"10\" ... /> \\nTo be able to leverage this data, we will need to be able to load the XML \\nfile, decode the HTML tags in the text, and represent questions and \\nassociated data in a format that would be easier to analyze such as a \\npandas DataFrame. The following function does just this. As a reminder, \\nthe code for this function, and all other code throughout this book, can be \\nfound in this book’s GitHub repository. \\nimport xml.etree.ElementTree as ElT \\n \\n \\ndef parse_xml_to_csv(path, save_path=None): \\n    \"\"\" \\n    Open .xml posts dump and convert the text to a csv, tokenizing it in \\nthe \\n         process \\n    :param path: path to the xml document containing posts \\n    :return: a dataframe of processed text \\n    \"\"\" \\n \\n    # Use python\\'s standard library to parse XML file \\n    doc = ElT.parse(path)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 85}, page_content='root = doc.getroot() \\n \\n    # Each row is a question \\n    all_rows = [row.attrib for row in root.findall(\"row\")] \\n \\n    # Using tdqm to display progress since preprocessing takes time \\n    for item in tqdm(all_rows): \\n        # Decode text from HTML \\n        soup = BeautifulSoup(item[\"Body\"], features=\"html.parser\") \\n        item[\"body_text\"] = soup.get_text() \\n \\n    # Create dataframe from our list of dictionaries \\n    df = pd.DataFrame.from_dict(all_rows) \\n    if save_path: \\n        df.to_csv(save_path) \\n    return df \\nEven for a relatively small dataset containing only 30,000 questions this \\nprocess takes more than a minute, so we serialize the processed file back \\nto disk to only have to process it once. To do this, we can simply use \\npanda’s to_csv function, as shown on the final line of the snippet. \\nThis is generally a recommended practice for any preprocessing required \\nto train a model. Preprocessing code that runs right before the model \\noptimization process can slow down experimentation significantly. As \\nmuch as possible, always preprocess data ahead of time and serialize it to \\ndisk. \\nOnce we have our data in this format, we can examine the aspects we \\ndescribed earlier. The entire exploration process we detail next can be \\nfound in the dataset exploration notebook in this book’s GitHub \\nrepository. \\nTo start, we use df.info() to display summary information about our \\nDataFrame, as well as any empty values. Here is what it returns: \\n>>>> df.info() \\n \\nAcceptedAnswerId         4124 non-null float64 \\nAnswerCount              33650 non-null int64 \\nBody                     33650 non-null object \\nClosedDate               969 non-null object \\nCommentCount             33650 non-null int64 \\nCommunityOwnedDate       186 non-null object \\nCreationDate             33650 non-null object \\nFavoriteCount            3307 non-null float64 \\nId                       33650 non-null int64 \\nLastActivityDate         33650 non-null object \\nLastEditDate             10521 non-null object \\nLastEditorDisplayName    606 non-null object'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 86}, page_content='LastEditorUserId         9975 non-null float64 \\nOwnerDisplayName         1971 non-null object \\nOwnerUserId              32117 non-null float64 \\nParentId                 25679 non-null float64 \\nPostTypeId               33650 non-null int64 \\nScore                    33650 non-null int64 \\nTags                     7971 non-null object \\nTitle                    7971 non-null object \\nViewCount                7971 non-null float64 \\nbody_text                33650 non-null object \\nfull_text                33650 non-null object \\ntext_len                 33650 non-null int64 \\nis_question              33650 non-null bool \\nWe can see that we have a little over 31,000 posts, with only about 4,000 \\nof them having an accepted answer. In addition, we can notice that some \\nof the values for Body, which represents the contents of a post, are null, \\nwhich seems suspicious. We would expect all posts to contain text. \\nLooking at rows with a null Body quickly reveals they belong to a type of \\npost that has no reference in the documentation provided with the dataset, \\nso we remove them. \\nLet’s quickly dive into the format and see if we understand it. Each post \\nhas a PostTypeId value of 1 for a question, or 2 for an answer. We would \\nlike to see which type of questions receive high scores, as we would like to \\nuse a question’s score as a weak label for our true label, the quality of a \\nquestion. \\nFirst, let’s match questions with the associated answers. The following \\ncode selects all questions that have an accepted answer and joins them \\nwith the text for said answer. We can then look at the first few rows and \\nvalidate that the answers do match up with the questions. This will also \\nallow us to quickly look through the text and judge its quality. \\nquestions_with_accepted_answers = df[ \\n    df[\"is_question\"] & ~(df[\"AcceptedAnswerId\"].isna()) \\n] \\nq_and_a = questions_with_accepted_answers.join( \\n    df[[\"Text\"]], on=\"AcceptedAnswerId\", how=\"left\", rsuffix=\"_answer\" \\n) \\n \\npd.options.display.max_colwidth = 500 \\nq_and_a[[\"Text\", \"Text_answer\"]][:5] \\nIn Table 4-2, we can see that questions and answers do seem to match up \\nand that the text seems mostly correct. We now trust that we can match \\nquestions with their associated answers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 87}, page_content='Id \\nbody_text \\nbody_text_answer \\n1 \\nI’ve always wanted to start writing (in \\na totally amateur way), but whenever \\nI want to start something I instantly \\nget blocked having a lot of questions \\nand doubts.\\\\nAre there some \\nresources on how to start becoming a \\nwriter?\\\\nl’m thinking something with \\ntips and easy exercises to get the ball \\nrolling.\\\\n \\nWhen I’m thinking about where I learned most \\nhow to write, I think that reading was the most \\nimportant guide to me. This may sound silly, but \\nby reading good written newspaper articles (facts, \\nopinions, scientific articles, and most of all, \\ncriticisms of films and music), I learned how \\nothers did the job, what works and what doesn’t. \\nIn my own writing, I try to mimic other people’s \\nstyles that I liked. Moreover, I learn new things by \\nreading, giving me a broader background that I \\nneed when re… \\n2 \\nWhat kind of story is better suited for \\neach point of view? Are there \\nadvantages or disadvantages inherent \\nto them?\\\\nFor example, writing in the \\nfirst person you are always following \\na character, while in the third person \\nyou can “jump” between story \\nlines.\\\\n \\nWith a story in first person, you are intending the \\nreader to become much more attached to the main \\ncharacter. Since the reader sees what that \\ncharacter sees and feels what that character feels, \\nthe reader will have an emotional investment in \\nthat character. Third person does not have this \\nclose tie; a reader can become emotionally \\ninvested but it will not be as strong as it will be in \\nfirst person.\\\\nContrarily, you cannot have \\nmultiple point characters when you use first \\nperson without ex… \\n3 \\nI finished my novel, and everyone \\nI’ve talked to says I need an agent. \\nHow do I find one?\\\\n \\nTry to find a list of agents who write in your \\ngenre, check out their websites!\\\\nFind out if they \\nare accepting new clients. If they aren’t, then \\ncheck out another agent. But if they are, try \\nsending them a few chapters from your story, a \\nbrief, and a short cover letter asking them to \\nrepresent you.\\\\nIn the cover letter mention your \\nprevious publication credits. If sent via post, then \\nI suggest you give them a means of reply, whether \\nit be an email or a stamped, addressed \\nenvelope.\\\\nAgents… \\nTable 4-2. Questions with their associated answers'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 88}, page_content='As one last sanity check, let’s look at how many questions received no \\nanswer, how many received at least one, and how many had an answer that \\nwas accepted. \\nhas_accepted_answer = df[df[\"is_question\"] & \\n~(df[\"AcceptedAnswerId\"].isna())] \\nno_accepted_answers = df[ \\n    df[\"is_question\"] \\n    & (df[\"AcceptedAnswerId\"].isna()) \\n    & (df[\"AnswerCount\"] != 0) \\n] \\nno_answers = df[ \\n    df[\"is_question\"] \\n    & (df[\"AcceptedAnswerId\"].isna()) \\n    & (df[\"AnswerCount\"] == 0) \\n] \\n \\nprint( \\n    \"%s questions with no answers, %s with answers, %s with an accepted \\nanswer\" \\n    % (len(no_answers), len(no_accepted_answers), len(has_accepted_answer)) \\n) \\n \\n3584 questions with no answers, 5933 with answers, 4964 with an \\naccepted answer. \\nWe have a relatively even split between answered and partially answered \\nand unanswered questions. This seems reasonable, so we can feel \\nconfident enough to carry on with our exploration. \\nWe understand the format of our data and have enough of it to get started. \\nIf you are working on a project and your current dataset is either too small \\nor contains a majority of features that are too hard to interpret, you should \\ngather some more data or try a different dataset entirely. \\nOur dataset is of sufficient quality to proceed. It is now time to explore it \\nmore in depth, with the goal of informing our modeling strategy. \\nLabel to Find Data Trends \\nIdentifying trends in our dataset is about more than just quality. This part \\nof the work is about putting ourselves in the shoes of our model and trying \\nto predict what kind of structure it will pick up on. We will do this by \\nseparating data into different clusters (I will explain clustering \\nin “Clustering”) and trying to extract commonalities in each cluster.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 89}, page_content='The following is a step-by-step list to do this in practice. We’ll start with \\ngenerating summary statistics of our dataset and then see how to rapidly \\nexplore it by leveraging vectorization techniques. With the help of \\nvectorization and clustering, we’ll explore our dataset efficiently. \\nSummary Statistics \\nWhen you start looking at a dataset, it is generally a good idea to look at \\nsome summary statistics for each of the features you have. This helps you \\nboth get a general sense for the features in your dataset and identify any \\neasy way to separate your classes. \\nIdentifying differences in distributions between classes of data early is \\nhelpful in ML, because it will either make our modeling task easier or \\nprevent us from overestimating the performance of a model that may just \\nbe leveraging one particularly informative feature. \\nFor example, if you are trying to predict whether tweets are expressing a \\npositive or negative opinion, you could start by counting the average \\nnumber of words in each tweet. You could then plot a histogram of this \\nfeature to learn about its distribution. \\nA histogram would allow you to notice if all positive tweets were shorter \\nthan negative ones. This could lead you to add word length as a predictor \\nto make your task easier or on the contrary gather additional data to make \\nsure that your model can learn about the content of the tweets and not just \\ntheir length. \\nLet’s plot a few summary statistics for our ML editor to illustrate this \\npoint. \\nSUMMARY STATISTICS FOR ML EDITOR \\nFor our example, we can plot a histogram of the length of questions in our \\ndataset, highlighting the different trends between high- and low-score \\nquestions. Here is how we do this using pandas: \\nimport matplotlib.pyplot as plt \\nfrom matplotlib.patches import Rectangle \\n \\n\"\"\" \\ndf contains questions and their answer counts from \\nwriters.stackexchange.com \\nWe draw two histograms:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 90}, page_content='one for questions with scores under the median score \\none for questions with scores over \\nFor both, we remove outliers to make our visualization simpler \\n\"\"\" \\n \\nhigh_score = df[\"Score\"] > df[\"Score\"].median() \\n# We filter out really long questions \\nnormal_length = df[\"text_len\"] < 2000 \\n \\nax = df[df[\"is_question\"] & high_score & normal_length][\"text_len\"].hist( \\n    bins=60, \\n    density=True, \\n    histtype=\"step\", \\n    color=\"orange\", \\n    linewidth=3, \\n    grid=False, \\n    figsize=(16, 10), \\n) \\n \\ndf[df[\"is_question\"] & ~high_score & normal_length][\"text_len\"].hist( \\n    bins=60, \\n    density=True, \\n    histtype=\"step\", \\n    color=\"purple\", \\n    linewidth=3, \\n    grid=False, \\n) \\n \\nhandles = [ \\n    Rectangle((0, 0), 1, 1, color=c, ec=\"k\") for c in [\"orange\", \"purple\"] \\n] \\nlabels = [\"High score\", \"Low score\"] \\nplt.legend(handles, labels) \\nax.set_xlabel(\"Sentence length (characters)\") \\nax.set_ylabel(\"Percentage of sentences\") \\nWe can see in Figure 4-2 that the distributions are mostly similar, with \\nhigh-score questions tending to be slightly longer (this trend is especially \\nnoticeable around the 800-character mark). This is an indication that \\nquestion length may be a useful feature for a model to predict a question’s \\nscore. \\nWe can plot other variables in a similar fashion to identify more potential \\nfeatures. Once we’ve identified a few features, let’s look at our dataset a \\nlittle more closely so that we can identify more granular trends.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 91}, page_content='Figure 4-2. Histogram of the length of text for high- and low-score questions \\nExplore and Label Efficiently \\nYou can only get so far looking at descriptive statistics such as averages \\nand plots such as histograms. To develop an intuition for your data, you \\nshould spend some time looking at individual data points. However, going \\nthrough points in a dataset at random is quite inefficient. In this section, \\nI’ll cover how to maximize your efficiency when visualizing individual \\ndata points. \\nClustering is a useful method to use here. Clustering is the task of \\ngrouping a set of objects in such a way that objects in the same group \\n(called a cluster) are more similar (in some sense) to each other than to \\nthose in other groups (clusters). We will use clustering both for exploring \\nour data and for our model predictions later (see “Dimensionality \\nreduction”). \\nMany clustering algorithms group data points by measuring the distance \\nbetween points and assigning ones that are close to each other to the same \\ncluster. Figure 4-3 shows an example of a clustering algorithm separating \\na dataset into three different clusters. Clustering is an unsupervised \\nmethod, and there is often no single correct way to cluster a dataset. In this \\nbook, we will use clustering as a way to generate some structure to guide \\nour exploration.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 92}, page_content='Because clustering relies on calculating the distance between data points, \\nthe way we choose to represent our data points numerically has a large \\nimpact on which clusters are generated. We will dive into this in the next \\nsection, “Vectorizing”. \\n \\nFigure 4-3. Generating three clusters from a dataset \\nThe vast majority of datasets can be separated into clusters based on their \\nfeatures, labels, or a combination of both. Examining each cluster \\nindividually and the similarities and differences between clusters is a great \\nway to identify structure in a dataset. \\nThere are multiple things to look out for here: \\n\\uf0b7 How many clusters do you identify in your dataset? \\n\\uf0b7 Do each of these clusters seem different to you? In which way?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 93}, page_content='\\uf0b7 Are any clusters much more dense than others? If so, your model is \\nlikely to struggle to perform on the sparser areas. Adding features \\nand data can help alleviate this problem. \\n\\uf0b7 Do all clusters represent data that seems as “hard” to model? If some \\nclusters seem to represent more complex data points, make note of \\nthem so you can revisit them when we evaluate our model’s \\nperformance. \\nAs we mentioned, clustering algorithms work on vectors, so we can’t \\nsimply pass a set of sentences to a clustering algorithm. To get our data \\nready to be clustered, we will first need to vectorize it. \\nVECTORIZING \\nVectorizing a dataset is the process of going from the raw data to a vector \\nthat represents it. Figure 4-4 shows an example of vectorized \\nrepresentations for text and tabular data. \\n \\nFigure 4-4. Examples of vectorized representations \\nThere are many ways to vectorize data, so we will focus on a few simple \\nmethods that work for some of the most common data types, such as \\ntabular data, text, and images. \\nTabular data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 94}, page_content='For tabular data consisting of both categorical and continuous features, a \\npossible vector representation is simply the concatenation of the vector \\nrepresentations of each feature. \\nContinuous features should be normalized to a common scale so that \\nfeatures with larger scale do not cause smaller features to be completely \\nignored by models. There are various way to normalize data, but starting \\nby transforming each feature such that its mean is zero and variance one is \\noften a good first step. This is often referred to as a standard score. \\nCategorical features such as colors can be converted to a one-hot \\nencoding: a list as long as the number of distinct values of the feature \\nconsisting of only zeros and a single one, whose index represents the \\ncurrent value (for example, in a dataset containing four distinct colors, we \\ncould encode red as [1, 0, 0, 0] and blue as [0, 0, 1, 0]). You may be \\ncurious as to why we wouldn’t simply assign each potential value a \\nnumber, such as 1 for red and 3 for blue. It is because such an encoding \\nscheme would imply an ordering between values (blue is larger than red), \\nwhich is often incorrect for categorical variables. \\nA property of one-hot encoding is that the distance between any two given \\nfeature values is always one. This often provides a good representation for \\na model, but in some cases such as days of the week, some values may be \\nmore similar than the others (Saturday and Sunday are both in the \\nweekend, so ideally their vectors would be closer together than \\nWednesday and Sunday, for example). Neural networks have started \\nproving themselves useful at learning such representations (see the \\npaper “Entity Embeddings of Categorical Variables”, by C. Guo and F. \\nBerkhahn). These representations have been shown to improve the \\nperformance of models using them instead of other encoding schemes. \\nFinally, more complex features such as dates should be transformed in a \\nfew numerical features capturing their salient characteristics. \\nLet’s go through a practical example of vectorization for tabular data. You \\ncan find the code for the example in the tabular data vectorization \\nnotebook in this book’s GitHub repository. \\nLet’s say that instead of looking at the content of questions, we want to \\npredict the score a question will get from its tags, number of comments,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 95}, page_content='and creation date. In Table 4-3, you can see an example of what this \\ndataset would look like for the writers.stackexchange.com dataset. \\nId \\nTags \\nCommentCount \\nCreationDate \\nScore \\n1 \\n<resources><first-time-author> \\n7 \\n2010-11-18T20:40:32.857 \\n32 \\n2 \\n<fiction><grammatical-\\nperson><third-person> \\n0 \\n2010-11-18T20:42:31.513 \\n20 \\n3 \\n<publishing><novel><agent> \\n1 \\n2010-11-18T20:43:28.903 \\n34 \\n5 \\n<plot><short-\\nstory><planning><brainstorming> \\n0 \\n2010-11-18T20:43:59.693 \\n28 \\n7 \\n<fiction><genre><categories> \\n1 \\n2010-11-18T20:45:44.067 \\n21 \\nTable 4-3. Tabular inputs without any processing \\nEach question has multiple tags, as well as a date and a number of \\ncomments. Let’s preprocess each of these. First, we normalize numerical \\nfields: \\ndef get_norm(df, col): \\n    return (df[col] - df[col].mean()) / df[col].std() \\n \\ntabular_df[\"NormComment\"]= get_norm(tabular_df, \"CommentCount\") \\ntabular_df[\"NormScore\"]= get_norm(tabular_df, \"Score\") \\nThen, we extract relevant information from the date. We could, for \\nexample, choose the year, month, day, and hour of posting. Each of these \\nis a numerical value our model can use. \\n# Convert our date to a pandas datetime \\ntabular_df[\"date\"] = pd.to_datetime(tabular_df[\"CreationDate\"]) \\n \\n# Extract meaningful features from the datetime object \\ntabular_df[\"year\"] = tabular_df[\"date\"].dt.year \\ntabular_df[\"month\"] = tabular_df[\"date\"].dt.month \\ntabular_df[\"day\"] = tabular_df[\"date\"].dt.day \\ntabular_df[\"hour\"] = tabular_df[\"date\"].dt.hour \\nOur tags are categorical features, with each question potentially being \\ngiven any number of tags. As we saw earlier, the easiest way to represent'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 96}, page_content='categorical inputs is to one-hot encode them, transforming each tag into its \\nown column, with each question having a value of 1 for a given tag feature \\nonly if that tag is associated to this question. \\nBecause we have more than three hundred tags in our dataset, here we \\nchose to only create a column for the five most popular ones that are used \\nin more than five hundred questions. We could add every single tag, but \\nbecause the majority of them appear only once, this would not be helpful \\nto identify patterns. \\n# Select our tags, represented as strings, and transform them into \\narrays of tags \\ntags = tabular_df[\"Tags\"] \\nclean_tags = tags.str.split(\"><\").apply( \\n    lambda x: [a.strip(\"<\").strip(\">\") for a in x]) \\n \\n# Use pandas\\' get_dummies to get dummy values \\n# select only tags that appear over 500 times \\ntag_columns = \\npd.get_dummies(clean_tags.apply(pd.Series).stack()).sum(level=0) \\nall_tags = \\ntag_columns.astype(bool).sum(axis=0).sort_values(ascending=False) \\ntop_tags = all_tags[all_tags > 500] \\ntop_tag_columns = tag_columns[top_tags.index] \\n \\n# Add our tags back into our initial DataFrame \\nfinal = pd.concat([tabular_df, top_tag_columns], axis=1) \\n \\n# Keeping only the vectorized features \\ncol_to_keep = [\"year\", \"month\", \"day\", \"hour\", \"NormComment\", \\n               \"NormScore\"] + list(top_tags.index) \\nfinal_features = final[col_to_keep] \\nIn Table 4-4, you can see that our data is now fully vectorized, with each \\nrow consisting only of numeric values. We can feed this data to a \\nclustering algorithm, or a supervised ML model. \\nId \\nYear \\nMonth \\nDay \\nHour \\nNorm-\\nComment \\nNorm-\\nScore \\nCreative \\nwriting \\nFiction \\nStyle \\nChar-\\nacters \\nTech-\\nnique \\n1 \\n2010 11 \\n18 \\n20 \\n0.165706 \\n0.140501 \\n0 \\n0 \\n0 \\n0 \\n0 \\n2 \\n2010 11 \\n18 \\n20 \\n-0.103524 \\n0.077674 \\n0 \\n1 \\n0 \\n0 \\n0 \\n3 \\n2010 11 \\n18 \\n20 \\n-0.065063 \\n0.150972 \\n0 \\n0 \\n0 \\n0 \\n0 \\n5 \\n2010 11 \\n18 \\n20 \\n-0.103524 \\n0.119558 \\n0 \\n0 \\n0 \\n0 \\n0'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 97}, page_content='Id \\nYear \\nMonth \\nDay \\nHour \\nNorm-\\nComment \\nNorm-\\nScore \\nCreative \\nwriting \\nFiction \\nStyle \\nChar-\\nacters \\nTech-\\nnique \\n7 \\n2010 11 \\n18 \\n20 \\n-0.065063 \\n0.082909 \\n0 \\n1 \\n0 \\n0 \\n0 \\nTable 4-4. Vectorized tabular inputs \\nVECTORIZATION AND DATA LEAKAGE \\nYou would usually use the same techniques to vectorize data to visualize it and to \\nfeed it to a model. There is an important distinction, however. When you vectorize \\ndata to feed it to a model, you should vectorize your training data and save the \\nparameters you used to obtain the training vectors. You should then use the same \\nparameters for your validation and test sets. \\nWhen normalizing data, for example, you should compute summary statistics such \\nas mean and standard deviation only on your training set (using the same values to \\nnormalize your validation data), and during inference in production. \\nUsing both your validation and training data for normalization, or to decide which \\ncategories to keep in your one-hot encoding, would cause data leakage, as you \\nwould be leveraging information from outside your training set to create training \\nfeatures. This would artificially inflate your model’s performance but make it \\nperform worse in production. We will cover this in more detail in “Data leakage”. \\nDifferent types of data call for different vectorization methods. In \\nparticular, text data often requires more creative approaches. \\nText data \\nThe simplest way to vectorize text is to use a count vector, which is the \\nword equivalent of one-hot encoding. Start by constructing a vocabulary \\nconsisting of the list of unique words in your dataset. Associate each word \\nin our vocabulary to an index (from 0 to the size of our vocabulary). You \\ncan then represent each sentence or paragraph by a list as long as our \\nvocabulary. For each sentence, the number at each index represents the \\ncount of occurrences of the associated word in the given sentence. \\nThis method ignores the order of the words in a sentence and so is referred \\nto as a bag of words. Figure 4-5 shows two sentences and their bag-of-\\nwords representations. Both sentences are transformed into vectors that'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 98}, page_content='contain information about the number of times a word occurs in a \\nsentence, but not the order in which words are present in the sentence. \\n \\nFigure 4-5. Getting bag-of-words vectors from sentences \\nUsing a bag-of-words representation or its normalized version TF-IDF \\n(short for Term Frequency–Inverse Document Frequency) is simple using \\nscikit-learn, as you can see here: \\n# Create an instance of a tfidf vectorizer, \\n# We could use CountVectorizer for a non normalized version \\nvectorizer = TfidfVectorizer() \\n \\n# Fit our vectorizer to questions in our dataset \\n# Returns an array of vectorized text \\nbag_of_words = vectorizer.fit_transform(df[df[\"is_question\"]][\"Text\"]) \\nMultiple novel text vectorization methods have been developed over the \\nyears, starting in 2013 with Word2Vec (see the paper, “Efficient \\nEstimation of Word Representations in Vector Space,” by Mikolov et \\nal.) and more recent approaches such as fastText (see the paper, “Bag of \\nTricks for Efficient Text Classification,” by Joulin et al.). These \\nvectorization techniques produce word vectors that attempt to learn a \\nrepresentation that captures similarities between concepts better than a TF-\\nIDF encoding. They do this by learning which words tend to appear in \\nsimilar contexts in large bodies of text such as Wikipedia. This approach is \\nbased on the distributional hypothesis, which claims that linguistic items \\nwith similar distributions have similar meanings.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 99}, page_content='Concretely, this is done by learning a vector for each word and training a \\nmodel to predict a missing word in a sentence using the word vectors of \\nwords around it. The number of neighboring words to take into account is \\ncalled the window size. In Figure 4-6, you can see a depiction of this task \\nfor a window size of two. On the left, the word vectors for the two words \\nbefore and after the target are fed to a simple model. This simple model \\nand the values of the word vectors are then optimized so that the output \\nmatches the word vector of the missing word. \\n \\nFigure 4-6. Learning word vectors, from the Word2Vec paper “Efficient Estimation of Word \\nRepresentations in Vector Space” by Mikolov et al. \\nMany open source pretrained word vectorizing models exist. Using vectors \\nproduced by a model that was pretrained on a large corpus (oftentimes \\nWikipedia or an archive of news stories) can help our models leverage the \\nsemantic meaning of common words better. \\nFor example, the word vectors mentioned in the Joulin et al. fastText paper \\nare available online in a standalone tool. For a more \\ncustomized approach, spaCy is an NLP toolkit that provides pretrained \\nmodels for a variety of tasks, as well as easy ways to build your own. \\nHere is an example of using spaCy to load pretrained word vectors and \\nusing them to get a semantically meaningful sentence vector. Under the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 100}, page_content='hood, spaCy retrieves the pretrained value for each word in our dataset (or \\nignores it if it was not part of its pretraining task) and averages all vectors \\nin a question to get a representation of the question. \\nimport spacy \\n \\n# We load a large model, and disable pipeline unnecessary parts for our \\ntask \\n# This speeds up the vectorization process significantly \\n# See https://spacy.io/models/en#en_core_web_lg for details about the \\nmodel \\nnlp = spacy.load(\\'en_core_web_lg\\', disable=[\"parser\", \"tagger\", \"ner\", \\n      \"textcat\"]) \\n \\n# We then simply get the vector for each of our questions \\n# By default, the vector returned is the average of all vectors in the \\nsentence \\n# See https://spacy.io/usage/vectors-similarity for more \\nspacy_emb = df[df[\"is_question\"]][\"Text\"].apply(lambda x: nlp(x).vector) \\nTo see a comparison of a TF-IDF model with pretrained word embeddings \\nfor our dataset, please refer to the vectorizing text notebook in the book’s \\nGitHub repository. \\nSince 2018, word vectorization using large language models on even \\nlarger datasets has started producing the most accurate results (see the \\npapers “Universal Language Model Fine-Tuning for Text Classification”, \\nby J. Howard and S. Ruder, and “BERT: Pre-training of Deep \\nBidirectional Transformers for Language Understanding”, by J. Devlin et \\nal.). These large models, however, do come with the drawback of being \\nslower and more complex than simple word embeddings. \\nFinally, let’s examine vectorization for another commonly used type of \\ndata, images. \\nImage data \\nImage data is already vectorized, as an image is nothing more but a \\nmultidimensional array of numbers, often referred to in the ML \\ncommunity as tensors. Most standard three-channel RGB images, for \\nexample, are simply stored as a list of numbers of length equal to the \\nheight of the image in pixels, multiplied by its width, multiplied by three \\n(for the red, green, and blue channels). In Figure 4-7, you can see how we \\ncan represent an image as a tensor of numbers, representing the intensity \\nof each of the three primary colors.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 101}, page_content='While we can use this representation as is, we would like our tensors to \\ncapture a little more about the semantic meaning of our images. To do this, \\nwe can use an approach similar to the one for text and leverage large \\npretrained neural networks. \\nModels that have been trained on massive classification datasets such \\nas VGG (see the paper by A. Simonyan and A. Zimmerman, “Very Deep \\nConvolutional Networks for Large-Scale Image \\nRecognition”) or Inception (see the paper by C. Szegedy et al., “Going \\nDeeper with Convolutions”) on the ImageNet dataset end up learning very \\nexpressive representations in order to classify well. These models mostly \\nfollow a similar high-level structure. The input is an image that passes \\nthrough many successive layers of computation, each generating a \\ndifferent representation of said image. \\nFinally, the penultimate layer is passed to a function that generates \\nclassification probabilities for each class. This penultimate layer thus \\ncontains a representation of the image that is sufficient to classify which \\nobject it contains, which makes it a useful representation for other tasks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 102}, page_content='Figure 4-7. Representing a 3 as a matrix of values from 0 to 1 (only showing the red channel) \\nExtracting this representation layer proves to work extremely well at \\ngenerating meaningful vectors for images. This requires no custom work \\nother than loading the pretrained model. In Figure 4-8 each rectangle \\nrepresents a different layer for one of those pretrained models. The most \\nuseful representation is highlighted. It is usually located just before the \\nclassification layer, since that is the representation that needs to summarize \\nthe image best for the classifier to perform well.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 103}, page_content='Figure 4-8. Using a pretrained model to vectorize images \\nUsing modern libraries such as Keras makes this task much easier. Here is \\na function that loads images from a folder and transforms them into \\nsemantically meaningful vectors for downstream analysis, using a \\npretrained network available in Keras: \\nimport numpy as np \\n \\nfrom keras.preprocessing import image \\nfrom keras.models import Model \\nfrom keras.applications.vgg16 import VGG16 \\nfrom keras.applications.vgg16 import preprocess_input \\n \\n \\ndef generate_features(image_paths): \\n    \"\"\" \\n    Takes in an array of image paths \\n    Returns pretrained features for each image \\n    :param image_paths: array of image paths \\n    :return: array of last-layer activations, \\n    and mapping from array_index to file_path \\n    \"\"\" \\n \\n    images = np.zeros(shape=(len(image_paths), 224, 224, 3)) \\n \\n    # loading a  pretrained model \\n    pretrained_vgg16 = VGG16(weights=\\'imagenet\\', include_top=True) \\n \\n    # Using only the penultimate layer, to leverage learned features \\n    model = Model(inputs=pretrained_vgg16.input, \\n                  outputs=pretrained_vgg16.get_layer(\\'fc2\\').output) \\n \\n    # We load all our dataset in memory (works for small datasets) \\n    for i, f in enumerate(image_paths): \\n        img = image.load_img(f, target_size=(224, 224)) \\n        x_raw = image.img_to_array(img) \\n        x_expand = np.expand_dims(x_raw, axis=0) \\n        images[i, :, :, :] = x_expand \\n \\n    # Once we\\'ve loaded all our images, we pass them to our model \\n    inputs = preprocess_input(images)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 104}, page_content='images_features = model.predict(inputs) \\n    return images_features \\nTRANSFER LEARNING \\nPretrained models are useful to vectorize our data, but they can also sometimes be \\nentirely adapted to our task. Transfer learning is the process of using a model that \\nwas previously trained on one dataset or task for a different dataset or task. More \\nthan simply reusing the same architecture or pipeline, transfer learning uses the \\npreviously learned weights of a trained model as a starting point for a new task. \\nTransfer learning can in theory work from any task to any other, but it is \\ncommonly used to improve performance on smaller datasets, by transferring \\nweights from large datasets such as ImageNet for computer vision or WikiText for \\nNLP. \\nWhile transfer learning often improves performance, it also may introduce an \\nadditional source of unwanted bias. Even if you clean your current dataset \\ncarefully, if you use a model that was pretrained on the entirety of Wikipedia, for \\nexample, it could carry over the gender bias shown to be present there (see the \\narticle “Gender Bias in Neural Natural Language Processing,” by K. Lu et al.). \\nOnce you have a vectorized representation, you can cluster it or pass your \\ndata to a model, but you can also use it to more efficiently inspect your \\ndataset. By grouping data points with similar representations together, you \\ncan more quickly look at trends in your dataset. We’ll see how to do this \\nnext. \\nDIMENSIONALITY REDUCTION \\nHaving vector representations is necessary for algorithms, but we can also \\nleverage those representations to visualize data directly! This may seem \\nchallenging, because the vectors we described are often in more than two \\ndimensions, which makes them challenging to display on a chart. How \\ncould we display a 14-dimensional vector? \\nGeoffrey Hinton, who won a Turing Award for his work in deep learning, \\nacknowledges this problem in his lecture with the following tip: “To deal \\nwith hyper-planes in a 14-dimensional space, visualize a 3D space and \\nsay fourteen to yourself very loudly. Everyone does it.” (See slide 16 from \\nG. Hinton et al.’s lecture, “An Overview of the Main Types of Neural \\nNetwork Architecture” here.) If this seems hard to you, you’ll be excited \\nto hear about dimensionality reduction, which is the technique of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 105}, page_content=\"representing vectors in fewer dimensions while preserving as much about \\ntheir structure as possible. \\nDimensionality reduction techniques such as t-SNE (see the paper by L. \\nvan der Maaten and G. Hinton, PCA, “Visualizing Data Using t-SNE”), \\nand UMAP (see the paper by L. McInnes et al, “UMAP: Uniform \\nManifold Approximation and Projection for Dimension Reduction”) allow \\nyou to project high-dimensional data such as vectors representing \\nsentences, images, or other features on a 2D plane. \\nThese projections are useful to notice patterns in data that you can then \\ninvestigate. They are approximate representations of the real data, \\nhowever, so you should validate any hypothesis you make from looking at \\nsuch a plot by using other methods. If you see clusters of points all \\nbelonging to one class that seem to have a feature in common, check that \\nyour model is actually leveraging that feature, for example. \\nTo get started, plot your data using a dimensionality reduction technique \\nand color each point by an attribute you are looking to inspect. For \\nclassification tasks, start by coloring each point based on its label. For \\nunsupervised tasks, you can color points based on the values of given \\nfeatures you are looking at, for example. This allows you to see whether \\nany regions seem like they will be easy for your model to separate, or \\ntrickier. \\nHere is how to do this easily using UMAP, passing it embeddings we \\ngenerated in “Vectorizing”: \\nimport umap \\n \\n# Fit UMAP to our data, and return the transformed data \\numap_emb = umap.UMAP().fit_transform(embeddings) \\n \\nfig = plt.figure(figsize=(16, 10)) \\ncolor_map = { \\n    True: '#ff7f0e', \\n    False:'#1f77b4' \\n} \\nplt.scatter(umap_emb[:, 0], umap_emb[:, 1], \\n            c=[color_map[x] for x in sent_labels], \\n            s=40, alpha=0.4) \\nAs a reminder, we decided to start with using only data from the writers’ \\ncommunity of Stack Exchange. The result for this dataset is displayed \\non Figure 4-9. At first glance, we can see a few regions we should explore,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 106}, page_content='such as the dense region of unanswered questions on the top left. If we can \\nidentify which features they have in common, we may discover a useful \\nclassification feature. \\nAfter data is vectorized and plotted, it is generally a good idea to start \\nsystematically identifying groups of similar data points and explore them. \\nWe could do this simply by looking at UMAP plots, but we can also \\nleverage clustering. \\n \\nFigure 4-9. UMAP plot colored by whether a given question was successfully answered  \\nCLUSTERING \\nWe mentioned clustering earlier as a method to extract structure from data. \\nWhether you are clustering data to inspect a dataset or using it to analyze a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 107}, page_content='model’s performance as we will do in Chapter 5, clustering is a core tool \\nto have in your arsenal. I use clustering in a similar fashion as \\ndimensionality reduction, as an additional way to surface issues and \\ninteresting data points. \\nA simple method to cluster data in practice is to start by trying a few \\nsimple algorithms such as k-means and tweak their hyperparameters such \\nas the number of clusters until you reach a satisfactory performance. \\nClustering performance is hard to quantify. In practice, using a \\ncombination of data visualization and methods such as the elbow \\nmethod or a silhouette plot is sufficient for our use case, which is not to \\nperfectly separate our data but to identify regions where our model may \\nhave issues. \\nThe following is an example snippet of code for clustering our dataset, as \\nwell as visualizing our clusters using a dimensionality technique we \\ndescribed earlier, UMAP. \\nfrom sklearn.cluster import KMeans \\nimport matplotlib.cm as cm \\n \\n# Choose number of clusters and colormap \\nn_clusters=3 \\ncmap = plt.get_cmap(\"Set2\") \\n \\n# Fit clustering algorithm to our vectorized features \\nclus = KMeans(n_clusters=n_clusters, random_state=10) \\nclusters = clus.fit_predict(vectorized_features) \\n \\n# Plot the dimentionality reduced features on a 2D plane \\nplt.scatter(umap_features[:, 0], umap_features[:, 1], \\n            c=[cmap(x/n_clusters) for x in clusters], s=40, alpha=.4) \\nplt.title(\\'UMAP projection of questions, colored by clusters\\', \\nfontsize=14) \\nAs you can see in Figure 4-10, the way we would instinctively cluster the \\n2D representation does not always match with the clusters our algorithm \\nfinds on the vectorized data. This can be because of artifacts in our \\ndimensionality reduction algorithm or a complex data topology. In fact, \\nadding a point’s assigned cluster as a feature can sometimes improve a \\nmodel’s performance by letting it leverage said topology. \\nOnce you have clusters, examine each cluster and try to identify trends in \\nyour data on each of them. To do so, you should select a few points per \\ncluster and act as if you were the model, thus labeling those points with'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 108}, page_content='what you think the correct answer should be. In the next section, I’ll \\ndescribe how to do this labeling work. \\n \\nFigure 4-10. Visualizing our questions, colored by cluster \\nBe the Algorithm \\nOnce you’ve looked at aggregate metrics and cluster information, I’d \\nencourage you to follow the advice in “Monica Rogati: How to Choose \\nand Prioritize ML Projects” and try to do your model’s job by labeling a \\nfew data points in each cluster with the results you would like a model to \\nproduce. \\nIf you have never tried doing your algorithm’s job, it will be hard to judge \\nthe quality of its results. On the other side, if you spend some time labeling \\ndata yourself, you will often notice trends that will make your modeling \\ntask much easier. \\nYou might recognize this advice from our previous section about \\nheuristics, and it should not surprise you. Choosing a modeling approach \\ninvolves making almost as many assumptions about our data as building \\nheuristics, so it makes sense for these assumptions to be data driven. \\nYou should label data even if your dataset contains labels. This allows you \\nto validate that your labels do capture the correct information and that they'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 109}, page_content='are correct. In our case study, we use a question’s score as a measure of its \\nquality, which is a weak label. Labeling a few examples ourselves will \\nallow us to validate the assumption that this label is appropriate. \\nOnce you label a few examples, feel free to update your vectorization \\nstrategy by adding any features you discover to help make your data \\nrepresentation as informative as possible, and go back to labeling. This is \\nan iterative process, as illustrated in Figure 4-11. \\n \\nFigure 4-11. The process of labeling data \\nTo speed up your labeling, make sure to leverage your prior analysis by \\nlabeling a few data points in each cluster you have identified and for each \\ncommon value in your feature distribution. \\nOne way to do this is to leverage visualization libraries to interactively \\nexplore your data. Bokeh offers the ability to make interactive plots. One'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 110}, page_content='quick way to label data is to go through a plot of our vectorized examples, \\nlabeling a few examples for each cluster. \\nFigure 4-12 shows a representative individual example from a cluster of \\nmostly unanswered questions. Questions in this cluster tended to be quite \\nvague and hard to answer objectively and did not receive answers. These \\nare accurately labeled as poor questions. To see the source code for this \\nplot and an example of its use for the ML Editor, navigate to the exploring \\ndata to generate features notebook in this book’s GitHub repository. \\n \\nFigure 4-12. Using Bokeh to inspect and label data \\nWhen labeling data, you can choose to store labels with the data itself (as \\nan additional column in a DataFrame, for example) or separately using a \\nmapping from file or identifier to label. This is purely a matter of \\npreference. \\nAs you label examples, try to notice which process you are using to make \\nyour decisions. This will help with identifying trends and generating \\nfeatures that will help your models. \\nData Trends \\nAfter having labeled data for a while, you will usually identify trends. \\nSome may be informative (short tweets tend to be simpler to classify as \\npositive or negative) and guide you to generate useful features for your \\nmodels. Others may be irrelevant correlations because of the way data was \\ngathered. \\nMaybe all of the tweets we collected that are in French happen to be \\nnegative, which would likely lead a model to automatically classify French'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 111}, page_content='tweets as negative. I’ll let you decide how inaccurate that might be on a \\nbroader, more representative sample. \\nIf you notice anything of the sort, do not despair! These kinds of trends are \\ncrucial to identify before you start building models, as they would \\nartificially inflate accuracy on training data and could lead you to put a \\nmodel in production that does not perform well. \\nThe best way to deal with such biased examples is to gather additional data \\nto make your training set more representative. You could also try to \\neliminate these features from your training data to avoid biasing your \\nmodel, but this may not be effective in practice, as models frequently pick \\nup on bias by leveraging correlations with other features (see Chapter 8). \\nOnce you’ve identified some trends, it is time to use them. Most often, you \\ncan do this in one of two ways, by creating a feature that characterizes that \\ntrend or by using a model that will easily leverage it. \\nLet Data Inform Features and Models \\nWe would like to use the trends we discover in the data to inform our data \\nprocessing, feature generation, and modeling strategy. To start, let’s look \\nat how we could generate features that would help us capture these trends. \\nBuild Features Out of Patterns \\nML is about using statistical learning algorithms to leverage patterns in the \\ndata, but some patterns are easier to capture for models than others. \\nImagine the trivial example of predicting a numerical value using the value \\nitself divided by 2 as a feature. The model would simply have to learn to \\nmultiply by 2 to predict the target perfectly. On the other hand, predicting \\nthe stock market from historical data is a problem that requires leveraging \\nmuch more complex patterns. \\nThis is why a lot of the practical gains of ML come from generating \\nadditional features that will help our models identify useful patterns. The \\nease with which a model identifies patterns depends on the way we \\nrepresent data and how much of it we have. The more data you have and \\nthe less noisy your data is, the less feature engineering work you usually \\nhave to do.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 112}, page_content='It is often valuable to start by generating features, however; first because \\nwe will usually be starting with a small dataset and second because it helps \\nencode our beliefs about the data and debug our models. \\nSeasonality is a common trend that benefits from specific feature \\ngeneration. Let’s say that an online retailer noticed that most of their sales \\nhappens on the last two weekends of the month. When building a model to \\npredict future sales, they want to make sure that it has the potential to \\ncapture this pattern. \\nAs you’ll see, depending on how they represent dates, the task could prove \\nquite difficult for their models. Most models are only able to take \\nnumerical inputs (see “Vectorizing” for methods to transform text and \\nimages into numerical inputs), so let’s examine a few ways to represent \\ndates. \\nRAW DATETIME \\nThe simplest way to represent time is in Unix time, which represents “the \\nnumber of seconds that have elapsed since 00:00:00 Thursday, 1 January \\n1970.” \\nWhile this representation is simple, our model would need to learn some \\npretty complex patterns to identify the last two weekends of the month. \\nThe last weekend of 2018, for example (from 00:00:00 on the 29th to \\n23:59:59 on the 30th of December), is represented in Unix time as the \\nrange from 1546041600 to 1546214399 (you can verify that if you take the \\ndifference between both numbers, which represents an interval of 23 \\nhours, 59 minutes, and 59 seconds measured in seconds). \\nNothing about this range makes it particularly easy to relate to other \\nweekends in other months, so it will be quite hard for a model to separate \\nrelevant weekends from others when using Unix time as an input. We can \\nmake the task easier for a model by generating features. \\nEXTRACTING DAY OF WEEK AND DAY OF MONTH \\nOne way to make our representation of dates clearer would be to extract \\nthe day of the week and day of the month into two separate attributes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 113}, page_content='The way we would represent 23:59:59 on the 30th of December, 2018, for \\nexample, would be with the same number as earlier, and two additional \\nvalues representing the day of the week (0 for Sunday, for example) and \\nday of the month (30). \\nThis representation will make it easier for our model to learn that the \\nvalues related to weekends (0 and 6 for Sunday and Saturday) and to later \\ndates in the month correspond to higher activity. \\nIt is also important to note that representations will often introduce bias to \\nour model. For example, by encoding the day of the week as a number, the \\nencoding for Friday (equal to five) will be five times greater than the one \\nfor Monday (equal to one). This numerical scale is an artifact of our \\nrepresentation and does not represent something we wish our model to \\nlearn. \\nFEATURE CROSSES \\nWhile the previous representation makes the task easier for our models, \\nthey would still have to learn a complex relationship between the day of \\nthe week and the day of the month: high traffic does not happen on \\nweekends early in the month or on weekdays late in the month. \\nSome models such as deep neural networks leverage nonlinear \\ncombinations of features and can thus pick up on these relationships, but \\nthey often need a significant amount of data. A common way to address \\nthis problem is by making the task even easier and introducing feature \\ncrosses. \\nA feature cross is a feature generated simply by multiplying (crossing) two \\nor more features with each other. This introduction of a nonlinear \\ncombination of features allows our model to discriminate more easily \\nbased on a combination of values from multiple features. \\nIn Table 4-5, you can see how each of the representations we described \\nwould look for a few example data points.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 114}, page_content='Human representation \\nRaw data (Unix \\ndatetime) \\nDay of week \\n(DoW) \\nDay of month \\n(DoM) \\nCross (DoW / \\nDoM) \\nSaturday, December 29, 2018, \\n00:00:00 \\n1,546,041,600 \\n7 \\n29 \\n174 \\nSaturday, December 29, 2018, \\n01:00:00 \\n1,546,045,200 \\n7 \\n29 \\n174 \\n… \\n… \\n… \\n… \\n… \\nSunday, December 30, 2018, \\n23:59:59 \\n1,546,214,399 \\n1 \\n30 \\n210 \\nTable 4-5. Representing your data in a clearer way will make it much easier for your algorithms \\nto perform well \\nIn Figure 4-13, you can see how these feature values change with time and \\nwhich ones make it simpler for a model to separate specific data points \\nfrom others.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 115}, page_content='Figure 4-13. The last weekends of the month are easier to separate using feature crosses and \\nextracted features \\nThere is one last way to represent our data that will make it even easier for \\nour model to learn the predictive value of the last two weekends of the \\nmonth. \\nGIVING YOUR MODEL THE ANSWER \\nIt may seem like cheating, but if you know for a fact that a certain \\ncombination of feature values is particularly predictive, you can create a \\nnew binary feature that takes a nonzero value only when these features \\ntake the relevant combination of values. In our case, this would mean \\nadding a feature called “is_last_two_weekends”, for example, that will be \\nset to one only during the last two weekends of the month. \\nIf the last two weekends are as predictive as we had supposed they were, \\nthe model will simply learn to leverage this feature and will be much more \\naccurate. When building ML products, never hesitate to make the task \\neasier for your model. Better to have a model that works on a simpler task \\nthan one that struggles on a complex one. \\nFeature generation is a wide field, and methods exist for most types of \\ndata. Discussing every feature that is useful to generate for different types \\nof data is outside the scope of this book. If you’d like to see more practical \\nexamples and methods, I recommend taking a look at Feature Engineering \\nfor Machine Learning (O’Reilly), by Alice Zheng and Amanda Casari. \\nIn general, the best way to generate useful features is by looking at your \\ndata using the methods we described and asking yourself what the easiest \\nway is to represent it in a way that will make your model learn its patterns. \\nIn the following section, I’ll describe a few examples of features I \\ngenerated using this process for the ML Editor. \\nML Editor Features \\nFor our ML Editor, using the techniques described earlier to inspect our \\ndataset (see details of the exploration in the exploring data to generate \\nfeatures notebook, in this book’s GitHub repository), we generated the \\nfollowing features:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 116}, page_content='\\uf0b7 Action verbs such as can and should are predictive of a question \\nbeing answered, so we added a binary value that checks whether \\nthey are present in each question. \\n\\uf0b7 Question marks are good predictors as well, so we have generated \\na has_question feature. \\n\\uf0b7 Questions about correct use of the English language tended not to \\nget answers, so we added a is_language_question feature. \\n\\uf0b7 The length of the text of the question is another factor, with very \\nshort questions tending to go unanswered. This led to the addition of \\na normalized question length feature. \\n\\uf0b7 In our dataset, the title of the question contains crucial information \\nas well, and looking at titles when labeling made the task much \\neasier. This led to include the title text in all the earlier feature \\ncalculations. \\nOnce we have an initial set of features, we can start building a model. \\nBuilding this first model is the topic of the next chapter, Chapter 5. \\nBefore moving on to models, I wanted to dive deeper on the topic of how \\nto gather and update a dataset. To do that, I sat down with Robert Munro, \\nan expert in the field. I hope you enjoy the summary of our discussion \\nhere, and that it leaves you excited to move on to our next part, building \\nour first model! \\nRobert Munro: How Do You Find, Label, and \\nLeverage Data? \\nRobert Munro has founded several AI companies, building some of the top \\nteams in artificial intelligence. He was chief technology officer at Figure \\nEight, a leading data labeling company during their biggest growth period. \\nBefore that, Robert ran product for AWS’s first native natural language \\nprocessing and machine translation services. In our conversation, Robert \\nshares some lessons he learned building datasets for ML. \\nQ: How do you get started on an ML project? \\nA: The best way is to start with the business problem, as it will give you \\nboundaries to work with. In your ML editor case study example, are you \\nediting text that someone else has written after they submit it, or are you'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 117}, page_content='suggesting edits live as somebody writes? The first would let you batch \\nprocess requests with a slower model, while the second one would require \\nsomething quicker. \\nIn terms of models, the second approach would invalidate sequence-to-\\nsequence models as they would be too slow. In addition, sequence-to-\\nsequence models today do not work beyond sentence-level \\nrecommendations and require a lot of parallel text to be trained. A faster \\nsolution would be to leverage a classifier and use the important features it \\nextracts as suggestions. What you want out of this initial model is an easy \\nimplementation and results you can have confidence in, starting with naive \\nBayes on bag of words features, for example. \\nFinally, you need to spend some time looking at some data and labeling it \\nyourself. This will give you an intuition for how hard the problem is and \\nwhich solutions might be a good fit. \\nQ: How much data do you need to get started? \\nA: When gathering data, you are looking to guarantee that you have a \\nrepresentative and diverse dataset. Start by looking at the data you have \\nand seeing if any types are unrepresented so that you can gather more. \\nClustering your dataset and looking for outliers can be helpful to speed up \\nthis process. \\nFor labeling data, in the common case of classification, we’ve seen that \\nlabeling on the order of 1,000 examples of your rarer category works well \\nin practice. You’ll at least get enough signal to tell you whether to keep \\ngoing with your current modeling approach. At around 10,000 examples, \\nyou can start to trust in the confidence of the models you are building. \\nAs you get more data, your model’s accuracy will slowly build up, giving \\nyou a curve of how your performance scales with data. At any point you \\nonly care about the last part of the curve, which should give you an \\nestimate of the current value more data will give you. In the vast majority \\nof cases, the improvement you will get from labeling more data will be \\nmore significant than if you iterated on the model. \\nQ: What process do you use to gather and label data?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 118}, page_content='A: You can look at your current best model and see what is tripping it up. \\nUncertainty sampling is a common approach: identify examples that your \\nmodel is the most uncertain about (the ones closest to its decision \\nboundary), and find similar examples to add to the training set. \\nYou can also train an “error model” to find more data your current model \\nstruggles on. Use the mistakes your model makes as labels (labeling each \\ndata point as “predicted correctly” or “predicted incorrectly”). Once you \\ntrain an “error model” on these examples, you can use it on your unlabeled \\ndata and label the examples that it predicts your model will fail on. \\nAlternatively, you can train a “labeling model” to find the best examples to \\nlabel next. Let’s say you have a million examples, of which you’ve labeled \\nonly 1,000. You can create a training set of 1,000 randomly sampled \\nlabeled images, and 1,000 unlabeled, and train a binary classifier to predict \\nwhich images you have labeled. You can then use this labeling model to \\nidentify data points that are most different from what you’ve already \\nlabeled and label those. \\nQ: How do you validate that your models are learning something useful? \\nA: A common pitfall is to end up focusing labeling efforts on a small part \\nof the relevant dataset. It may be that your model struggles with articles \\nthat are about basketball. If you keep annotating more basketball articles, \\nyour model may become great at basketball but bad at everything else. \\nThis is why while you should use strategies to gather data, you should \\nalways randomly sample from your test set to validate your model. \\nFinally, the best way to do it is to track when the performance of your \\ndeployed model drifts. You could track the uncertainty of the model or \\nideally bring it back to the business metrics: are your usage metrics \\ngradually going down? This could be caused by other factors, but is a good \\ntrigger to investigate and potentially update your training set. \\nConclusion \\nIn this chapter, we covered important tips to efficiently and effectively \\nexamine a dataset.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 119}, page_content='We started by looking at the quality of data and how to decide whether it is \\nsufficient for our needs. Next, we covered the best way to get familiar with \\nthe type of data you have: starting with summary statistics and moving on \\nto clusters of similar points to identify broad trends. \\nWe then covered why it is valuable to spend some significant time labeling \\ndata to identify trends that we can then leverage to engineer valuable \\nfeatures. Finally, we got to learn from Robert Munro’s experience helping \\nmultiple teams build state-of-the-art datasets for ML. \\nNow that we’ve examined a dataset and generated features we hope to be \\npredictive, we are ready to build our first model, which we will do \\nin Chapter 5.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 120}, page_content='Part III. Iterate on Models \\nPart I covered best practices to set up an ML project and track its progress. \\nIn Part II, we saw the value of building an end-to-end pipeline as fast as \\npossible along with exploring an initial dataset. \\nBecause of its experimental nature, ML is very much an iterative process. \\nYou should plan to repeatedly iterate on models and data, following an \\nexperimental loop as pictured in Figure III-1. \\n \\nFigure III-1. The ML loop \\nPart III will describe one iteration of the loop. When working on ML \\nprojects, you should plan to go through multiple such iterations before \\nexpecting to reach satisfying performance. Here is an overview of the \\nchapters in this part of the book: \\nChapter 5 \\nIn this chapter, we will train a first model and benchmark it. Then, \\nanalyze its performance in depth and identify how it could be \\nimproved. \\nChapter 6'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 121}, page_content='This chapter covers techniques to build and debug models quickly \\nand avoid time-consuming errors. \\nChapter 7 \\nIn this chapter, we will use the ML Editor as a case study to show \\nhow to leverage a trained classifier to provide suggestions to users \\nand build a fully functioning suggestion model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 122}, page_content='Chapter 5. Train and Evaluate Your \\nModel \\nIn the previous chapters we’ve covered how to identify the right problem \\nto tackle, make a plan to tackle it, build a simple pipeline, explore a \\ndataset, and generate an initial set of features. These steps have allowed us \\nto gather enough information to begin training an adequate model. An \\nadequate model here means a model that is a good fit for the task at hand \\nand that has good chances of performing well. \\nIn this chapter, we will start by briefly going over some concerns when \\nchoosing a model. Then, we will describe best practices to separate your \\ndata, which will help evaluate your models in realistic conditions. Finally, \\nwe’ll look at methods to analyze modeling results and diagnose errors. \\nThe Simplest Appropriate Model \\nNow that we are ready to train a model, we need to decide which model to \\nstart with. It may be tempting to try every possible model, benchmark \\nthem all, and pick the one with the best results on a held-out test set \\naccording to some metrics. \\nIn general, this is not the best approach. Not only is it computationally \\nintensive (there are many sets of models and many parameters for each \\nmodel, so realistically you will only be able to test a suboptimal subset), it \\nalso treats models as predictive black boxes and entirely ignores that ML \\nmodels encode implicit assumptions about the data in the way they learn. \\nDifferent models make different assumptions about the data and so are \\nsuited for different tasks. In addition, since ML is an iterative field, you’ll \\nwant to pick models that you can build and evaluate quickly. \\nLet’s first define how to identify simple models. Then, we will cover some \\nexamples of data patterns and appropriate models to leverage them. \\nSimple Models \\nA simple model should be quick to implement, understandable, and \\ndeployable: quick to implement because your first model will likely not be'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 123}, page_content='your last, understandable because it will allow you to debug it more easily, \\nand deployable because that is a fundamental requirement for an ML-\\npowered application. Let’s start by exploring what I mean by quick to \\nimplement. \\nQUICK TO IMPLEMENT \\nChoose a model that will be simple for you to implement. Generally, this \\nmeans picking a well-understood model that has multiple tutorials written \\nabout it and that people will be able to help you with (especially if you ask \\nwell-formulated questions using our ML Editor!). For a new ML-driven \\napplication, you will have enough challenges to tackle in terms of \\nprocessing data and deploying a reliable result that you should initially do \\nyour best to avoid all model headaches. \\nIf possible, start by using models from popular libraries such as Keras or \\nscikit-learn, and hold off before diving into an experimental GitHub \\nrepository that has no documentation and hasn’t been updated in the last \\nnine months. \\nOnce your model is implemented, you’ll want to inspect and understand \\nhow it is leveraging your dataset. To do so, you need a model that is \\nunderstandable. \\nUNDERSTANDABLE \\nModel explainability and interpretability describe the ability for a model \\nto expose reasons (such as a given combination of predictors) that caused \\nit to make predictions. Explainability can be useful for a variety of \\nreasons, such as verifying that our models are not biased in undesirable \\nways or explaining to a user what they could do to improve prediction \\nresults. It also makes iterating and debugging much easier. \\nIf you can extract the features a model relies on to make decisions, you’ll \\nhave a clearer view of which features to add, tweak, or remove, or which \\nmodel could make better choices. \\nUnfortunately, model interpretability is often complex even for simple \\nmodels and sometimes intractable for larger ones. In “Evaluate Feature \\nImportance”, we will see ways to tackle this challenge and help you \\nidentify points of improvement for your model. Among other things, we'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 124}, page_content='will use black-box explainers that attempt to provide explanations of a \\nmodel’s prediction regardless of its internal workings. \\nSimpler models such as logistic regression or decision trees tend to be \\neasier to explain as they provide some measure of feature importance, \\nwhich is another reason they are usually good models to try first. \\nDEPLOYABLE \\nAs a reminder, the end goal of your model is to provide a valuable service \\nto people who will use it. This means when you think of which model to \\ntrain, you should always consider whether you will be able to deploy it. \\nWe will cover deployment in Part IV, but you should already be thinking \\nabout questions such as the following: \\n\\uf0b7 How long will it take a trained model to make a prediction for a \\nuser? When thinking of prediction latency, you should include not \\nonly the time it takes for a model to output a result, but the delay in \\nbetween when a user submits a prediction request and receives the \\nresult. This includes any preprocessing steps such as feature \\ngeneration, any network calls, and any postprocessing steps that \\nhappen in between a model’s output and the data that is presented to \\na user. \\n\\uf0b7 Is this inference pipeline fast enough for our use case if we take into \\naccount the number of concurrent users we expect? \\n\\uf0b7 How long does it take to train the model, and how often do we need \\nto train it? If training takes 12 hours and you need to retrain your \\nmodel every 4 hours to be fresh, not only will your compute bill be \\nquite expensive, but your model will always be out of date. \\nWe can compare how simple models are by using a table such as Figure 5-\\n1. As the field of ML evolves and new tooling is built, models that may be \\ncomplex to deploy or hard to interpret today may become simpler to use, \\nand this table will need to be updated. For this reason, I suggest you build \\nyour own version based on your particular problem domain.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 125}, page_content='Figure 5-1. Scoring models based on their simplicity \\nEven among models that are simple, interpretable, and deployable, there \\nare still many potential candidates. To choose a model, you should also \\ntake into account the patterns you identified in Chapter 4. \\nFrom Patterns to Models \\nThe patterns we have identified and the features we have generated should \\nguide our model choice. Let’s cover a few examples of patterns in the data \\nand appropriate models to leverage them. \\nWE WANT TO IGNORE FEATURE SCALE \\nMany models will leverage larger features more heavily than smaller ones. \\nThis can be fine in some cases, but undesirable in others. For models using \\noptimization procedures like gradient descent such as neural networks, \\ndifferences in feature scale can sometimes lead to instability in the training \\nprocedure. \\nIf you want to use both age in years (ranging from one to a hundred) and \\nincome in dollars (let’s say our data reaches up to nine figures) as two \\npredictors, you need to make sure that your model is able to leverage the \\nmost predictive features, regardless of their scale. \\nYou can ensure this by preprocessing features to normalize their scale to \\nhave zero mean and unit variance. If all features are normalized to the \\nsame range, a model will consider each of them equally (at least initially).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 126}, page_content='Another solution is to turn to models that are not affected by differences in \\nfeature scale. The most common practical examples are decision trees, \\nrandom forests, and gradient-boosted decision trees. XGBoost is an \\nimplementation of gradient-boosted trees commonly used in production \\nbecause of its robustness, as well as its speed. \\nOUR PREDICTED VARIABLE IS A LINEAR COMBINATION \\nOF PREDICTORS \\nSometimes, there is good reason to believe that we can make good \\npredictions using only a linear combination of our features. In these cases, \\nwe should use a linear model such as a linear regression for continuous \\nproblems or a logistic regression or naive Bayes classifier for classification \\nproblems. \\nThese models are simple, efficient, and often allow for a direct \\ninterpretation of their weights that can help us identify important features. \\nIf we believe the relationships between our features and our predicted \\nvariable are more complex, using a nonlinear model such as a multilayer \\nneural network or generating feature crosses (see the beginning of “Let \\nData Inform Features and Models”) can help. \\nOUR DATA HAS A TEMPORAL ASPECT \\nIf we are dealing with time series of data points where the value at a given \\ntime depends on previous values, we would want to leverage models that \\nexplicitly encode this information. Examples of such models include \\nstatistical models such as autoregressive integrated moving average \\n(ARIMA) or recurrent neural networks (RNN). \\nEACH DATA POINT IS A COMBINATION OF PATTERNS \\nWhen tackling problems in the image domain, for example, convolutional \\nneural networks (CNNs) have proven useful through their ability to \\nlearn translation-invariant filters. This means that they are able to extract \\nlocal patterns in an image regardless of their position. Once a CNN learns \\nhow to detect an eye, it can detect it anywhere in an image, not just in the \\nplaces that it appeared in the training set. \\nConvolutional filters have proven useful in other fields that contain local \\npatterns, such as speech recognition or text classification, where CNNs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 127}, page_content='have been used successfully for sentence classification. For an example, \\nsee the implementation by Yoon Kim in the paper, “Convolutional Neural \\nNetworks for Sentence Classification”. \\nThere are many additional points to consider when thinking of the right \\nmodels to use. For most classical ML problems, I recommend using this \\nhandy flowchart that the scikit-learn team helpfully provides. It provides \\nmodel suggestions for many common use cases. \\nML EDITOR MODEL \\nFor the ML Editor, we would like our first model to be fast and reasonably \\neasy to debug. In addition, our data consists of individual examples, \\nwithout a need to consider a temporal aspect (such as a series of questions, \\nfor example). For that reason, we will start with a popular and resilient \\nbaseline, a random forest classifier. \\nOnce you’ve identified a model that seems reasonable, it is time to train it. \\nAs a general guideline, you should not train your model on the entirety of \\nthe dataset you gathered in Chapter 4. You’ll want to start by holding out \\nsome data from your training set. Let’s cover why and how you should do \\nthat. \\nSplit Your Dataset \\nThe main goal of our model is to provide valid predictions for data that our \\nusers will submit. This means that our model will eventually have to \\nperform well on data that it has never seen before. \\nWhen you train a model on a dataset, measuring its performance on the \\nsame dataset only tells you how good it is at making predictions on data it \\nhas already seen. If you only train a model on a subset of your data, you \\ncan then use the data the model was not trained on to estimate how well it \\nwould perform on unseen data. \\nIn Figure 5-2, you can see an example of a split into three separate sets \\n(train, validation, and test) based on an attribute of our dataset (the author \\nof a question). In this chapter, we will cover what each of these sets \\nmeans, and how to think about them.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 128}, page_content='Figure 5-2. Splitting data on author while attributing the right proportion of questions to each split  \\nThe first held-out set to consider is the validation set. \\nVALIDATION SET \\nTo estimate how our model performs on unseen data, we purposefully hold \\nout part of our dataset from training and then use the performance on this \\nheld-out dataset as a proxy for our model’s performance in production. \\nThe held-out set allows us to validate that our model can generalize to \\nunseen data and thus is often called a validation set. \\nYou can choose different sections of your data to hold out as a validation \\nset to evaluate your model and train it on the remaining data. Doing \\nmultiple rounds of this process helps control for any variance due to a \\nparticular choice of validation set and is called cross-validation. \\nAs you change your data preprocessing strategy and the type of model you \\nuse or its hyperparameters, your model’s performance on the validation set \\nwill change (and ideally improve). Using the validation set allows you to \\ntune hyperparameters the same way that using a training set allows a \\nmodel to tune its parameters. \\nAfter multiple iterations of using the validation set to make model \\nadjustments, your modeling pipeline can become tailored specifically to \\nperforming well on your validation data. This defeats the purpose of the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 129}, page_content='validation set, which is supposed to be a proxy for unseen data. For this \\nreason, you should hold out an additional test set. \\nTEST SET \\nSince we will go through multiple cycles of iteration on our model and \\nmeasure its performance on a validation set at each cycle, we may bias our \\nmodel so that it performs well on the validation set. This helps our model \\ngeneralize beyond the training set but also carries the risk of simply \\nlearning a model that performs well only on our particular validation sets. \\nIdeally, we would want to have a model that works well on new data that \\nis hence not contained in the validation set. \\nFor this reason, we usually hold out a third set called a test set, which \\nserves as a final benchmark of our performance on unseen data, once we \\nare satisfied with our iterations. While using a test set is a best practice, \\npractitioners sometimes use the validation set as a test set. This increases \\nthe risk of biasing a model toward the validation set but can be appropriate \\nwhen running only a few experiments. \\nIt is important to avoid using performance on the test set to inform \\nmodeling decisions, as this set is supposed to represent the unseen data we \\nwill face in production. Adapting a modeling approach to perform well on \\nthe test set risks leading to overestimating the performance of the model. \\nTo have a model that performs in production, the data you train on should \\nresemble data produced by users who will interact with your product. \\nIdeally, any kind of data you could receive from users should be \\nrepresented in your dataset. If that is not the case, then keep in mind that \\nyour test set performance is indicative of performance for only a subset of \\nyour users. \\nFor the ML Editor, this means that users who do not conform to the \\ndemographics of writers.stackoverflow.com may not be as well served by \\nour recommendations. If we wanted to address this problem, we should \\nexpand the dataset to contain questions more representative of these users. \\nWe could start by incorporating questions from other Stack Exchange \\nwebsites to cover a broader set of topics, or different question and \\nanswering websites altogether.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 130}, page_content='Correcting a dataset in such a manner can be challenging for a side project. \\nWhen building consumer-grade products, however, it is necessary to help \\nmodel weaknesses be caught early before users are exposed to them. \\nMany of the failure modes we will cover in Chapter 8 could have been \\navoided with a more representative dataset. \\nRELATIVE PROPORTIONS \\nIn general, you should maximize the amount of data the model can use to \\nlearn from, while holding out large enough validation and test sets to \\nprovide accurate performance metrics. Practitioners often use 70% of the \\ndata for training, 20% for validation, and 10% for testing, but this depends \\nentirely on the quantity of data. For very large datasets, you can afford to \\nuse a larger proportion of data for training while still having enough data \\nto validate models. For smaller datasets, you may need to use a smaller \\nproportion for training in order to have a validation set that is large enough \\nto provide an accurate performance measurement. \\nNow you know why you’d want to split data, and which splits to consider, \\nbut how should you decide which datapoint goes in each split? The \\nsplitting methods you use have a significant impact on modeling \\nperformance and should depend on the particular features of your dataset. \\nDATA LEAKAGE \\nThe method you use to separate your data is a crucial part of validation. \\nYou should aim to make your validation/test set close to what you expect \\nunseen data to be like. \\nMost often, train, validation, and test sets are separated by sampling data \\npoints randomly. In some cases, this can lead to data leakage. Data \\nleakage happens when (because of our training procedure) a model \\nreceives information during training that it won’t have access to when \\nbeing used in front of real users in production. \\nData leakage should be avoided at all costs, because it leads to an inflated \\nview of the performance of our model. A model trained on a dataset \\nexhibiting data leakage is able to leverage information to make predictions \\nthat it will not have when it encounters different data. This makes the task \\nartificially easier for the model, but only due to the leaked information.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 131}, page_content='The model’s performance appears high on the held-out data but will be \\nmuch worse in production. \\nIn Figure 5-3, I’ve drawn a few common causes where randomly splitting \\nyour data into sets will cause data leakage. There are many potential \\ncauses of data leakage, and we will explore two frequent ones next. \\nTo start our exploration, let’s tackle the example at the top of Figure 5-3, \\ntemporal data leakage. Then, we will move on to sample contamination, a \\ncategory that encompasses the bottom two examples in Figure 5-3. \\n \\nFigure 5-3. Splitting data randomly can often lead to data leakage \\nTemporal data leakage \\nIn time-series forecasting, a model needs to learn from data points in the \\npast to predict events that have not happened yet. If we perform a random \\nsplit on a forecasting dataset, we will introduce data leakage: a model that \\nis trained on a random set of points and evaluated on the rest will have \\naccess to training data that happens after events it is trying to predict. \\nThe model will perform artificially well on the validation and test sets but \\nfail in production, because all it has learned is to leverage future \\ninformation, which is unavailable in the real world. \\nOnce you are aware of it, temporal data leakage is usually easy to catch. \\nOther types of data leakage can give a model access to information it \\nshould not have during training and artificially inflate its performance by \\n“contaminating” its training data. They can often be much harder to detect.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 132}, page_content='Sample contamination \\nA common source of data leakage lies in the level at which the \\nrandomness occurs. When building a model to predict the grade students’ \\nessays will receive, a data scientist I was assisting once found that his \\nmodel performed close to perfect on a held-out test set. \\nOn such a hard task, a model that performs so well should be closely \\nexamined as it frequently indicates the presence of a bug or data leakage. \\nSome would say that the ML equivalent of Murphy’s law is that the more \\npleasantly surprised you are by the performance of your model on your \\ntest data, the more likely you are to have an error in your pipeline. \\nIn this example, because most students had written multiple essays, \\nsplitting data randomly led to essays by the same students being present \\nboth in the training and in the test sets. This allowed the model to pick up \\non features that identified students and use that information to make \\naccurate predictions (students in this dataset tended to have similar grades \\nacross all their essays). \\nIf we were to deploy this essay score predictor for future use, it would not \\nbe able to predict useful scores for students it hadn’t seen before and \\nwould simply predict historical scores for students whose essays it has \\nbeen trained on. This would not be useful at all. \\nTo solve the data leakage in this example, a new split was made at the \\nstudent rather than the essay level. This meant that each student appeared \\neither only in the training set or only in the validation set. Since the task \\nbecame much harder, this led to a decrease in model accuracy. However, \\nsince the training task was now much closer to what it would be in \\nproduction, this new model was much more valuable. \\nSample contamination can happen in nuanced ways in common tasks. \\nLet’s take the example of an apartment rental booking website. This \\nwebsite incorporates a click prediction model that, given a user query and \\nan item, predicts whether the user will click on the item. This model is \\nused to decide which listings to display to users. \\nTo train such a model, this website could use a dataset of user features \\nsuch as their number of previous bookings, paired with apartments that \\nwere presented to them and whether they clicked on them. This data is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 133}, page_content='usually stored in a production database that can be queried to produce such \\npairs. If engineers of this website were to simply query the database to \\nbuild such a dataset, they would likely be faced with a case of data \\nleakage. Can you see why? \\nIn Figure 5-4, I’ve sketched out an illustration of what can go wrong by \\ndepicting a prediction for a specific user. At the top, you can see the \\nfeatures that a model could use in production to provide a click prediction. \\nHere, a new user with no previous bookings is presented with a given \\napartment. At the bottom, you can see the state of the features a few days \\nlater when engineers extract data from the database. \\n \\nFigure 5-4. Data leakage can happen for subtle reasons, such as due to a lack of data versioning  \\nNotice the difference in previous_bookings, which is due to user activity that \\nhappened after they were initially presented with the listing. By using a \\nsnapshot of a database, information about future actions of the user was \\nleaked into the training set. We now know that the user will eventually \\nbook five apartments! Such leakage can lead a model trained with \\ninformation at the bottom to output a correct prediction on the incorrect \\ntraining data. The accuracy of the model on the generated dataset will be \\nhigh because it is leveraging data it will not have access to in production. \\nWhen the model is deployed, it will perform worse than expected. \\nIf you take anything away from this anecdote, it is to always investigate \\nthe results of a model, especially if it shows surprisingly strong \\nperformance. \\nML Editor Data Split \\nThe dataset we are using to train our ML Editor contains questions asked \\non Stack Overflow, as well as their answers. At first glance, a random split'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 134}, page_content='can seem sufficient and is quite simple to implement in scikit-learn. We \\ncould, for example, write a function like the one shown here: \\nfrom sklearn.model_selection import train_test_split \\n \\ndef get_random_train_test_split(posts, test_size=0.3, random_state=40): \\n    \"\"\" \\n    Get train/test split from DataFrame \\n    Assumes the DataFrame has one row per question example \\n    :param posts: all posts, with their labels \\n    :param test_size: the proportion to allocate to test \\n    :param random_state: a random seed \\n    \"\"\" \\n    return train_test_split( \\n        posts, test_size=test_size, random_state=random_state \\n    ) \\nThere is a potential reach for leakage with such an approach; can you \\nidentify it? \\nIf we think back to our use case, we know we would like our model to \\nwork on questions it has not seen before, only looking at their content. On \\na question and answering website, however, many other factors can play \\ninto whether a question is answered successfully. One of these factors is \\nthe identity of the author. \\nIf we split our data randomly, a given author could appear both in our \\ntraining and validation sets. If certain popular authors have a distinctive \\nstyle, our model could overfit on this style and reach artificially high \\nperformance on our validation set due to data leakage. To avoid this, it \\nwould be safer for us to make sure each author appears only in training or \\nvalidation. This is the same type of leakage we described in the student \\ngrading example earlier. \\nUsing scikit-learn’s GroupShuffleSplit class and passing the feature \\nrepresenting an author’s unique ID to its split method, we can guarantee \\nthat a given author appears in only one of the splits. \\nfrom sklearn.model_selection import GroupShuffleSplit \\n \\ndef get_split_by_author( \\n    posts, author_id_column=\"OwnerUserId\", test_size=0.3, random_state=40 \\n): \\n    \"\"\" \\n    Get train/test split \\n    Guarantee every author only appears in one of the splits \\n    :param posts: all posts, with their labels \\n    :param author_id_column: name of the column containing the author_id \\n    :param test_size: the proportion to allocate to test'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 135}, page_content=':param random_state: a random seed \\n    \"\"\" \\n    splitter = GroupShuffleSplit( \\n        n_splits=1, test_size=test_size, random_state=random_state \\n    ) \\n    splits = splitter.split(posts, groups=posts[author_id_column]) \\n    return next(splits) \\nTo see a comparison between both splitting methods, refer to the splitting \\ndata notebook in this book’s GitHub repository. \\nOnce a dataset is split, a model can be fit to the training set. We’ve \\ncovered the required parts of a training pipeline in “Start with a Simple \\nPipeline”. In the training of a simple model notebook in the GitHub \\nrepository for this book, I show an example of an end-to-end training \\npipeline for the ML Editor. We will analyze the results of this pipeline. \\nWe’ve covered the main risks we want to keep in mind when splitting \\ndata, but what should we do once our dataset is split and we’ve trained a \\nmodel on the training split? In the next section, we’ll talk about different \\npractical ways to evaluate trained models and how to leverage them best. \\nJudge Performance \\nNow that we have split our data, we can train our model and judge how it \\nperformed. Most models are trained to minimize a cost function, which \\nrepresents how far a model’s predictions are from the true labels. The \\nsmaller the value of the cost function, the better the model fits the data. \\nWhich function you minimize depends on your model and your problem, \\nbut it is generally a good idea to take a look at its value both on the \\ntraining set and on the validation set. \\nThis commonly helps estimate the bias-variance trade-off of our model, \\nwhich measures the degree to which our model has learned valuable \\ngeneralizable information from the data, without memorizing the details of \\nour training set. \\nNOTE \\nI’m assuming familiarity with standard classification metrics, but here is a \\nshort reminder just in case. For classification problems, accuracy \\nrepresents the proportion of examples a model predicts correctly. In other \\nwords, it is the proportion of true results, which are both true positives and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 136}, page_content='true negatives. In cases with a strong imbalance, a high accuracy can mask \\na poor model. If 99% of cases are positive, a model that always predicts \\nthe positive class will have 99% accuracy but may not be very useful. \\nPrecision, recall, and f1 score address this limitation. Precision is the \\nproportion of true positives among examples predicted as positive. \\nRecall is the proportion of true positives among elements that had a \\npositive label. The f1 score is the harmonic mean of precision and recall. \\nIn the training of a simple model notebook in this book’s GitHub \\nrepository, we train a first version of a random forest using TF-IDF vectors \\nand the features we identified in “ML Editor Features”. \\nHere are the accuracy, precision, recall, and f1 scores for our training set \\nand our validation set. \\nTraining accuracy = 0.585, precision = 0.582, recall = 0.585, f1 = 0.581 \\nValidation accuracy = 0.614, precision = 0.615, recall = 0.614, f1 = \\n0.612 \\nTaking a quick look at these metrics allows us to notice two things: \\n\\uf0b7 Since we have a balanced dataset consisting of two classes, picking a \\nclass at random for every example would give us roughly 50% \\naccuracy. Our model’s accuracy reaches 61%, better than a random \\nbaseline. \\n\\uf0b7 Our accuracy on the validation set is higher than on the training set. \\nIt seems our model works well on unseen data. \\nLet’s dive deeper to find out more about the performance of the model. \\nBIAS VARIANCE TRADE-OFF \\nWeak performance on the training set is a symptom of high bias, also \\ncalled underfitting, which means a model has failed to capture useful \\ninformation: it is not even able to perform well on data points it has \\nalready been given the label for. \\nStrong performance on the training set but weak performance on the \\nvalidation set is a symptom of high variance, also called overfitting, \\nmeaning that a model has found ways to learn the input/output mapping \\nfor the data it has been trained on, but what it has learned does not \\ngeneralize to unseen data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 137}, page_content='Underfitting and overfitting are two extreme cases of the bias-variance \\ntrade-off, which describes how the types of errors a model makes change \\nas its complexity increases. As model complexity grows, variance \\nincreases and bias decreases, and the model goes from underfitting to \\noverfitting. You can see this depicted in Figure 5-5. \\n \\nFigure 5-5. As complexity increases, bias decreases but variance increases as well \\nIn our case, since our validation performance is better than our training \\nperformance, we can see that our model is not overfitting the training data. \\nWe can likely increase the complexity of our model or features to improve \\nperformance. Fighting the bias-variance trade-off requires finding an \\noptimal point between reducing bias, which increases a model’s \\nperformance on the training set, and reducing variance, which increases its \\nperformance on the validation set (often worsening training performance \\nas a byproduct). \\nPerformance metrics help generate an aggregate perspective of a model’s \\nperformance. This is helpful to guess how a model is doing but does not \\nprovide much intuition as to what precisely a model is succeeding or \\nfailing at. To improve our model, we need to dive deeper.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 138}, page_content='GOING BEYOND AGGREGATE METRICS \\nA performance metric helps determine whether a model has learned \\ncorrectly from a dataset or whether it needs to be improved. The next step \\nis to examine results further in order to understand in which way a model \\nis failing or succeeding. This is crucial for two reasons: \\nPerformance validation \\nPerformance metrics can be very deceptive. When working on a \\nclassification problem with severely imbalanced data such as \\npredicting a rare disease that appears in fewer than 1% of patients, \\nany model that always predicts that a patient is healthy will reach an \\naccuracy of 99%, even though it has no predictive power at all. \\nThere exists performance metrics suited for most problems (the f1 \\nscore would work better for the previous problem), but the key is to \\nremember that they are aggregate metrics and paint an incomplete \\npicture of the situation. To trust the performance of a model, you \\nneed to inspect results at a more granular level. \\nIteration \\nModel building is an iterative process, and the best way to start an \\niteration loop is by identifying both what to improve and how to \\nimprove it. Performance metrics do not help identify where a model \\nis struggling and which part of the pipeline needs improvement. Too \\noften, I’ve seen data scientists try to improve model performance by \\nsimply trying many other models or hyperparameters, or building \\nadditional features haphazardly. This approach amounts to throwing \\ndarts at the wall while blindfolded. The key to building successful \\nmodels quickly is to identify and address specific reasons models are \\nfailing. \\nWith these two motivations in mind, we will cover a few ways to dive \\ndeeper into the performance of a model. \\nEvaluate Your Model: Look Beyond Accuracy \\nThere are a myriad of ways to inspect how a model is performing, and we \\nwill not cover every potential evaluation method. We will focus on a few \\nthat are often helpful to tease out what might be happening below the \\nsurface.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 139}, page_content='When it comes to investigating model performance, think of yourself as a \\ndetective and each of the methods covered next as different ways to \\nsurface clues. We’ll start by covering multiple techniques that contrast a \\nmodel’s prediction with the data to uncover interesting patterns. \\nContrast Data and Predictions \\nThe first step to evaluating a model in depth is to find more granular ways \\nthan aggregate metrics to contrast data and predictions. We’d like to break \\ndown aggregate performance metrics such as accuracy, precision, or recall \\non different subsets of our data. Let’s see how to do this for the common \\nML challenge of classification. \\nYou can find all the code examples in the comparing data to predictions \\nnotebook in this book’s GitHub repository. \\nFor classification problems, I usually recommend starting by looking at a \\nconfusion matrix, shown in Figure 5-6, whose rows represent each true \\nclass, and columns represent the predictions of our model. A model with \\nperfect predictions will have a confusion matrix with zeros everywhere \\nexcept in the diagonal going from the top left to the bottom right. In \\nreality, that is rarely the case. Let’s take a look at why a confusion matrix \\nis often very useful. \\nConfusion Matrix \\nA confusion matrix allows us at a glance to see whether our model is \\nparticularly successful on certain classes and struggles on some others. \\nThis is particularly useful for datasets with many different classes or \\nclasses that are imbalanced. \\nOftentimes, I’ve seen models with impressive accuracy show a confusion \\nmatrix with one column entirely empty, meaning that there is a class that \\nthe model never predicts. This often happens for rare classes and can \\nsometimes be harmless. If the rare class represents an important outcome, \\nhowever, such as a borrower defaulting on a loan, a confusion matrix will \\nhelp us notice the problem. We can then correct it by weighing the rare \\nclass more heavily in our model’s loss function, for example. \\nThe top row of Figure 5-6 shows that the initial model we’ve trained does \\nwell when it comes to predicting low-quality questions. The bottom row'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 140}, page_content='shows that the model struggles to detect all high-quality questions. Indeed, \\nout of all the questions that received a high score, our model only predicts \\ntheir class correctly half of the time. Looking at the right column, \\nhowever, we can see that when the model predicts that a question is high \\nquality, its prediction tends to be accurate. \\nConfusion matrices can be even more useful when working on problems \\nwith more than two classes. For example, I once worked with an engineer \\nwho was trying to classify words from speech utterances who plotted a \\nconfusion matrix for his latest model. He immediately noticed two \\nsymmetrical, off-diagonal values that were abnormally high. These two \\nclasses (which each represented a word) were confusing the model and the \\ncause of a majority of its errors. Upon further inspection, it turns out that \\nthe words that were confusing the model were when and where. Gathering \\nadditional data for these two examples was enough to help the model \\nbetter differentiate these similar-sounding words. \\nA confusion matrix allows us to compare a model’s predictions with the \\ntrue classes for each class. When debugging models, we may want to look \\ndeeper than their predictions and examine the probabilities output by the \\nmodel.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 141}, page_content='Figure 5-6. Confusion matrix for an initial baseline on our question classification task \\nROC Curve \\nFor binary classification problems, receiver operating characteristic (ROC) \\ncurves can also be very informative. An ROC curve plots the true positive \\nrate (TPR) as a function of the false positive rate (FPR). \\nThe vast majority of models used in classification return a probability \\nscore that a given example belongs to a certain class. This means that at \\ninference time, we can choose to attribute an example to a certain class if \\nthe probability given by the model is above a certain threshold. This is \\nusually called the decision threshold. \\nBy default, most classifiers use a probability of 50% for their decision \\nthreshold, but this is something we can change based on our use case. By'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 142}, page_content='varying the threshold regularly from 0 to 1 and measuring the TPR and \\nFPR at each point, we obtain an ROC curve. \\nOnce we have a model’s prediction probability and the associated true \\nlabels, getting FPRs and TPRs is simple using scikit-learn. We can then \\ngenerate an ROC curve. \\nfrom sklearn.metrics import roc_curve \\n \\nfpr, tpr, thresholds = roc_curve(true_y, predicted_proba_y) \\nTwo details are important to understand for ROC curves such as the one \\nplotted in Figure 5-7. First, the diagonal line going between the bottom left \\nto the top right represents guessing randomly. This means that to beat a \\nrandom baseline, a classifier/threshold pair should be above this line. In \\naddition, the perfect model would be represented by the green dotted line \\non the top left.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 143}, page_content='Figure 5-7. ROC curve for an initial model \\nBecause of these two details, classification models often use the area under \\nthe curve (AUC) to represent performance. The larger the AUC, the closer \\nto a “perfect” model our classifier could be. A random model will have an \\nAUC of 0.5, while a perfect model has an AUC of 1. When concerning \\nourselves with a practical application, however, we should choose one \\nspecific threshold that gives us the most useful TPR/FPR ratio for our use \\ncase. \\nFor that reason, I recommend adding vertical or horizontal lines to an ROC \\ncurve that represent our product needs. When building a system that routes \\ncustomer requests to staff if it is deemed urgent enough, the FPR you can \\nafford is then entirely determined by the capacity of your support staff and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 144}, page_content='the number of users you have. This means that any models with an FPR \\nhigher than that limit should not even be considered. \\nPlotting a threshold on an ROC curve allows you to have a more concrete \\ngoal than simply getting the largest AUC score. Make sure your efforts \\ncount toward your goal! \\nOur ML Editor model classifies questions as good or bad. In this context, \\nthe TPR represents the proportion of high-quality questions our model \\ncorrectly judges as good. The FPR is the proportion of bad questions our \\nmodel claims is good. If we do not help our users, we’d like to at least \\nguarantee we don’t harm them. This means that we should not use any \\nmodel that risks recommending bad questions too frequently. We should \\nthus set a threshold for our FPR, such as 10%, for example, and use the \\nbest model we can find under that threshold. In Figure 5-8, you can see \\nthis requirement represented on our ROC curve; it has significantly \\nreduced the space of acceptable decision thresholds for models. \\nAn ROC curve gives us a more nuanced view of how a model’s \\nperformance changes as we make its predictions more or less conservative. \\nAnother way to look at a model’s prediction probability is to compare its \\ndistributions with the true class distributions to see whether it is well \\ncalibrated.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 145}, page_content='Figure 5-8. Adding ROC lines representing our product need \\nCalibration Curve \\nCalibration plots are another informative plot for binary classification \\ntasks, as they can help us get a sense for whether our model’s outputted \\nprobability represents its confidence well. A calibration plot shows the \\nfraction of true positive examples as a function of the confidence of our \\nclassifier. \\nFor example, out of all the data points our classifier gives a probability of \\nbeing classified as positive that is higher than 80%, how many of those \\ndata points are actually positive? A calibration curve for a perfect model \\nwill be a diagonal line from bottom left to top right.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 146}, page_content='In Figure 5-9, we can see at the top that our model is well calibrated \\nbetween .2 and .7, but not for probabilities outside of that range. Taking a \\nlook at the histogram of predicted probabilities below reveals that our \\nmodel very rarely predicts probabilities outside of that range, which is \\nlikely leading to the extreme results shown earlier. The model is rarely \\nconfident in its predictions. \\n \\nFigure 5-9. A calibration curve: the diagonal line represents a perfect model (top); histogram of \\npredicted values (bottom) \\nFor many problems, such as predicting CTR in ad serving, the data will \\nlead to our models being quite skewed when probabilities get close to 0 or \\n1, and a calibration curve will help us see this at a glance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 147}, page_content='To diagnose the performance of a model, it can be valuable to visualize \\nindividual predictions. Let’s cover methods to make this visualization \\nprocess efficient. \\nDimensionality Reduction for Errors \\nWe described vectorization and dimensionality reduction techniques for \\ndata exploration in “Vectorizing” and “Dimensionality reduction”. Let’s \\nsee how the same techniques can be used to make error analysis more \\nefficient. \\nWhen we first covered how to use dimensionality reduction methods to \\nvisualize data, we colored each point in a dataset by its class to observe the \\ntopology of labels. When analyzing model errors, we can use different \\ncolor schemes to identify errors. \\nTo identify error trends, color each data point by whether a model’s \\nprediction was correct or not. This will allow you to identify types of \\nsimilar data points a model performs poorly on. Once you identify a region \\nin which a model performs poorly, visualize a few data points in it. \\nVisualizing hard examples is a great way to generate features represented \\nin these examples to help a model fit them better. \\nTo help surface trends in hard examples, you can also use the clustering \\nmethods from “Clustering”. After clustering data, measure model \\nperformance on each cluster and identify clusters where the model \\nperforms worst. Inspect data points in these clusters to help you generate \\nmore features. \\nDimensionality reduction techniques are one way of surfacing challenging \\nexamples. To do so, we can also directly use a model’s confidence score. \\nThe Top-k Method \\nFinding dense error regions helps identify failure modes for a model. \\nAbove, we used dimensionality reduction to help us find such regions, but \\nwe can also directly use a model itself. By leveraging prediction \\nprobabilities, we can identify data points that were most challenging or for \\nwhich a model was most uncertain. Let’s call this approach the top-k \\nmethod.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 148}, page_content='The top-k method is straightforward. First, pick a manageable number of \\nexamples to visualize that we will call k. For a personal project where a \\nsingle person will be visualizing results, start with ten to fifteen examples. \\nFor each class or cluster you have previously found, visualize: \\n\\uf0b7 The k best performing examples \\n\\uf0b7 The k worst performing examples \\n\\uf0b7 The k most uncertain examples \\nVisualizing these examples will help you identify examples that are easy, \\nhard, or confusing to your model. Let’s dive into each category in more \\ndetail. \\nTHE K BEST PERFORMING EXAMPLES \\nFirst, display the k examples your model predicted correctly and was most \\nconfident about. When visualizing those examples, aim to identify any \\ncommonality in feature value among them that could explain model \\nperformance. This will help you identify features that are successfully \\nleveraged by a model. \\nAfter visualizing successful examples to identify features leveraged by a \\nmodel, plot unsuccessful ones to identify features it fails to pick up on. \\nTHE K WORST PERFORMING EXAMPLES \\nDisplay the k examples your model predicted incorrectly and was most \\nconfident about. Start with k examples in training data and then validation. \\nJust like visualizing error clusters, visualizing k examples that a model \\nperforms the worst on in a training set can help identify trends in data \\npoints the model fails on. Display these data points to help you identify \\nadditional features that would make them easier for a model. \\nWhen exploring an initial model’s error for the ML Editor, for example, I \\nfound that some questions posted received a low score because they did \\nnot contain an actual question. The model was not initially able to predict \\na low score for such questions, so I added a feature to count question \\nmarks in the body of the text. Adding this feature allowed the model to \\nmake accurate predictions for these “nonquestion” questions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 149}, page_content='Visualizing the k worst examples in the validation data can help identify \\nexamples that significantly differ from the training data. If you do identify \\nexamples in the validation set that are too hard, refer to tips in “Split Your \\nDataset” to update your data splitting strategy. \\nFinally, models are not always confidently right or wrong; they can also \\noutput uncertain predictions. I’ll cover those next. \\nTHE K MOST UNCERTAIN EXAMPLES \\nVisualizing the k most uncertain examples consists of displaying examples \\nfor which a model was least confident in its predictions. For a \\nclassification model, which this book focuses mostly on, uncertain \\nexamples are ones where a model outputs as close to an equal probability \\nas possible for each class. \\nIf a model is well calibrated (see “Calibration Curve” for an explanation of \\ncalibration), it will output uniform probabilities for examples that a human \\nlabeler would be uncertain about as well. For a cat versus dog classifier, \\nfor example, a picture containing both a dog and a cat would fall into that \\ncategory. \\nUncertain examples in the training set are often a symptom of conflicting \\nlabels. Indeed, if a training set contains two duplicate or similar examples \\nthat are each labeled a different class, a model will minimize its loss \\nduring training by outputting an equal probability for each class when \\npresented with this example. Conflicting labels thus lead to uncertain \\npredictions, and you can use the top-k method to attempt to find these \\nexamples. \\nPlotting the top-k most uncertain examples in your validation set can help \\nfind gaps in your training data. Validation examples that a model is \\nuncertain about but are clear to a human labeler are often a sign that the \\nmodel has not been exposed to this kind of data in its training set. Plotting \\nthe top-k uncertain examples for a validation set can help identify data \\ntypes that should be present in the training set. \\nTop-k evaluation can be implemented in a straightforward manner. In the \\nnext section, I’ll share a working example.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 150}, page_content='TOP-K IMPLEMENTATION TIPS \\nThe following is a simple top-k implementation that works with pandas \\nDataFrames. The function takes as input a DataFrame containing predicted \\nprobabilities and labels and returns each of the top-k above. It can be \\nfound in this book’s GitHub repository. \\ndef get_top_k(df, proba_col, true_label_col, k=5, decision_threshold=0.5): \\n    \"\"\" \\n    For binary classification problems \\n    Returns k most correct and incorrect example for each class \\n    Also returns k most unsure examples \\n    :param df: DataFrame containing predictions, and true labels \\n    :param proba_col: column name of predicted probabilities \\n    :param true_label_col: column name of true labels \\n    :param k: number of examples to show for each category \\n    :param decision_threshold: classifier decision boundary to classify \\nas \\n           positive \\n    :return: correct_pos, correct_neg, incorrect_pos, incorrect_neg, \\nunsure \\n    \"\"\" \\n    # Get correct and incorrect predictions \\n    correct = df[ \\n        (df[proba_col] > decision_threshold) == df[true_label_col] \\n    ].copy() \\n    incorrect = df[ \\n        (df[proba_col] > decision_threshold) != df[true_label_col] \\n    ].copy() \\n \\n    top_correct_positive = correct[correct[true_label_col]].nlargest( \\n        k, proba_col \\n    ) \\n    top_correct_negative = correct[~correct[true_label_col]].nsmallest( \\n        k, proba_col \\n    ) \\n \\n    top_incorrect_positive = \\nincorrect[incorrect[true_label_col]].nsmallest( \\n        k, proba_col \\n    ) \\n    top_incorrect_negative = \\nincorrect[~incorrect[true_label_col]].nlargest( \\n        k, proba_col \\n    ) \\n \\n    # Get closest examples to decision threshold \\n    most_uncertain = df.iloc[ \\n        (df[proba_col] - decision_threshold).abs().argsort()[:k] \\n    ] \\n \\n    return ( \\n        top_correct_positive, \\n        top_correct_negative, \\n        top_incorrect_positive,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 151}, page_content='top_incorrect_negative, \\n        most_uncertain, \\n    ) \\nLet’s illustrate the top-k method by using it for the ML Editor. \\nTOP-K METHOD FOR THE ML EDITOR \\nWe’ll apply the top-k method to the first classifier that we trained. A \\nnotebook containing usage examples for the top-k method is available \\nin this book’s GitHub repository. \\nFigure 5-10 shows the top two most correct examples for each class for \\nour first ML Editor model. The feature that differs the most between both \\nclasses is text_len, which represents the length of the text. The classifier \\nhas learned that good questions tend to be long, and poor ones are short. It \\nrelies heavily on text length to discriminate between classes. \\n \\nFigure 5-10. Top-k most correct \\nFigure 5-11 confirms this hypothesis. The unanswered questions our \\nclassifier predicts as most likely to be answered are the longest ones, and \\nvice versa. This observation also corroborates what we found in “Evaluate \\nFeature Importance”, where we saw text_len was the most important \\nfeature.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 152}, page_content='Figure 5-11. Top-k most incorrect \\nWe’ve established that the classifier leverages text_len to easily identify \\nanswered and unanswered questions, but that this feature is not sufficient \\nand leads to misclassifications. We should add more features to improve \\nour model. Visualizing more than two examples would help identify more \\ncandidate features. \\nUsing the top-k method on both training and validation data helps identify \\nlimits of both our model and dataset. We’ve covered how it can help \\nidentify whether a model has the capacity to represent data, whether a \\ndataset is balanced enough, and whether it contains enough representative \\nexamples. \\nWe mostly covered evaluation methods for classification models, since \\nsuch models are applicable for many concrete problems. Let’s briefly take \\na look at ways to inspect performance when not doing classification. \\nOther Models \\nMany models can be evaluated using a classification framework. In object \\ndetection, for example, where the goal is for a model to output bounding \\nboxes around objects of interest in an image, accuracy is a common \\nmetric. Since each image can have multiple bounding boxes representing \\nobjects and predictions, calculating an accuracy requires an additional \\nstep. First, computing the overlap between predictions and labels (often \\nusing the Jaccard index) allows each prediction to be marked as correct or'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 153}, page_content='incorrect. From there, one can calculate accuracy and use all the previous \\nmethods in this chapter. \\nSimilarly, when building models aiming to recommend content, the best \\nway to iterate is often to test the model on a variety of categories and \\nreport its performance. The evaluation then becomes similar to a \\nclassification problem, where each category represents a class. \\nFor types of problems where such methods may prove tricky, such as with \\ngenerative models, you can still use your previous exploration of data to \\nseparate a dataset in multiple categories and generate performance metrics \\nfor each category. \\nWhen I worked with a data scientist building a sentence simplification \\nmodel, examining the model’s performance conditioned on sentence \\nlength showed that longer sentences proved much harder for the model. \\nThis took inspection and hand labeling but led to a clear next action step of \\naugmenting the training data with longer sentences, which helped improve \\nperformance significantly. \\nWe’ve covered many ways to inspect a model’s performance by \\ncontrasting its predictions with labels, but we can also inspect the model \\nitself. If a model isn’t performing well at all, it may be worthwhile to try to \\ninterpret its predictions. \\nEvaluate Feature Importance \\nAn additional way to analyze a model’s performance is to inspect which \\nfeatures of the data it is using to make predictions. Doing so is called \\nfeature importance analysis. Evaluating feature importance is helpful to \\neliminate or iterate on features that are currently not helping the model. \\nFeature importance can also help identify features that are suspiciously \\npredictive, which is often a sign of data leakage. We will start by \\ngenerating feature importance for models that can do so easily and then \\ncover cases where such features may not be easy to extract directly. \\nDirectly from a Classifier \\nTo validate that a model is working correctly, visualize which features the \\nmodel is using or ignoring. For simple models such as regression or'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 154}, page_content='decision trees, extracting the importance of features is straightforward by \\nlooking at the learned parameters of the model. \\nFor the first model we used in the ML Editor case study, which is a \\nrandom forest, we can simply use scikit-learn’s API to obtain a ranked list \\nof the importance of all features. The feature importance code and its \\nusages can be found in the feature importance notebook in this book’s \\nGitHub repository. \\ndef get_feature_importance(clf, feature_names): \\n    importances = clf.feature_importances_ \\n    indices_sorted_by_importance = np.argsort(importances)[::-1] \\n    return list( \\n        zip( \\n            feature_names[indices_sorted_by_importance], \\n            importances[indices_sorted_by_importance], \\n        ) \\n    ) \\nIf we use the function above on our trained model, with some simple list \\nprocessing we can get a simple list of the ten most informative features: \\nTop 10 importances: \\n \\ntext_len: 0.0091 \\nare: 0.006 \\nwhat: 0.0051 \\nwriting: 0.0048 \\ncan: 0.0043 \\nve: 0.0041 \\non: 0.0039 \\nnot: 0.0039 \\nstory: 0.0039 \\nas: 0.0038 \\nThere are a few things to notice here: \\n\\uf0b7 The length of the text is the most informative feature. \\n\\uf0b7 The other features we generated do not appear at all, with \\nimportances more than an order of magnitude lower than others. The \\nmodel was not able to leverage them to meaningfully separate \\nclasses. \\n\\uf0b7 The other features represent either very common words, or nouns \\nrelevant to the topic of writing. \\nBecause our model and features are simple, these results can actually give \\nus ideas for new features to build. We could, for example, add a feature'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 155}, page_content='that counts the usage of common and rare words to see if they are \\npredictive of an answer receiving a high score. \\nIf features or models become complex, generating feature importances \\nrequires using model explainability tools. \\nBlack-Box Explainers \\nWhen features become complicated, feature importances can become \\nharder to interpret. Some more complex models such as neural networks \\nmay not even be able to expose their learned feature importances. In such \\nsituations, it can be useful to leverage black-box explainers, which attempt \\nto explain a model’s predictions independently of its inner workings. \\nCommonly, these explainers identify predictive features for a model on a \\ngiven data point instead of globally. They do this by changing each feature \\nvalue for a given example and observing how the model’s predictions \\nchange as a consequence. LIME and SHAP are two popular black-box \\nexplainers. \\nFor an end-to-end example of using these, see the black-box explainer \\nnotebook in the book’s GitHub repository. \\nFigure 5-12 shows an explanation provided by LIME around which words \\nwere most important in deciding to classify this example question as high \\nquality. LIME generated these explanations by repeatedly removing words \\nfrom the input question and seeing which words make our model lean \\nmore towards one class or another. \\n \\nFigure 5-12. Explaining one particular example \\nWe can see that the model correctly predicted that the question would \\nreceive a high score. However, the model was not confident, outputting \\nonly a 52% probability. The right side of Figure 5-12 shows the most'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 156}, page_content='impactful words for the prediction. These words do not seem like they \\nshould be particularly relevant to a question being of high quality, so let’s \\nexamine more examples to see if the model leverages more useful patterns. \\nTo get a quick sense of trends, we can use LIME on a larger sample of \\nquestions. Running LIME on each question and aggregating the results can \\ngive us an idea of which word our model finds predictive overall to make \\nits decisions. \\nIn Figure 5-13 we plot the most important predictions across 500 questions \\nin our dataset. We can see the trend of our model leveraging common \\nwords is apparent in this larger sample as well. It seems that the model is \\nhaving a hard time generalizing beyond leveraging frequent words. The \\nbag of words features representing rare words most often have a value of \\nzero. To improve on this, we could either gather a larger dataset to expose \\nour models to a more varied vocabulary, or create features that will be less \\nsparse.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 157}, page_content='Figure 5-13. Explaining multiple examples \\nYou’ll often be surprised by the predictors your model ends up using. If \\nany features are more predictive for the model than you’d expect, try to \\nfind examples containing these features in your training data and examine \\nthem. Use this opportunity to double check how you split your dataset and \\nwatch for data leakage. \\nWhen building a model to automatically classify emails into different \\ntopics based on their content, for example, an ML Engineer I was \\nmentoring once found that the best predictor was a three-letter code at the \\ntop of the email. It turns out this was an internal code to the dataset that \\nmapped almost perfectly to the labels. The model was entirely ignoring the \\ncontent of the email, and memorizing a pre-existing label. This was a clear'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 158}, page_content='example of data leakage, which was only caught by looking at feature \\nimportance. \\nConclusion \\nWe started this chapter by covering criteria to decide on an initial model \\nbased on all we’ve learned so far. Then, we covered the importance of \\nsplitting data into multiple sets, and methods to avoid data leakage. \\nAfter training an initial model, we took a deep dive into ways to judge how \\nwell it is performing by finding different ways to compare and contrast its \\npredictions to the data. Finally, we went on to inspect the model itself by \\ndisplaying feature importances and using a black-box explainer to gain an \\nintuition for the feature it uses to make predictions. \\nBy now, you should have some intuition about improvements you could \\nmake to your modeling. This takes us to the topic of Chapter 6 where we \\nwill take a deeper dive on methods to tackle the problems we’ve surfaced \\nhere, by debugging and troubleshooting an ML pipeline.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 159}, page_content='Chapter 6. Debug Your ML Problems \\nIn the previous chapter, we trained and evaluated our first model. \\nGetting a pipeline to a satisfactory level of performance is hard and \\nrequires multiple iterations. The goal of this chapter is to guide you \\nthrough one such iteration cycle. In this chapter, I will cover tools to debug \\nmodeling pipelines and ways to write tests to make sure they stay working \\nonce we start changing them. \\nSoftware best practices encourage practitioners to regularly test, validate, \\nand inspect their code, especially for sensitive steps such as security or \\ninput parsing. This should be no different for ML, where errors in a model \\ncan be much harder to detect than in traditional software. \\nWe will cover some tips that will help you make sure that your pipeline is \\nrobust and that you can try it out without causing your entire system to \\nfail, but first let’s dig into software best practices! \\nSoftware Best Practices \\nFor most ML projects, you will repeat the process of building a model, \\nanalyzing its shortcomings, and addressing them multiple times. You are \\nalso likely to change each part of your infrastructure more than once, so it \\nis crucial to find methods to increase iteration speed. \\nIn ML just like with any other software project, you should follow time-\\ntested software best practices. Most of them can be applied to ML projects \\nwith no modifications, such as building only what you need, often referred \\nto as the Keep It Stupid Simple (KISS) principle. \\nML projects are iterative in nature and go through many different \\niterations of data cleaning and feature generation algorithms, as well as \\nmodel choices. Even when following these best practices, two areas often \\nend up slowing down iteration speed: debugging and testing. Speeding up \\ndebugging and test writing can have a significant impact on any projects \\nbut is even more crucial for ML projects, where the stochastic nature of \\nmodels often turns a simple error into a days-long investigation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 160}, page_content='Many resources exist to help you learn how to debug general programs, \\nsuch as the University of Chicago’s concise debugging guide. If, like most \\nML practitioners, your language of choice is Python, I recommend looking \\nthrough the Python documentation for pdb, the standard library debugger. \\nMore than most pieces of software, however, ML code can often execute \\nseemingly correctly but produce entirely absurd results. This means that \\nwhile these tools and tips apply as is to most ML code, they are not \\nsufficient to diagnose common problems. I illustrate this in Figure 6-1: \\nwhile in most software applications, having strong test coverage can give \\nus a high level of confidence that our application is functioning well, ML \\npipelines can pass many tests but still give entirely incorrect results. An \\nML program doesn’t just have to run—it should produce accurate \\npredictive outputs. \\n \\nFigure 6-1. An ML pipeline can execute with no errors and still be wrong \\nBecause ML presents an additional set of challenges when it comes to \\ndebugging, let’s cover a few specific methods that help. \\nML-Specific Best Practices \\nWhen it comes to ML more than any type of software, merely having a \\nprogram execute end-to-end is not sufficient to be convinced of its \\ncorrectness. An entire pipeline can run with no errors and produce an \\nentirely useless model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 161}, page_content='Let’s say your program loads data and passes it to a model. Your model \\ntakes in these inputs and optimizes the model’s parameters based on a \\nlearning algorithm. Finally, your trained model produces outputs from a \\ndifferent set of data. Your program has run without any visible bugs. The \\nproblem is that just by having your program run, you have no guarantee at \\nall that your model’s predictions are correct. \\nMost models simply take a numerical input of a given shape (say a matrix \\nrepresenting an image) and output data of a different shape (a list of \\ncoordinates of key points in the input image, for example). This means that \\nmost models will still run even if a data processing step corrupted the data \\nbefore passing it to the model, as long as the data is still numeric and of a \\nshape the model can take as input. \\nIf your modeling pipeline performs poorly, how can you know whether it \\nis due to the quality of a model or the presence of a bug earlier in the \\nprocess? \\nThe best way to tackle these problems in ML is to follow a progressive \\napproach. Start by validating the data flow, then the learning capacity, and \\nfinally generalization and inference. Figure 6-2 shows an overview of the \\nprocess we will cover in this chapter.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 162}, page_content='Figure 6-2. The order in which to debug a pipeline \\nThis chapter will take you through each of these three steps, explaining \\neach one in depth. It can be tempting to skip steps in this plan when faced \\nwith a perplexing bug, but the vast majority of times I’ve found that \\nfollowing this principled approach is the fastest way to identify and correct \\nerrors. \\nLet’s start by validating the data flow. The simplest way to do this is by \\ntaking a very small subset of data and verifying that it can flow all the way \\nthrough your pipeline. \\nDebug Wiring: Visualizing and Testing \\nThis first step is simple and will make life dramatically simpler once you \\nadopt it: start by making your pipelines work for a small subset of \\nexamples in your dataset. This corresponds to the wiring step in Figure 6-\\n2. Once you’ve made sure your pipeline works for a few examples, you’ll \\nbe able to write tests to make sure your pipeline keeps functioning as you \\nmake changes. \\nStart with One Example \\nThe goal of this initial step is to verify that you are able to ingest data, \\ntransform it in the right format, pass it to a model, and have the model \\noutput something correct. At this stage, you aren’t judging whether your \\nmodel can learn something, just whether the pipeline can let data through. \\nConcretely this means: \\n\\uf0b7 Selecting a few examples in your dataset \\n\\uf0b7 Getting your model to output a prediction for these examples \\n\\uf0b7 Getting your model to update its parameters to output the correct \\npredictions for these examples \\nThe first two items are focused on verifying that our model can ingest \\ninput data and produce a reasonable-looking output. This initial output will \\nmost likely be wrong from a modeling perspective but will allow us to \\ncheck that the data is flowing all the way through.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 163}, page_content='The last item aims to make sure our model has the ability to learn a \\nmapping from a given input to the associated output. Fitting a few data \\npoints will not produce a useful model and will likely lead to overfitting. \\nThis process simply allows us to validate that the model can update its \\nparameters to fit a set of inputs and outputs. \\nHere is how this first step would look in practice: if you are training a \\nmodel to predict whether Kickstarter campaigns will be successful, you \\nmay be planning on training it on all campaigns from the last few years. \\nFollowing this tip, you should start by checking whether your model can \\noutput a prediction for two campaigns. Then, use the label for these \\ncampaigns (whether they were successful or not) to optimize the model’s \\nparameters until it predicts the correct outcome. \\nIf we have chosen our model appropriately, it should have the capacity to \\nlearn from our dataset. And if our model can learn from our entire dataset, \\nit should have the capacity to memorize a data point. The ability to learn \\nfrom a few examples is a necessary condition for a model to learn from an \\nentire dataset. It is also much easier to validate than the entire learning \\nprocess, so starting with one allows us to quickly narrow down any \\npotential future problems. \\nThe vast majority of errors that can come up at this initial stage relate to \\ndata mismatch: the data you are loading and preprocessing is fed to your \\nmodel in a format that it cannot accept. Since most models accept only \\nnumerical values, for example, they may fail when a given value is left \\nempty and has a null value. \\nSome cases of mismatch can be more elusive and lead to silent failure. A \\npipeline fed values that are not in the correct range or shape may still run \\nbut would produce a poorly performing model. Models that require \\nnormalized data will often still train on nonnormalized data: they simply \\nwill not be able to fit it in a useful manner. Similarly, feeding a matrix of \\nthe wrong shape to a model can cause it to misinterpret the input and \\nproduce incorrect outputs. \\nCatching such errors is harder, because they will manifest later in the \\nprocess once we evaluate the performance of a model. The best way to \\nproactively detect them is to visualize data as you build your pipeline and \\nbuild tests to encode assumptions. We will see how to do this next.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 164}, page_content='VISUALIZATION STEPS \\nAs we’ve seen in previous chapters, while metrics are a crucial part of \\nmodeling work, regularly inspecting and investigating our data is equally \\nimportant. Observing just a few examples to start makes it easier to notice \\nchanges or inconsistencies. \\nThe goal of this process is to inspect changes at regular intervals. If you \\nthink of a data pipeline as an assembly line, you’d want to inspect the \\nproduct after every meaningful change. This means checking the value of \\nyour datapoint at every line is probably too frequent, and looking only at \\nthe input and output values is definitely not informative enough. \\nIn Figure 6-3, I illustrate a few example inspection points you could use to \\ntake a look at a data pipeline. In this example, we inspect the data at \\nmultiple steps, starting with raw data all the way to model outputs. \\n \\nFigure 6-3. Potential inspection points \\nNext, we’ll cover a few key steps that are often worth inspecting. We’ll \\nstart with data loading and move on to cleaning, feature generation, \\nformatting, and model outputs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 165}, page_content='DATA LOADING \\nWhether you are loading your data from disk or through an API call, you \\nwill want to verify that it is formatted correctly. This process is similar to \\nthe one you go through when doing EDA but is done here within the \\ncontext of the pipeline you have built to verify that no error has led to the \\ndata becoming corrupted. \\nDoes it contain all the fields you expect it to? Are any of these fields null \\nor of constant value? Do any of the values lie in a range that seems \\nincorrect, such as an age variable sometimes being negative? If you are \\nworking with text or speech or images, do the examples match your \\nexpectations of what they would look, sound, or read like? \\nMost of our processing steps rely on assumptions we make about the \\nstructure of our input data, so it is crucial to validate this aspect. \\nBecause the goal here is to identify inconsistencies between our \\nexpectation of the data and reality, you may want to visualize more than \\none or two data points. Visualizing a representative sample will assure we \\ndo not only observe a “lucky” example and wrongly assume all data points \\nare of the same quality. \\nFigure 6-4 shows an example for our case study from the dataset \\nexploration notebook in this book’s GitHub repository. Here, hundreds of \\nposts in our archive are of an undocumented post type and thus need to be \\nfiltered out. In the figure, you can see rows with a PostTypeId of 5, which \\nis not referenced in the dataset documentation and that we thus remove \\nfrom the training data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 166}, page_content='Figure 6-4. Visualizing a few rows of data \\nOnce you’ve verified that the data conforms to expectations laid out in the \\ndataset documentation, it is time to start processing it for modeling \\npurposes. This starts with data cleaning. \\nCLEANING AND FEATURE SELECTION \\nThe next step in most pipelines is to remove any unnecessary information. \\nThis can include fields or values that are not going to be used by the model \\nbut also any fields that may contain information about our label that our \\nmodel would not have access to in production (see “Split Your Dataset”). \\nRemember that each feature you remove is a potential predictor for your \\nmodel. The task of deciding which features to keep and which features to \\nremove is called feature selection and is an integral part of iterating on \\nmodels. \\nYou should verify that no crucial information is lost, that all unneeded \\nvalues are removed, and that you have not left any extra information in our \\ndataset that will artificially boost our model’s performance by leaking \\ninformation (see “Data leakage”). \\nOnce the data is cleaned, you’ll want to generate some features for your \\nmodel to use. \\nFEATURE GENERATION \\nWhen generating a new feature, such as adding the frequency of references \\nto a product name in the description of a kickstarter campaign, for \\nexample, it is important to inspect its values. You need to check that the \\nfeature values are populated and that the values seem reasonable. This is a \\nchallenging task, as it requires not only identifying all features but \\nestimating reasonable values for each of these features. \\nAt this point, you do not need to analyze it any deeper, as this step is \\nfocusing on validating assumptions about data flowing through the model, \\nnot the usefulness of the data or the model yet. \\nOnce features have been generated, you should make sure they can be \\npassed to the model in a format it can understand.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 167}, page_content='DATA FORMATTING \\nAs we’ve discussed in earlier chapters, before passing data points to a \\nmodel, you will need to transform them to a format it can understand. This \\ncan include normalizing input values, vectorizing text by representing it in \\na numerical fashion, or formatting a black and white video as a 3D tensor \\n(see “Vectorizing”). \\nIf you are working on a supervised problem, you’ll use a label in addition \\nto the input, such as class names in classification, or a segmentation map \\nin image segmentation. These will also need to be transformed to a model-\\nunderstandable format. \\nIn my experience working on multiple image segmentation problems, for \\nexample, data mismatch between labels and model predictions is one of \\nthe most common causes of errors. Segmentation models use segmentation \\nmasks as labels. These masks are the same size as the input image, but \\ninstead of pixel values, they contain class labels for each pixel. \\nUnfortunately, different libraries use different conventions to represent \\nthese masks, so the labels often end up in the wrong format, preventing the \\nmodel from learning. \\nI’ve illustrated this common pitfall in Figure 6-5. Let’s say a model \\nexpects segmentation masks to be passed with a value of 255 for pixels \\nthat are of a certain class, and 0 otherwise. If a user instead assumes that \\npixels contained within the mask should have a value of 1 instead of 255, \\nthey may pass their labeled masks in the format seen in “provided.” This \\nwould lead the mask to be considered as almost entirely empty, and the \\nmodel would output inaccurate predictions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 168}, page_content='Figure 6-5. Poorly formatted labels will prevent a model from learning \\nSimilarly, classification labels are often represented as a list of zeros with \\na single one at the index of the true class. A simple off-by-one error can \\nlead to labels being shifted and a model learning to always predict the \\nshifted-by-one label. This kind of error can be hard to troubleshoot if you \\ndo not take the time to look at your data. \\nBecause ML models will manage to fit to most numerical outputs \\nregardless of whether they have an accurate structure or content, this stage \\nis where many tricky bugs occur and where this method is useful to find \\nthem. \\nHere is an example of what such a formatting function looks like for our \\ncase study. I generate a vectorized representation of our question text. \\nThen, I append additional features to this representation. Since the \\nfunction consists of multiple transformations and vector operations, \\nvisualizing the return value of this function will allow me to verify that it \\ndoes format data the way we intend it to. \\ndef get_feature_vector_and_label(df, feature_names): \\n    \"\"\" \\n    Generate input and output vectors using the vectors feature and \\n     the given feature names \\n    :param df: input DataFrame \\n    :param feature_names: names of feature columns (other than vectors) \\n    :return: feature array and label array \\n    \"\"\" \\n    vec_features = vstack(df[\"vectors\"]) \\n    num_features = df[feature_names].astype(float) \\n    features = hstack([vec_features, num_features]) \\n    labels = df[\"Score\"] > df[\"Score\"].median() \\n    return features, labels \\n \\nfeatures = [ \\n    \"action_verb_full\", \\n    \"question_mark_full\", \\n    \"text_len\", \\n    \"language_question\", \\n] \\n \\nX_train, y_train = get_feature_vector_and_label(train_df, features) \\nWhen working with text data especially, there are usually multiple steps \\ninvolved before data is properly formatted for a model. Going from a \\nstring of text to a tokenized list to a vectorized representation including \\npotential additional features is an error-prone process. Even inspecting the \\nshape of the objects at each step can help catch many simple mistakes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 169}, page_content='Once the data is in the appropriate format, you can pass it to a model. The \\nlast step is to visualize and validate the model’s outputs. \\nMODEL OUTPUT \\nAt first, looking at outputs helps us see whether our model’s predictions \\nare the right type or shape (if we are predicting house price and duration \\non market, is our model outputting an array of two numbers?). \\nIn addition, when fitting a model to only a couple data points, we should \\nsee its outputs start matching the true label. If the model doesn’t fit the \\ndata points, this may be an indication of the data being incorrectly \\nformatted or becoming corrupted. \\nIf the output of the model does not change at all during training, this may \\nmean that our model is actually not leveraging the input data. In such a \\ncase, I recommend referring to “Stand on the Shoulders of Giants” to \\nvalidate that the model is being used correctly. \\nOnce we’ve gone through the entire pipeline for a few examples, it is time \\nto write a few tests to automate some of this visualization work. \\nSYSTEMATIZING OUR VISUAL VALIDATION \\nGoing through the visualization work described earlier helps catch a \\nsignificant amount of bugs and is a good time investment for every novel \\npipeline. Validating assumptions about how data is flowing through the \\nmodel helps save a significant amount of time down the line, which can \\nnow be spent focusing on training and generalization. \\nPipelines change often, however. As you update different aspects \\niteratively to improve your model and modify some of the processing \\nlogic, how can you guarantee that everything is still working as intended? \\nGoing through the pipeline and visualizing an example at all steps each \\ntime you make any change would quickly get tiring. \\nThis is where the software engineering best practices we talked about \\nearlier come into play. It is time to isolate each part of this pipeline, and \\nencode our observations into tests that we will be able to run as our \\npipeline changes, to validate it.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 170}, page_content='SEPARATE YOUR CONCERNS \\nJust like regular software, ML benefits greatly from a modular \\norganization. To make current and future debugging easier, separate each \\nfunction so that you can check that it individually works before looking at \\nthe broader pipeline. \\nOnce a pipeline is broken down into individual functions, you’ll be able to \\nwrite tests for them. \\nTest Your ML Code \\nTesting a model’s behavior is hard. The majority of code in an ML \\npipeline is not about the training pipeline or the model itself, however. If \\nyou look back to our pipeline example in “Start with a Simple Pipeline”, \\nmost functions behave in a deterministic way and can be tested. \\nIn my experience, helping engineers and data scientists debug their \\nmodels, I’ve learned that the vast majority of errors come from the way \\ndata is acquired, processed, or fed to the model. Testing data processing \\nlogic is thus crucial in order to build a successful ML product. \\nFor even more information about potential tests of an ML system, I \\nrecommend the paper by E. Breck et al., “The ML Test Score: A Rubric \\nfor ML Production Readiness and Technical Debt Reduction”, which \\ncontains many more examples and lessons learned from deploying such \\nsystems at Google. \\nIn this next section, we will describe useful tests to write for three key \\nareas. In Figure 6-6, you can see each of these areas, along with a few \\nexamples of tests that we will describe next.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 171}, page_content='Figure 6-6. The three key areas to test \\nPipelines start by ingesting data, so we’ll want to test that part first. \\nTEST DATA INGESTION \\nData usually lives serialized on a disk or in a database. When moving data \\nfrom storage to our pipeline, we should make sure to verify the integrity \\nand correctness of the data. We can start by writing tests that verify that \\nthe data points we load possess every feature we’ll need. \\nThe following are three tests validating that our parser returns the right \\ntype (a dataframe), that all important columns are defined, and that \\nfeatures are not all null. You can find the tests we’ll cover in this chapter \\n(and additional ones) in the tests folder on this book’s GitHub repository. \\ndef test_parser_returns_dataframe(): \\n    \"\"\" \\n    Tests that our parser runs and returns a DataFrame \\n    \"\"\" \\n    df = get_fixture_df() \\n    assert isinstance(df, pd.DataFrame) \\n \\n \\ndef test_feature_columns_exist(): \\n    \"\"\" \\n    Validate that all required columns are present \\n    \"\"\" \\n    df = get_fixture_df() \\n    for col in REQUIRED_COLUMNS: \\n        assert col in df.columns \\n \\n \\ndef test_features_not_all_null(): \\n    \"\"\" \\n    Validate that no features are missing every value'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 172}, page_content='\"\"\" \\n    df = get_fixture_df() \\n    for col in REQUIRED_COLUMNS: \\n        assert not df[col].isnull().all() \\nWe can also test each feature for its type and validate that it is not null. \\nFinally, we can encode assumptions we have about the distribution and \\nranges of these values by testing their average, minimum, and maximum \\nvalues. Recently, libraries such as Great Expectations have emerged to test \\nthe distributions of features directly. \\nHere, you can see how you could write a simple mean test: \\nACCEPTABLE_TEXT_LENGTH_MEANS = pd.Interval(left=20, right=2000) \\n \\ndef test_text_mean(): \\n    \"\"\" \\n    Validate that text mean matches with exploration expectations \\n    \"\"\" \\n    df = get_fixture_df() \\n    df[\"text_len\"] = df[\"body_text\"].str.len() \\n    text_col_mean = df[\"text_len\"].mean() \\n    assert text_col_mean in ACCEPTABLE_TEXT_LENGTH_MEANS \\nThese tests allow us to verify that no matter which changes are made on \\nthe storage side or with the API of our data source, we can know that our \\nmodel has access to the same kind of data it was first trained on. Once \\nwe’re confident as to the consistency of the data we ingest, let’s look at the \\nnext step in the pipeline, data processing. \\nTEST DATA PROCESSING \\nAfter testing that the data that makes it to the beginning of our pipeline \\nconforms to our expectations, we should test that our cleaning and feature \\ngeneration steps do what we expect. We can start by writing tests for the \\npreprocessing function we have, verifying that it does indeed do what we \\nintend it to. Also, we can write similar tests to the data ingestion ones and \\nfocus on guaranteeing that our assumptions about the state of the data \\ngoing into our model are valid. \\nThis means testing for the presence, type, and characteristics of the data \\npoints after our processing pipeline. The following are examples of tests \\nfor the presence of generated features, their type, and minimum, \\nmaximum, and mean values: \\ndef test_feature_presence(df_with_features):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 173}, page_content='for feat in REQUIRED_FEATURES: \\n        assert feat in df_with_features.columns \\n \\ndef test_feature_type(df_with_features): \\n    assert df_with_features[\"is_question\"].dtype == bool \\n    assert df_with_features[\"action_verb_full\"].dtype == bool \\n    assert df_with_features[\"language_question\"].dtype == bool \\n    assert df_with_features[\"question_mark_full\"].dtype == bool \\n    assert df_with_features[\"norm_text_len\"].dtype == float \\n    assert df_with_features[\"vectors\"].dtype == list \\n \\n  def test_normalized_text_length(df_with_features): \\n    normalized_mean = df_with_features[\"norm_text_len\"].mean() \\n    normalized_max = df_with_features[\"norm_text_len\"].max() \\n    normalized_min = df_with_features[\"norm_text_len\"].min() \\n    assert normalized_mean in pd.Interval(left=-1, right=1) \\n    assert normalized_max in pd.Interval(left=-1, right=1) \\n    assert normalized_min in pd.Interval(left=-1, right=1) \\nThese tests allow us to notice any changes to our pipelines that impact the \\ninput to our model without having to write any additional tests. We will \\nonly need to write new tests when we add new features or change the input \\nto our model. \\nWe can now feel confident both in the data we ingest and in the \\ntransformations we apply to it, so it is time to test the next part of the \\npipeline, the model. \\nTEST MODEL OUTPUTS \\nSimilarly to the two previous categories, we will write tests to validate that \\nthe values the model outputs have the correct dimensions and ranges. We \\nwill also test predictions for specific inputs. This helps proactively detect \\nregressions in prediction quality in new models and guarantee that any \\nmodel we use always produces the expected output on these example \\ninputs. When a new model shows better aggregate performance, it can be \\nhard to notice whether its performance worsened on specific types of \\ninputs. Writing such tests helps detect such issues more easily. \\nIn the following examples, I start by testing the shape of the predictions of \\nour model, as well as their values. The third test aims to prevent \\nregressions by guaranteeing that the model classifies a specific poorly \\nworded input question as low quality. \\ndef test_model_prediction_dimensions( \\n    df_with_features, trained_v1_vectorizer, trained_v1_model \\n):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 174}, page_content='df_with_features[\"vectors\"] = get_vectorized_series( \\n        df_with_features[\"full_text\"].copy(), trained_v1_vectorizer \\n    ) \\n \\n    features, labels = get_feature_vector_and_label( \\n        df_with_features, FEATURE_NAMES \\n    ) \\n \\n    probas = trained_v1_model.predict_proba(features) \\n    # the model makes one prediction per input example \\n    assert probas.shape[0] == features.shape[0] \\n    # the model predicts probabilities for two classes \\n    assert probas.shape[1] == 2 \\n \\n \\ndef test_model_proba_values( \\n    df_with_features, trained_v1_vectorizer, trained_v1_model \\n): \\n    df_with_features[\"vectors\"] = get_vectorized_series( \\n        df_with_features[\"full_text\"].copy(), trained_v1_vectorizer \\n    ) \\n \\n    features, labels = get_feature_vector_and_label( \\n        df_with_features, FEATURE_NAMES \\n    ) \\n \\n    probas = trained_v1_model.predict_proba(features) \\n    # the model\\'s probabilities are between 0 and 1 \\n    assert (probas >= 0).all() and (probas <= 1).all() \\n \\n \\ndef test_model_predicts_no_on_bad_question(): \\n    input_text = \"This isn\\'t even a question. We should score it \\npoorly\" \\n    is_question_good = \\nget_model_predictions_for_input_texts([input_text]) \\n    # The model classifies the question as poor \\n    assert not is_question_good[0] \\nWe first visually inspected the data to verify it remained useful and usable \\nthroughout our pipeline. Then, we wrote tests to guarantee these \\nassumptions remain correct as our processing strategy evolves. It is now \\ntime to tackle the second part of Figure 6-2, debugging the training \\nprocedure. \\nDebug Training: Make Your Model Learn \\nOnce you’ve tested your pipeline and validated that it works for one \\nexample, you know a few things. Your pipeline takes in data and \\nsuccessfully transforms it. It then passes this data to a model in the right'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 175}, page_content='format. Finally, the model can take a few data points and learn from them, \\noutputting the correct results. \\nIt is now time to see whether your model can work on more than a few \\ndata points and learn from your training set. The focus of this next section \\nis on being able to train your model on many examples and have it fit to all \\nof your training data. \\nTo do so, you can now pass your entire training set to your model and \\nmeasure its performance. Alternatively, if you have a large amount of data, \\nyou can instead gradually increase the quantity of data you feed to your \\nmodel while keeping an eye on aggregate performance. \\nOne advantage of progressively increasing the size of your training dataset \\nis that you’ll be able to measure the effect of additional data on the \\nperformance of your model. Start with a few hundred examples, and then \\nmove to a few thousand, before passing in your whole dataset (if your \\ndataset is smaller than a thousand examples, feel free to skip straight to \\nusing it in its entirety). \\nAt each step, fit your model on the data and evaluate its performance on \\nthe same data. If your model has the capacity to learn from the data you \\nare using, its performance on the training data should stay relatively stable. \\nTo contextualize model performance, I recommend generating an estimate \\nof what an acceptable error level for your task is by labeling a few \\nexamples yourself, for example, and comparing your predictions to the \\ntrue label. Most tasks also come with an irreducible error, representing the \\nbest performance given the complexity of the task. See Figure 6-7 for an \\nillustration of usual training performance compared to such metrics. \\nA model’s performance on the whole dataset should be worse than when \\nusing only one example, since memorizing an entire training set is harder \\nthan a single example, but should still remain within the boundaries \\ndefined earlier. \\nIf you are able to feed your entire training set and the performance of your \\nmodel reaches the requirement you defined when looking at your product \\ngoal, feel free to move on to the next section! If not, I’ve outlined a couple \\ncommon reasons a model can struggle on a training set in the next section.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 176}, page_content='Figure 6-7. Training accuracy as a function of dataset size \\nTask Difficulty \\nIf a model’s performance is drastically lower than expected, the task may \\nbe too difficult. To evaluate how hard a task is, consider: \\n\\uf0b7 The quantity and diversity of data you have \\n\\uf0b7 How predictive the features you have generated are \\n\\uf0b7 The complexity of your model \\nLet’s look at each of those in a little more detail. \\nDATA QUALITY, QUANTITY, AND DIVERSITY \\nThe more diverse and complex your problem is, the more data you will \\nneed for a model to learn from it. For your model to learn patterns, you \\nshould aim to have many examples of each type of data you have. If you \\nare classifying pictures of cats as one of a hundred possible breeds, for \\nexample, you’d need many more pictures than if you were simply trying to \\ntell cats apart from dogs. In fact, the quantity of data you need often scales \\nexponentially with the number of classes, as having more classes means \\nmore opportunities for misclassification.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 177}, page_content='In addition, the less data you have, the more impact any errors in your \\nlabels or any missing values have. This is why it is worth spending the \\ntime to inspect and verify features and labels of your dataset. \\nFinally, most datasets contain outliers, data points that are radically \\ndifferent from others and very hard for a model to handle. Removing \\noutliers from your training set can often improve the performance of a \\nmodel by simplifying the task at hand, but it is not always the right \\napproach: if you believe that your model may encounter similar data points \\nin production, you should keep outliers and focus on improving your data \\nand model so that the model can successfully fit to them. \\nThe more complex a dataset is, the more helpful it can be to work on ways \\nto represent your data that will make it easier for a model to learn from it. \\nLet’s look at what this means. \\nDATA REPRESENTATION \\nHow easy is it to detect the patterns you care about using only the \\nrepresentation you give your model? If a model is struggling to perform \\nwell on training data, you should add features that make the data more \\nexpressive and thus help the model learn better. \\nThis can consist of novel features we had previously decided to ignore but \\nthat may be predictive. In our ML Editor example, a first iteration of the \\nmodel only took into account the text in the body of a question. After \\nexploring the dataset further, I noticed that question titles are often very \\ninformative as to whether a question is good or not. Incorporating that \\nfeature back into the dataset allowed the model to perform better. \\nNew features can often be generated by iterating on existing ones or \\ncombining them in a creative manner. We saw an example of this in “Let \\nData Inform Features and Models”, when we looked at ways to combine \\nthe day of the week and day of the month to generate a feature that was \\nrelevant to a particular business case. \\nIn some cases, the problem lies with your model. Let’s look at these cases \\nnext.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 178}, page_content='MODEL CAPACITY \\nIncreasing data quality and improving features often provides the largest \\nbenefits. When a model is the cause for poor performance, it can often \\nmean that it is not adequate to the task at hand. As we saw in “From \\nPatterns to Models”, specific datasets and problems call for specific \\nmodels. A model that is not appropriate for a task will struggle to perform \\non it, even if it was able to overfit a few examples. \\nIf a model struggles on a dataset that seems to have many predictive \\nfeatures, start by asking yourself whether you are using the right type of \\nmodel. If possible, use a simpler version of the given model to more easily \\ninspect it. For example, if a random forest model isn’t performing at all, \\ntry a decision tree on the same task and visualize its splits to examine \\nwhether they use the features you thought would be predictive. \\nOn the other hand, the model you are using may be too simple. Starting \\nwith the simplest model is good to quickly iterate, but some tasks are \\nentirely out of reach of some models. To tackle them, you may need to add \\ncomplexity to your model. To verify that a model is indeed adapted to a \\ntask, I recommend looking at prior art as we described in “Stand on the \\nShoulders of Giants”. Find examples of similar tasks, and examine which \\nmodels were used to tackle them. Using one of those models should be a \\ngood starting point. \\nIf the model seems appropriate for the task, its lackluster performance \\ncould be due to the training procedure. \\nOptimization Problems \\nStarting by validating that a model can fit a small set of examples makes \\nus confident that data can flow back and forth. We do not know, however, \\nwhether our training procedure can adequately fit a model to the entire \\ndataset. The method that our model is using to update its weights may be \\ninadequate for our current dataset. Such problems often occur in more \\ncomplex models such as neural networks, where hyperparameter choice \\ncan have a significant impact on training performance. \\nWhen dealing with models that are fit using gradient descent techniques \\nsuch as neural networks, using visualization tools such as TensorBoard can \\nhelp surface training problems. When plotting the loss during your'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 179}, page_content='optimization process, you should see it decline steeply initially and then \\ngradually. In Figure 6-8, you can see an example of a TensorBoard \\ndashboard depicting a loss function (cross-entropy in this case) as training \\nprogresses. \\nSuch a curve can show that the loss is decreasing very slowly, indicating \\nthat a model may be learning too slowly. In such a case, you could \\nincrease the learning rate and plot the same curve to see whether the loss \\ndecreases faster. If a loss curve looks very unstable, on the other hand, it \\nmay be due to the learning rate being too large. \\n \\nFigure 6-8. TensorBoard dashboard screenshot from the TensorBoard documentation  \\nIn addition to the loss, visualizing weight values and activations can help \\nyou identify if a network is not learning properly. In Figure 6-9, you can \\nsee a change in the distribution of the weights as training progresses. If \\nyou see the distributions remain stable for a few epochs, it may be a sign \\nthat you should increase the learning rate. If they vary too much, lower it \\ninstead.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 180}, page_content='Figure 6-9. Weight histograms changing as training progresses \\nSuccessfully fitting a model to the training data is an important milestone \\nin an ML project, but it is not the last step. The end goal of building an ML \\nproduct is to build a model that can perform well on examples it has never \\nseen before. To do this, we need a model that can generalize well to \\nunseen examples, so I’ll cover generalization next. \\nDebug Generalization: Make Your Model Useful \\nGeneralization is the third and final part of Figure 6-2 and focuses on \\ngetting an ML model to work well on data it has not seen before. In “Split \\nYour Dataset”, we saw the importance of creating separate training, \\nvalidation, and test splits to evaluate a model’s ability to generalize to \\nunseen examples. In “Evaluate Your Model: Look Beyond Accuracy”, we \\ncovered methods to analyze the performance of a model and identify \\npotential additional features to help improve it. Here, we’ll cover some \\nrecommendations when a model still fails to perform on the validation set \\nafter multiple iterations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 181}, page_content='Data Leakage \\nWe covered data leakage in more detail in “Data leakage”, but I want to \\nmention it here within the context of generalization. A model will often \\ninitially perform worse on the validation set than the training set. This is to \\nbe expected since it is harder to make predictions on data that a model has \\nnot been exposed to before than on data it was trained to fit. \\nNOTE \\nWhen looking at validation loss and training loss during training before it \\nis complete, validation performance may appear better than training \\nperformance. This is because the training loss accumulates over the epoch \\nas the model is trained, while the validation loss is calculated after the \\nepoch has completed, using the latest version of the model. \\nIf validation performance is better than training performance, it can \\nsometimes be due to data leakage. If examples in the training data contain \\ninformation about others in the validation data, a model will be able to \\nleverage this information and perform well on the validation set. If you are \\nsurprised by validation performance, inspect the features a model uses and \\nsee if they show data leakage. Fixing such a leakage issue will lead to a \\nlower validation performance, but a better model. \\nData leakage can lead us to believe that a model is generalizing when it \\nreally isn’t. In other cases, it is clear from looking at performance on a \\nheld-out validation set that the model performs well only on training. In \\nthose kinds of cases, the model may be overfitting. \\nOverfitting \\nIn “Bias variance trade-off”, we saw that when a model is struggling to fit \\nthe training data, we say the model is underfitting. We also saw that the \\nopposite of underfitting is overfitting, and this is when our model fits our \\ntraining data too well. \\nWhat does fitting data too well mean? It means that instead of learning \\ngeneralizable trends that correlate with good or poor writing, for example, \\na model may pick up on specific patterns present in individual examples in \\na training set that are not present in different data. Those patterns help it'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 182}, page_content='get a high score on the training set but aren’t useful to classify other \\nexamples. \\nFigure 6-10 shows a practical example of overfitting and underfitting for a \\ntoy dataset. The overfit model fits the training data perfectly but doesn’t \\naccurately approximate the underlying trend; thus, it fails to accurately \\npredict unseen points. The underfit model does not capture the trend of the \\ndata at all. The model labeled reasonable fit performs worse on the training \\ndata than the overfit model but better on unseen data. \\n \\nFigure 6-10. Overfitting versus underfitting \\nWhen a model performs drastically better on the training set than on the \\ntest set, that usually means that it is overfit. It has learned the specific \\ndetails of the training data but is not able to perform on unseen data. \\nSince overfitting is due to a model learning too much about training data, \\nwe can prevent it by reducing the ability of a model to learn from a \\ndataset. There are a few ways to do that, which we’ll cover here. \\nREGULARIZATION \\nRegularization adds a penalty on a model’s capacity to represent \\ninformation. Regularizing aims to limit the ability of a model to focus on \\nmany irrelevant patterns and encourages it to pick fewer, more predictive \\nfeatures.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 183}, page_content='A common way to regularize a model is to impose a penalty on the \\nabsolute value of its weights. For models such as linear and logistic \\nregression, for example, L1 and L2 regularization add an additional term \\nto the loss function that penalizes large weights. In the case of L1, this \\nterm is the sum of the absolute value of weights. For L2, it is the sum of \\nthe squared values of weight. \\nDifferent regularization methods have different effects. L1 \\nregularization can help select informative features by setting \\nuninformative ones to zero (read more on the “Lasso (statistics)” \\nWikipedia page). L1 regularization is also useful when some features are \\ncorrelated by encouraging the model to leverage only one of them. \\nRegularization methods can also be model specific. Neural networks often \\nuse dropout as a regularization method. Dropout randomly ignores some \\nproportion of neurons in a network during training. This prevents a single \\nneuron from becoming excessively influential, which could enable the \\nnetwork to memorize aspects of training data. \\nFor tree-based models such as random forests, reducing the maximum \\ndepth of trees reduces the ability of each tree to overfit to the data and thus \\nhelps regularize the forest. Increasing the number of trees used in a forest \\nalso regularizes it. \\nAnother way to prevent a model from overfitting to training data is to \\nmake the data itself harder to overfit to. We can do this through a process \\ncalled data augmentation. \\nDATA AUGMENTATION \\nData augmentation is the process of creating new training data by slightly \\naltering existing data points. The goal is to artificially produce data points \\nthat are different from existing ones in order to expose a model to a more \\nvaried type of input. Augmentation strategies depend on the type of data. \\nIn Figure 6-11, you can see a few potential augmentations for images.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 184}, page_content='Figure 6-11. A few examples of data augmentation for images \\nData augmentation makes a training set less homogeneous and thus more \\ncomplex. This makes fitting the training data harder but exposes a model \\nto a wider range of inputs during training. Data augmentation often leads \\nto lower performance on the training set but higher performance on unseen \\ndata such as a validation set and examples in production. This strategy is \\nespecially effective if we can use augmentation to make our training set \\nmore similar to examples in the wild. \\nI once helped an engineer use satellite imagery to detect flooded roads \\nafter a hurricane. The project was challenging, since he only had access to \\nlabeled data for non-flooded cities. To help improve his model’s \\nperformance on hurricane imagery, which is significantly darker and lower \\nquality, they built augmentation pipelines that made training images look \\ndarker and blurrier. This lowered training performance since the roads \\nwere now harder to detect. On the other hand, it increased the model’s \\nperformance on the validation set because the augmentation process \\nexposed the model to images that were more similar to the ones it would \\nencounter in the validation set. Data augmentation helped make the \\ntraining set more representative and thus made the model more robust.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 185}, page_content='If after using the methods described earlier a model still performs poorly \\non a validation set, you should iterate on the dataset itself. \\nDATASET REDESIGN \\nIn some cases, a difficult training/validation split can lead to a model \\nunderfitting and struggling on the validation set. If a model is exposed to \\nonly easy examples in its training set and only challenging ones in its \\nvalidation set, it will be unable to learn from difficult data points. \\nSimilarly, some categories of examples may be underrepresented in the \\ntraining set, preventing a model ever from learning from them. If a model \\nis trained to minimize an aggregate metric, it risks fitting mostly the \\nmajority of classes, ignoring minority ones. \\nWhile augmentation strategies can help, redesigning a training split to \\nmake it more representative is often the best path forward. When doing \\nthis, we should carefully control for data leakage and make the splits as \\nbalanced as possible in terms of difficulty. If the new data split allocates \\nall the easy examples to the validation set, the model’s performance on the \\nvalidation set will be artificially high, but it will not translate to results in \\nproduction. To alleviate concerns that data splits may be of unequal \\nquality, we can use k-fold cross-validation, where we perform k successive \\ndifferent splits, and measure the performance of the model on each split. \\nOnce we’ve balanced our training and validation set to make sure that they \\nare of similar complexity, our model’s performance should improve. If the \\nperformance is still not satisfactory, we may be simply tackling a really \\nhard problem. \\nConsider the Task at Hand \\nA model can struggle to generalize because the task is too complex. The \\ninput we are using may not be predictive of the target, for example. To \\nmake sure the task you are tackling is of appropriate difficulty for the \\ncurrent state of ML, I suggest referring once more to “Stand on the \\nShoulders of Giants”, where I described how to explore and evaluate the \\ncurrent state of the art. \\nIn addition, having a dataset does not mean that a task is solvable. \\nConsider the impossible task of accurately predicting random outputs from \\nrandom inputs. You could build a model that performs well on a training'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 186}, page_content='set by memorizing it, but this model would not be able to accurately \\npredict other random outputs from random inputs. \\nIf your models aren’t generalizing, your task may be too hard. There may \\nnot be enough information in your training examples to learn meaningful \\nfeatures that will be informative for future data points. If that is the case, \\nthen the problem you have is not well suited for ML, and I would invite \\nyou to revisit Chapter 1 to find a better framing. \\nConclusion \\nIn this chapter, we covered the three successive steps you should follow to \\nget a model to work. First, debug the wiring of your pipeline by inspecting \\nthe data and writing tests. Then, get a model to perform well on a training \\ntest to validate that it has the capacity to learn. Finally, verify that it is able \\nto generalize and produce useful outputs on unseen data. \\nThis process will help you debug models, build them faster, and make \\nthem more robust. Once you have built, trained, and debugged your first \\nmodel, the next step is to judge its performance and either iterate on it or \\ndeploy it. \\nIn Chapter 7, we will cover how to use a trained classifier to provide \\nactionable recommendations for users. We will then compare candidate \\nmodels for the ML Editor and decide which one should be used to power \\nthese recommendations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 187}, page_content='Chapter 7. Using Classifiers for Writing \\nRecommendations \\nThe best way to make progress in ML is through repeatedly following the \\niterative loop depicted in Figure 7-1, which we saw in the introduction to \\nPart III. Start by establishing a modeling hypothesis, iterate on a modeling \\npipeline, and perform detailed error analysis to inform your next \\nhypothesis. \\n \\nFigure 7-1. The ML loop \\nThe previous chapters described multiple steps in this loop. In Chapter 5, \\nwe covered how to train and score a model. In Chapter 6, we shared advice \\non how to build models faster and troubleshoot ML-related errors. This \\nchapter closes an iteration of the loop by first showcasing methods to use \\ntrained classifiers to provide suggestions to users, then selecting a model \\nto use for the ML Editor, and finally combining both to build a working \\nML Editor. \\nIn “ML Editor Planning” we outlined our plan for the ML Editor, which \\nconsists of training a model that classifies questions into high- and low-\\nscore categories and use this trained model to guide users to write better \\nquestions. Let’s see how we can use such a model to provide writing \\nadvice to users.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 188}, page_content='Extracting Recommendations from Models \\nThe goal of the ML Editor is to provide writing recommendations. \\nClassifying a question as good or bad is a first step in this direction since it \\nmakes it possible to display the current quality of a question to a user. \\nWe’d like to go one step beyond this and help users improve the \\nformulation of their questions by providing them with actionable \\nrecommendations. \\nThis section covers methods to provide such recommendations. We will \\nstart with simple approaches that rely on aggregate feature metrics and do \\nnot require the use of a model at inference time. Then, we will see how to \\nboth use a model’s score and its sensitivity to perturbations to generate \\nmore personalized recommendations. You can find examples of each of \\nthe methods showcased in this chapter applied to the ML Editor in the \\ngenerating recommendations notebook on this book’s GitHub site. \\nWhat Can We Achieve Without a Model? \\nTraining a model that performs well is achieved through multiple \\niterations of the ML loop. Each iteration helps create a better set of \\nfeatures through researching prior art, iterating on potential datasets, and \\nexamining model results. To provide users with recommendations, you can \\nleverage this feature iteration work. This approach does not necessarily \\nrequire running a model on each question a user submits and focuses \\ninstead on making general recommendations. \\nYou can do so either by using the features directly or by incorporating a \\ntrained model to help select relevant ones. \\nUSING FEATURE STATISTICS \\nOnce predictive features have been identified, they can be directly \\ncommunicated to a user without using a model. If the mean value of a \\nfeature is significantly different for each class, you can share this \\ninformation directly to help users nudge their examples in the direction of \\nthe target class. \\nOne of the features we identified early on for the ML Editor was the \\npresence of question marks. Inspecting the data showed that questions with \\nhigh scores tend to have fewer question marks. To use this information to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 189}, page_content='generate recommendations, we can write a rule that warns a user if the \\nproportion of question marks in their question is much larger than in \\nhighly rated questions. \\nVisualizing average feature values for each label can be done in a few \\nlines of code using pandas. \\nclass_feature_values = feats_labels.groupby(\"label\").mean() \\nclass_feature_values = class_feature_values.round(3) \\nclass_feature_values.transpose() \\nRunning the previous code produces the result shown in Table 7-1. In \\nthese results, we can see that many of the features we’ve generated have \\nsignificantly different values for high- and low-score questions, labeled \\nTrue and False here. \\nLabel \\nFalse \\nTrue \\nnum_questions \\n0.432 \\n0.409 \\nnum_periods \\n0.814 \\n0.754 \\nnum_commas \\n0.673 \\n0.728 \\nnum_exclam \\n0.019 \\n0.015 \\nnum_quotes \\n0.216 \\n0.199 \\nnum_colon \\n0.094 \\n0.081 \\nnum_stops \\n10.537 \\n10.610 \\nnum_semicolon \\n0.013 \\n0.014 \\nnum_words \\n21.638 \\n21.480 \\nnum_chars \\n822.104 \\n967.032 \\nTable 7-1. Differences in feature values between classes'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 190}, page_content='Using feature statistics is a simple way to provide robust \\nrecommendations. It is in many ways similar to the heuristic approach that \\nwe first built in “The Simplest Approach: Being the Algorithm”. \\nWhen comparing feature values between classes, it can be hard to identify \\nwhich features contribute the most to a question being classified a certain \\nway. To estimate this better, we can use feature importance. \\nExtracting Global Feature Importance \\nWe first showed examples of generating feature importance in the context \\nof model evaluation in “Evaluate Feature Importance”. Feature \\nimportances can also be used to prioritize feature-based recommendations. \\nWhen displaying recommendations to users, features that are most \\npredictive for a trained classifier should be prioritized. \\nNext, I’ve displayed the results of a feature importance analysis for a \\nquestion classification model that uses a total of 30 features. Each of the \\ntop features has a much larger importance than the bottom features. \\nGuiding users to act based on these top features first will help them \\nimprove their questions faster according to the model. \\nTop 5 importances: \\n \\nnum_chars: 0.053 \\nnum_questions: 0.051 \\nnum_periods: 0.051 \\nADV: 0.049 \\nADJ: 0.049 \\n \\nBottom 5 importances: \\n \\nX: 0.011 \\nnum_semicolon: 0.0076 \\nnum_exclam: 0.0072 \\nCONJ: 0 \\nSCONJ: 0 \\nCombining feature statistics and feature importance can make \\nrecommendations more actionable and focused. The first approach \\nprovides target values for each feature, while the latter prioritizes a smaller \\nsubset of the most important features to display. These approaches also \\nprovide recommendations quickly, since they do not require running a \\nmodel at inference time, only checking an input against feature statistics \\nfor the most important features.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 191}, page_content='As we saw in “Evaluate Feature Importance”, extracting feature \\nimportances can be more difficult for complex models. If you are using a \\nmodel that does not expose feature importances, you can leverage a black-\\nbox explainer on a large sample of examples to attempt to infer their \\nvalues. \\nFeature importance and feature statistics come with another drawback, \\nwhich is that they do not always provide accurate recommendations. Since \\nrecommendations are based on statistics aggregated over the entire dataset, \\nthey will not be applicable to each individual example. Feature statistics \\nonly provide general recommendations, such as “questions that contain \\nmore adverbs tend to receive higher ratings.” However, there exists \\nexamples of questions with a below average proportion of adverbs that \\nreceive a high score. Such recommendations are not useful for these \\nquestions. \\nIn the next two sections, we will cover methods to provide more granular \\nrecommendations that work at the level of individual examples. \\nUsing a Model’s Score \\nChapter 5 described how classifiers output a score for each example. The \\nexample is then assigned a class based on whether this score is above a \\ncertain threshold. If a model’s score is well calibrated (see “Calibration \\nCurve” for more on calibration), then it can be used as an estimate of the \\nprobability of an input example belonging to the given class. \\nTo display a score instead of a class for a scikit-learn model, use \\nthe predict_proba function and select the class for which you’d like to \\ndisplay a score. \\n# probabilities is an array containing one probability per class \\nprobabilities = clf.predict_proba(features) \\n \\n# Positive probas contains only the score of the positive class \\npositive_probs = clf[:,1] \\nIf it is well calibrated, presenting a score to users allows them to track \\nimprovements in their question as they follow recommendations to modify \\nit, leading to it receiving a higher score. Quick feedback mechanisms like \\na score help users have an increased sense of trust in the recommendations \\nprovided by a model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 192}, page_content='On top of a calibrated score, a trained model can also be used to provide \\nrecommendations to improve a specific example. \\nExtracting Local Feature Importance \\nRecommendations can be generated for an individual example by using a \\nblack-box explainer on top of a trained model. In “Evaluate Feature \\nImportance”, we saw how black-box explainers estimate the importance of \\nfeature values for a specific example by repeatedly applying slight \\nperturbations to input features and observing changes in the model’s \\npredicted score. This makes such explainers a great tool to provide \\nrecommendations. \\nLet’s demonstrate this using the LIME package to generate explanations \\nfor an example. In the following code example, we first instantiate a \\ntabular explainer, and then we choose an example to explain in our test \\ndata. We show the explanations in the generating recommendations \\nnotebook on this book’s GitHub repository, and display them in array \\nformat. \\nfrom lime.lime_tabular import LimeTabularExplainer \\n \\nexplainer = LimeTabularExplainer( \\n    train_df[features].values, \\n    feature_names=features, \\n    class_names=[\"low\", \"high\"], \\n    discretize_continuous=True, \\n) \\n \\nidx = 8 \\nexp = explainer.explain_instance( \\n    test_df[features].iloc[idx, :], \\n    clf.predict_proba, \\n    num_features=10, \\n    labels=(1,), \\n) \\n \\nprint(exp_array) \\nexp.show_in_notebook(show_table=True, show_all=False) \\nexp_array = exp.as_list() \\nRunning the previous code produces the plot shown in Figure 7-2 as well \\nas the array of feature importances shown in the following code. The \\nmodel’s predicted probabilities are displayed on the left side of the figure. \\nIn the middle of the figure, feature values are ranked by their contributions \\nto the prediction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 193}, page_content=\"Figure 7-2. Explanations as recommendations \\nThose values are identical to the ones in the more readable console output \\nbelow. Each row in this output represents a feature value and its impact on \\nthe score of the model. For example, the fact that the \\nfeature num_diff_words had a value lower than 88.00 lowered the score of \\nthe model by about .038. According to this model, increasing the length of \\nthe input question beyond this number would increase its quality. \\n[('num_diff_words <= 88.00', -0.038175093133182826), \\n ('num_questions > 0.57', 0.022220445063244717), \\n ('num_periods <= 0.50', 0.018064270196074716), \\n ('ADJ <= 0.01', -0.01753028452563776), \\n ('408.00 < num_chars <= 655.00', -0.01573650444507041), \\n ('num_commas <= 0.39', -0.015551364531963608), \\n ('0.00 < PROPN <= 0.00', 0.011826217792851488), \\n ('INTJ <= 0.00', 0.011302327527387477), \\n ('CONJ <= 0.00', 0.0), \\n ('SCONJ <= 0.00', 0.0)] \\nFor more usage examples, please refer to the generating recommendations \\nnotebook in the book’s GitHub repository. \\nBlack-box explainers can generate accurate recommendations for an \\nindividual model, but they do come with a drawback. These explainers \\ngenerate estimates by perturbing input features and running a model on \\neach perturbed input, so using them to generate recommendations is \\nslower than the methods discussed. For example, the default number of \\nperturbations that LIME uses to evaluate feature importance is 500. This \\nmakes this method two orders of magnitude slower than methods that need \\nto run a model only once and even slower than ones that do not need to run \\na model at all. On my laptop, running LIME on an example question takes\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 194}, page_content='a little over 2 seconds. Such a delay could prevent us from serving \\nrecommendations to users as they are typing and require them to submit \\nquestions manually instead. \\nJust like many ML models, the recommendation methods we’ve seen here \\npresent a trade-off between accuracy and latency. The right \\nrecommendation for a product depends on its requirements. \\nEvery recommendation method we’ve covered relies on features that were \\ngenerated during model iteration, and some of them leverage the models \\nthat were trained as well. In the next section, we’ll compare different \\nmodel options for the ML Editor and decide which one is the most \\nappropriate for recommendations. \\nComparing Models \\n“Measuring Success” covered important metrics to judge the success of a \\nproduct. “Judge Performance” described methods to evaluate models. \\nSuch methods can also be used to compare successive iterations of models \\nand features to identify top-performing ones. \\nIn this section we will choose a subset of key metrics and use them to \\nevaluate three successive iterations of the ML Editor in terms of model \\nperformance and usefulness of recommendations. \\nThe goal of the ML Editor is to provide recommendations using the \\ntechniques mentioned. To power such recommendations, a model should \\nmatch the following requirements. It should be well calibrated so that its \\npredicted probabilities represent a meaningful estimate of the quality of a \\nquestion. As we covered in “Measuring Success”, it should have high \\nprecision so that the recommendations it makes are accurate. The features \\nit uses should be understandable to a user, since they will serve as the basis \\nfor recommendations. Finally, it should be fast enough to allow us to use a \\nblack-box explainer to provide recommendations. \\nLet’s describe a few successive modeling approaches for the ML Editor \\nand compare their performance. The code for these performance \\ncomparisons can be found in the comparing models notebook in this \\nbook’s GitHub repository.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 195}, page_content='Version 1: The Report Card \\nIn Chapter 3, we built a first version of the editor that was entirely based \\non heuristics. This first version used hard-coded rules meant to encode \\nreadability and displayed results to users in a structured format. Building \\nthis pipeline allowed us to modify our approach and focus ML efforts on \\nproviding clearer recommendations, rather than a set of measurements. \\nSince this initial prototype was built in order to develop an intuition for the \\nproblem we were tackling, we won’t be comparing it to other models here. \\nVersion 2: More Powerful, More Unclear \\nAfter building a heuristic-based version and exploring the Stack Overflow \\ndataset, we settled on an initial modeling approach. The simple model we \\ntrained can be found in the simple model notebook in this book’s GitHub \\nrepository. \\nThis model used a combination of features generated by vectorizing text \\nusing the methods described in “Vectorizing” and manually created \\nfeatures that were surfaced during data exploration. When first exploring \\nthe dataset, I noticed a few patterns: \\n\\uf0b7 Longer questions received higher scores. \\n\\uf0b7 Questions that were specifically about use of the English language \\nreceived lower scores. \\n\\uf0b7 Questions that contained at least one question mark received higher \\nscores. \\nI created features to encode these assumptions by counting the length of \\nthe text, the presence of words such as punctuate and abbreviate, and the \\nfrequency of question marks. \\nIn addition to these features, I vectorized input questions using TF-IDF. \\nUsing a simple vectorization scheme allows me to tie a model’s feature \\nimportances back to individual words, which can allow for word-level \\nrecommendations using the methods described earlier.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 196}, page_content='This first approach showed acceptable aggregate performance, with a \\nprecision of 0.62. Its calibration, however, left much to be desired, as you \\ncan see in Figure 7-3. \\n \\nFigure 7-3. V2 model calibration \\nAfter inspecting this model’s feature importances, I realized the only \\npredictive manually created feature was question length. Other generated \\nfeatures had no predictive power. Exploring the dataset once more \\nrevealed a few more features that seemed predictive: \\n\\uf0b7 A restrained usage of punctuation seemed to be predictive of high \\nscores. \\n\\uf0b7 Questions that were more emotionally charged seemed to receive a \\nlower score. \\n\\uf0b7 Questions that were descriptive and used more adjectives seemed to \\nreceive a higher score.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 197}, page_content='To encode these new hypotheses, I generated a new set of features. I \\ncreated counts for each possible punctuation element. I then created counts \\nthat for each part-of-speech category, such as verb or adjective, measured \\nhow many words in a question belonged to that category. Finally, I added \\na feature to encode the emotional sentiment of a question. For more details \\nabout these features, refer to the second model notebook in this book’s \\nGitHub repository. \\nThis updated version of the model performed slightly better in aggregate, \\nwith a precision of 0.63. Its calibration did not improve upon the previous \\nmodel. Displaying the feature importances for this model revealed that this \\nmodel exclusively relies on the manually crafted features, revealing that \\nthese features have some predictive power. \\nHaving a model rely on such understandable features makes it easier to \\nexplain recommendations to a user than when using vectorized word-level \\nfeatures. For example, the most important word-level features for this \\nmodel are the words are and what. We can guess why these words may be \\ncorrelated with question quality, but recommending to a user that they \\nshould reduce or increase the occurrence of arbitrary words in their \\nquestion does not make for clear recommendations. \\nTo address this limitation of a vectorized representation and recognizing \\nthat the manually crafted features were predictive, I attempted to build a \\nsimpler model that does not use any vectorization features. \\nVersion 3: Understandable Recommendations \\nThe third model contains only the features described earlier (counts of \\npunctuation and parts of speech, question sentiment, and question length). \\nThe model thus only uses 30 features, as opposed to more than 7,000 when \\nusing vectorized representations. See the third model notebook in this \\nbook’s GitHub repository for more details. Removing vectorized features \\nand keeping manual ones allows the ML Editor to only leverage features \\nthat are explainable to a user. However, it may lead to a model performing \\nmore poorly. \\nIn terms of aggregate performance, this model does perform worse than \\nprevious ones with a precision of 0.597. However, it is significantly better \\ncalibrated than previous models. In Figure 7-4, you can see that model 3 is \\nwell calibrated for most probabilities, even ones above .7 that other models'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 198}, page_content='struggle with. The histogram shows that this is due to this model \\npredicting such probabilities more often than other models as well. \\nBecause of the increased range of scores it produces and the improved \\ncalibration of scores, this model is the best choice when it comes to \\ndisplaying a score to guide users. When it comes to making clear \\nrecommendations, this model is also the best choice since it only relies on \\nexplainable features. Finally, because it relies on fewer features than other \\nmodels, it is also the fastest to run. \\n \\nFigure 7-4. Calibration comparison \\nModel 3 is the best choice for the ML Editor and is thus the model we \\nshould deploy for an initial version. In the next section, we will briefly \\ncover how to use this model with the recommendation techniques to \\nprovide editing recommendations to users. \\nGenerating Editing Recommendations'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 199}, page_content='The ML Editor can benefit from any of the four methods we described to \\ngenerate recommendations. In fact, all of these methods are showcased in \\nthe generating recommendations notebook in the book’s GitHub \\nrepository. Because the model we are using is fast, we will illustrate the \\nmost elaborate approach here, using black-box explainers. \\nLet’s start by taking a look at the entire recommendation function that \\ntakes in a question and provides editing advice based on a trained model. \\nHere is what this function looks like: \\ndef get_recommendation_and_prediction_from_text(input_text, num_feats=10): \\n    global clf, explainer \\n    feats = get_features_from_input_text(input_text) \\n    pos_score = clf.predict_proba([feats])[0][1] \\n \\n    exp = explainer.explain_instance( \\n        feats, clf.predict_proba, num_features=num_feats, labels=(1,) \\n    ) \\n    parsed_exps = parse_explanations(exp.as_list()) \\n    recs = get_recommendation_string_from_parsed_exps(parsed_exps) \\n    return recs, pos_score \\nCalling this function on an example input and pretty printing its results \\nproduces recommendations such as the following ones. We can then \\ndisplay these recommendations to users to allow them to iterate on their \\nquestion. \\n>> recos, score = \\nget_recommendation_and_prediction_from_text(example_question\\n) \\n>> print(\"%s score\" % score) \\n0.4 score \\n>> print(*recos, sep=\"\\\\n\") \\nIncrease question length \\nIncrease vocabulary diversity \\nIncrease frequency of question marks \\nNo need to increase frequency of periods \\nDecrease question length \\nDecrease frequency of determiners \\nIncrease frequency of commas \\nNo need to decrease frequency of adverbs \\nIncrease frequency of coordinating conjunctions \\nIncrease frequency of subordinating conjunctions \\nLet’s break this function down. Starting with its signature, the function \\ntakes as arguments an input string representing a question, as well as an \\noptional argument determining how many of the most important features \\nto make recommendations for. It returns recommendations, as well as a \\nscore representing the current quality of the question.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 200}, page_content='Diving into the body of the question, the first line refers to two globally \\ndefined variables, the trained model and an instance of a LIME explainer \\nlike the one we defined in “Extracting Local Feature Importance”. The \\nnext two lines generate features from the input text and pass these features \\nto the classifier for it to predict. Then, exp is defined by using LIME to \\ngenerate explanations. \\nThe last two function calls turn these explanations into human-readable \\nrecommendations. Let’s see how by looking at the definitions of these \\nfunctions, starting with parse_explanations. \\ndef parse_explanations(exp_list): \\n    global FEATURE_DISPLAY_NAMES \\n    parsed_exps = [] \\n    for feat_bound, impact in exp_list: \\n        conditions = feat_bound.split(\" \") \\n \\n        # We ignore doubly bounded conditions , e.g. 1 <= a < 3 because \\n        # they are harder to formulate as a recommendation \\n        if len(conditions) == 3: \\n            feat_name, order, threshold = conditions \\n \\n            simple_order = simplify_order_sign(order) \\n            recommended_mod = \\nget_recommended_modification(simple_order, impact) \\n \\n            parsed_exps.append( \\n                { \\n                    \"feature\": feat_name, \\n                    \"feature_display_name\": \\nFEATURE_DISPLAY_NAMES[feat_name], \\n                    \"order\": simple_order, \\n                    \"threshold\": threshold, \\n                    \"impact\": impact, \\n                    \"recommendation\": recommended_mod, \\n                } \\n            ) \\n    return parsed_exps \\nThis function is long, but it is accomplishing a relatively simple goal. It \\ntakes the array of feature importances returned by LIME and produces a \\nmore structured dictionary that can be used in recommendations. Here is \\nan example of this transformation: \\n# exps is in the format of LIME explanations \\n>> exps = [(\\'num_chars <= 408.00\\', -0.03908691525058592), \\n (\\'DET > 0.03\\', -0.014685507408497802)] \\n \\n>> parse_explanations(exps)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 201}, page_content='[{\\'feature\\': \\'num_chars\\', \\n  \\'feature_display_name\\': \\'question length\\', \\n  \\'order\\': \\'<\\', \\n  \\'threshold\\': \\'408.00\\', \\n  \\'impact\\': -0.03908691525058592, \\n  \\'recommendation\\': \\'Increase\\'}, \\n {\\'feature\\': \\'DET\\', \\n  \\'feature_display_name\\': \\'frequency of determiners\\', \\n  \\'order\\': \\'>\\', \\n  \\'threshold\\': \\'0.03\\', \\n  \\'impact\\': -0.014685507408497802, \\n  \\'recommendation\\': \\'Decrease\\'}] \\nNotice that the function call converted the threshold value displayed by \\nLIME to a recommendation of whether a feature value should be increased \\nor decreased. This is done using the get_recommended_modification function \\ndisplayed here: \\ndef get_recommended_modification(simple_order, impact): \\n    bigger_than_threshold = simple_order == \">\" \\n    has_positive_impact = impact > 0 \\n \\n    if bigger_than_threshold and has_positive_impact: \\n        return \"No need to decrease\" \\n    if not bigger_than_threshold and not has_positive_impact: \\n        return \"Increase\" \\n    if bigger_than_threshold and not has_positive_impact: \\n        return \"Decrease\" \\n    if not bigger_than_threshold and has_positive_impact: \\n        return \"No need to increase\" \\nOnce the explanations are parsed to recommendations, all that is left is to \\ndisplay them in an appropriate format. This is accomplished by the last \\nfunction call in get_recommendation_and_prediction_from_text, which is \\ndisplayed here: \\ndef get_recommendation_string_from_parsed_exps(exp_list): \\n    recommendations = [] \\n    for feature_exp in exp_list: \\n        recommendation = \"%s %s\" % ( \\n            feature_exp[\"recommendation\"], \\n            feature_exp[\"feature_display_name\"], \\n        ) \\n        recommendations.append(recommendation) \\n    return recommendations \\nIf you’d like to experiment with this editor and iterate on it, feel free to \\nrefer to the generating recommendations notebook in this book’s GitHub \\nrepository. At the end of the notebook, I’ve included an example of using \\nthe model recommendations to rephrase a question multiple times and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 202}, page_content='increase its score. I’m reproducing this example here to demonstrate how \\nsuch recommendations can be used to guide users’ editing questions. \\n// First attempt at a question \\n>> get_recommendation_and_prediction_from_text( \\n    \"\"\" \\nI want to learn how models are made \\n\"\"\" \\n) \\n \\n0.39 score \\nIncrease question length \\nIncrease vocabulary diversity \\nIncrease frequency of question marks \\nNo need to increase frequency of periods \\nNo need to decrease frequency of stop words \\n \\n// Following the first three recommendations \\n>> get_recommendation_and_prediction_from_text( \\n    \"\"\" \\nI\\'d like to learn about building machine learning products. \\nAre there any good product focused resources? \\nWould you be able to recommend educational books? \\n\"\"\" \\n) \\n \\n0.48 score \\nIncrease question length \\nIncrease vocabulary diversity \\nIncrease frequency of adverbs \\nNo need to decrease frequency of question marks \\nIncrease frequency of commas \\n \\n// Following the recommendations once more \\n>> get_recommendation_and_prediction_from_text( \\n    \"\"\" \\nI\\'d like to learn more about ML, specifically how to build ML products. \\nWhen I attempt to build such products, I always face the same challenge: \\nhow do you go beyond a model? \\nWhat are the best practices to use a model in a concrete application? \\nAre there any good product focused resources? \\nWould you be able to recommend educational books? \\n\"\"\" \\n) \\n \\n0.53 score \\nVoilà, we now have a pipeline that can take in a question and provide \\nactionable recommendations to users. This pipeline is by no means perfect, \\nbut we now have a working end-to-end ML-powered editor. If you’d like \\nto try your hand at improving it, I encourage you to interact with this \\ncurrent version and identify failure modes to address. Interestingly, while \\nmodels can always be iterated upon, I would argue that the most promising'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 203}, page_content='aspect to improve for this editor would be to generate new features that are \\neven clearer to users. \\nConclusion \\nIn this chapter, we’ve covered different methods to generate suggestions \\nfrom a trained classification model. With these methods in mind, we \\ncompared different modeling approaches for the ML Editor and chose the \\none that would optimize our product goal of helping users ask better \\nquestions. We then built an end-to-end pipeline for the ML Editor and \\nused it to provide recommendations. \\nThe model we settled on still has much room for improvement and can \\nbenefit from more iteration cycles. If you’d like to practice using the \\nconcepts we outlined in Part III, I encourage you to go through these \\ncycles yourself. Overall, every chapter in Part III represents one aspect of \\nthe ML iteration loop. To progress on ML projects, repeatedly go through \\nthe steps outlined in this section until you estimate that a model is ready to \\nbe deployed. \\nIn Part IV, we will cover risks that come with deploying models, how to \\nmitigate them, and methods to monitor and react to model performance \\nvariability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 204}, page_content='Part IV. Deploy and Monitor \\nOnce we have built a model and validated it, we would like to give users \\naccess to it. There are many different methods to surface ML models. The \\nsimplest case involves building a small API, but in order to guarantee that \\nyour models run well for all of your users, you will need more. \\nSee Figure IV-1 for an illustration of some of the systems we will cover in \\nthe next few chapters and that usually accompany a model in production. \\n \\nFigure IV-1. Typical production modeling pipeline \\nProduction ML pipelines need to be able to detect data and model failures \\nand handle them with grace. Ideally, you should also aim to proactively \\npredict any failure and have a strategy to deploy updated models. If any of \\nthis sounds challenging to you, worry not! This is what we will cover \\nin Part IV. \\nChapter 8 \\nBefore deploying, we should always perform a final round of \\nvalidation. The goal is to thoroughly examine potential abuse and \\nnegative uses of our models and do our best to anticipate and build \\nsafeguards around them. \\nChapter 9 \\nWe will cover different methods and platforms to deploy models and \\nhow you would go about choosing one versus the other. \\nChapter 10'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 205}, page_content='In this chapter, we will learn how to build a robust production \\nenvironment that can support models. This includes detecting and \\naddressing model failure, optimizing model performance, and \\nsystematizing retraining. \\nChapter 11 \\nIn this final chapter, we will tackle the crucial step of monitoring. In \\nparticular, we will cover why we need to monitor models, the best \\nways to monitor models, and how we can couple our monitoring \\nsetup to our deployment strategy.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 206}, page_content='Chapter 8. Considerations When \\nDeploying Models \\nThe previous chapters covered model training and generalization \\nperformance. These are necessary steps to deploy a model, but they are not \\nsufficient to guarantee the success of an ML-powered product. \\nDeploying a model requires a deeper dive into failure modes that could \\nimpact users. When building products that learn from data, here are a few \\nquestions you should answer: \\n\\uf0b7 How was the data you are using collected? \\n\\uf0b7 What assumptions is your model making by learning from this \\ndataset? \\n\\uf0b7 Is this dataset representative enough to produce a useful model? \\n\\uf0b7 How could the results of your work be misused? \\n\\uf0b7 What is the intended use and scope of your model? \\nThe field of data ethics aims to answer some of these questions, and the \\nmethods used are constantly evolving. If you’d like to dive deeper, \\nO’Reilly has a comprehensive report on the subject, Ethics and Data \\nScience, by Mike Loukides et al. \\nIn this chapter, we will discuss some concerns around data collection and \\nusage and the challenges involved with making sure models keep working \\nwell for everyone. We will conclude the section with a practical interview \\ncovering tips to translate model predictions to user feedback. \\nLet’s start by looking at data, first covering ownership concerns and then \\nmoving on to bias. \\nData Concerns \\nIn this section, we will start by outlining tips to keep in mind when you \\nstore, use, and generate data. We will start by covering data ownership and \\nthe responsibilities that come with storing data. Then, we will discuss \\ncommon sources of bias in datasets and methods to take this bias into'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 207}, page_content='account when building models. Finally, we’ll cover examples of the \\nnegative consequences of such biases and why they are important to \\nmitigate. \\nData Ownership \\nData ownership refers to the requirements associated with the collection \\nand use of data. Here are a few important aspects to consider with regard \\nto data ownership: \\n\\uf0b7 Data collection: Are you legally authorized to collect and use the \\ndataset you want to train your model on? \\n\\uf0b7 Data usage and permission: Have you clearly explained to your \\nusers why you needed their data and how you wanted to use it, and \\ndid they agree? \\n\\uf0b7 Data storage: How are you storing your data, who has access to it, \\nand when will you delete it? \\nCollecting data from users can help personalize and tailor product \\nexperiences. It also implies both moral and legal responsibilities. While \\nthere has always been a moral obligation to safe-keep data provided by \\nusers, new regulations increasingly make it a legal one. In Europe, for \\nexample, the GDPR regulation now sets strict guidelines regarding data \\ncollection and processing. \\nFor organizations storing large amounts of data, data breaches represent a \\nsignificant liability risk. Such breaches both erode the trust of users in the \\norganization and often lead to legal action. Limiting the amount of data \\ncollected thus limits legal exposure. \\nFor our ML editor, we will start by using publicly available datasets, \\nwhich were collected with the agreement of users and are stored online. If \\nwe wanted to record additional data, such as records of how our service is \\nused in order to improve it, we would have to clearly define a data \\ncollection policy and share it with users. \\nIn addition to data collection and storage, it is important to consider \\nwhether using collected data may lead to poor performance. A dataset is \\nappropriate to use in some cases, but not in others. Let’s explore why.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 208}, page_content='Data Bias \\nDatasets are the result of specific data collection decisions. These \\ndecisions lead to datasets presenting a biased view of the world. ML \\nmodels learn from datasets and thus will reproduce these biases. \\nFor example, let’s say a model is trained on historical data to predict \\nleadership skills by forecasting the likelihood of a person becoming a CEO \\nbased on information including their gender. Historically, according to the \\n“The Data on Women Leaders” fact sheet compiled by the Pew Research \\nCenter, most Fortune 500 CEOs have been male. Using this data to train a \\nmodel will lead to it learning that being male is a valuable predictor of \\nleadership. Being male and being a CEO are correlated in the chosen \\ndataset due to societal reasons, which led to fewer opportunities for \\nwomen to even be considered for such roles. By blindly training a model \\non this data and using it to make predictions, we would simply be \\nreinforcing biases of the past. \\nIt can be tempting to consider data as ground truth. In reality, most \\ndatasets are a collection of approximate measurements that ignore a larger \\ncontext. We should start with the assumption that any dataset is biased and \\nestimate how this bias will affect our model. We can then take steps to \\nimprove a dataset by making it more representative and adjust models to \\nlimit their ability to propagate existing bias. \\nHere are a few examples of common sources of errors and biases in \\ndatasets: \\n\\uf0b7 Measurement errors or corrupted data: Each data point comes with \\nuncertainty due to the method used to produce it. Most models \\nignore such uncertainty and can thus propagate systematic \\nmeasurement errors. \\n\\uf0b7 Representation: Most datasets present an unrepresentative view of a \\npopulation. Many early face recognition datasets mostly contained \\nimages of white men. This led to models performing well for this \\ndemographic but failing on others. \\n\\uf0b7 Access: Some datasets can be harder to find than others. For \\nexample, English text is easier to gather online than other languages. \\nThis ease of access leads to most of the state-of-the-art language \\nmodels being trained exclusively on English data. As a consequence, \\nEnglish speakers will have access to better ML-powered services'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 209}, page_content='than non-English speakers. This disparity often is self-reinforcing, as \\nthe additional volume of users for English products helps make those \\nmodels even better compared to ones for other languages. \\nTest sets are used to evaluate the performance of models. For this reason, \\nyou should take special care to make sure that your test set is as accurate \\nand representative as possible. \\nTEST SETS \\nRepresentation appears in every ML problem. In “Split Your Dataset”, we \\ncovered the value of separating data in different sets to evaluate a model’s \\nperformance. When doing this, you should attempt to build a test set that is \\ninclusive, representative, and realistic. This is because a test set serves as a \\nproxy for performance in production. \\nTo do this, when designing your test set, think of every user that could \\ninteract with your model. To improve the chances that every user has an \\nequally positive experience, try to include examples representative of \\nevery type of user in your test set. \\nDesign your test set to encode product goals. When building a diagnosis \\nmodel, you’ll want to make sure that it performs adequately for all \\ngenders. To evaluate whether that is the case, you’ll need to have them all \\nrepresented in your test set. Gathering a diverse set of point of views can \\nhelp with this endeavor. If you can, before deploying a model, give a \\ndiverse set of users an opportunity to examine it, interact with it, and share \\nfeedback. \\nI want to make a final point when it comes to bias. Models are often \\ntrained on historical data, which represents the state of the world in the \\npast. Because of this, bias most often affects populations that are already \\ndisenfranchised. Working to eliminate bias is thus an endeavor that can \\nhelp make systems fairer for the people who need it most. \\nSystemic Bias \\nSystemic bias refers to institutional and structural policies that have led to \\nsome populations being unfairly discriminated against. Because of this \\ndiscrimination, such populations are often over- or underrepresented in \\nhistorical datasets. For example, if societal factors have contributed to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 210}, page_content='some populations being historically overrepresented in criminal arrest \\ndatabases, an ML model trained from that data will encode this bias and \\ncarry it forward to modern-day predictions. \\nThis can have disastrous consequences and lead to the marginalization of \\nsubsets of the population. For a concrete example, see J. Angwin et \\nal.’s “Machine Bias” ProPublica report, on ML bias for crime prediction. \\nRemoving or limiting bias in a dataset is challenging. When trying to \\nprevent a model from being biased against certain features such as \\nethnicity or gender, some have tried to remove the attribute in question \\nfrom the list of features that a model uses to make predictions. \\nIn practice, simply removing a feature does not prevent a model from \\nbeing biased against it, because most datasets contain many other features \\nthat are strongly correlated with it. For example, ZIP code and income are \\nhighly correlated with ethnicity in the United States. If you remove only \\none feature, a model may be just as biased, albeit in ways that are harder to \\ndetect. \\nInstead, you should be explicit about which fairness constraints you are \\ntrying to enforce. For example, you could follow the approach outlined in \\nthe paper by M. B. Zafar et al., “Fairness Constraints: Mechanisms for Fair \\nClassification”, where the fairness of a model is measured using the p% \\nrule. The p% rule is defined as “the ratio between the percentage of \\nsubjects having a certain sensitive attribute value receiving a positive \\noutcome and the percentage of subjects not having that value receiving the \\nsame outcome should be no less than p:100.” Using such a rule allows us \\nto quantify bias and thus address it better, but it requires keeping track of \\nthe feature we’d like a model not to be biased against. \\nIn addition to evaluating risk, biases, and errors in a dataset, ML requires \\nevaluating models. \\nModeling Concerns \\nHow can we minimize the risk of a model introducing undesirable bias? \\nThere are multiple ways that models can impact users negatively. First, \\nwe’ll tackle runaway feedback loops, and then we’ll explore the risks of a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 211}, page_content='model discreetly failing on a small segment of the population. We will \\nthen discuss the importance of contextualizing ML predictions \\nappropriately for users and end this section by covering the risk of having \\nnefarious actors abusing models. \\nFeedback Loops \\nIn most ML-powered systems, having a user follow a model’s \\nrecommendation will make it more likely for future models to make the \\nsame recommendation. When left unchecked, this phenomenon can lead to \\nmodels entering a self-reinforcing feedback loop. \\nFor example, if we train a model to recommend videos to users and our \\nfirst version of the model is slightly more likely to recommend videos of \\ncats than dogs, then users will watch more cat videos than dog videos on \\naverage. If we train a second version of the model using a dataset of \\nhistorical recommendations and clicks, we will incorporate the first \\nmodel’s bias into our dataset, and our second model will favor cats much \\nmore heavily. \\nSince content recommendation models often get updated multiple times a \\nday, it would not take long before our most recent version of the model \\nrecommends exclusively cat videos. You can see an example of this \\nin Figure 8-1. Due to an initial popularity of a cat video, the model \\nprogressively learns to recommend more cat videos, until it reaches the \\nstate on the right, only ever recommending cat videos.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 212}, page_content='Figure 8-1. Example of a feedback loop \\nFilling the internet up with cat videos might not seem like a tragedy, but \\nyou can imagine how these mechanisms can rapidly reinforce negative \\nbiases and recommend inappropriate or dangerous content to unsuspecting \\nusers. In fact, models that attempt to maximize the probability of a user \\nclicking will learn to recommend clickbait content, content that is very \\ntempting to click but does not provide any value to the user. \\nFeedback loops also tend to introduce bias to favor a minority of very \\nactive users. If a video platform uses the number of clicks on each video to \\ntrain its recommendation algorithm, it risks having its recommendation \\noverfit to its most active users who represent the vast majority of clicks. \\nEvery other user of the platform would then be exposed to the same videos \\nregardless of their individual preferences. \\nTo limit negative effects of feedback loops, choose a label that is less \\nprone to creating such a loop. Clicks only measure whether a user opens a \\nvideo, not whether they enjoy it. Using clicks as an optimization goal leads \\nto recommending more eye-catching content, without any concerns for its \\nrelevance. Replacing the target metric with watch time, which is more \\ncorrelated with the user satisfaction, would help alleviate such a feedback \\nloop. \\nEven then, recommendation algorithms that optimize for engagement of \\nany sort always carry the risk of degenerating into a feedback loop since \\ntheir only objective is to maximize a practically limitless metric. For \\nexample, even if an algorithm optimizes for watch time to encourage more \\nengaging content, the state of the world that would maximize this metric is \\none where every user spent their entire day watching videos. Using such \\nengagement metrics may help increase usage, but this raises the question \\nof whether that is always a worthwhile goal to optimize for. \\nIn addition to the risk of creating feedback loops, models can also exhibit \\npoorer performance than expected in production despite receiving \\nconvincing scores on offline validation metrics. \\nInclusive Model Performance \\nIn “Evaluate Your Model: Look Beyond Accuracy”, we covered a variety \\nof evaluation metrics that attempt to judge performance on different'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 213}, page_content='subsets of a dataset. This type of analysis is helpful to ensure that a model \\nperforms equally well for different types of users. \\nThis is especially important when training new versions of existing models \\nand deciding whether to deploy them. If you only compare aggregate \\nperformance, you could fail to notice a significant degradation of \\nperformance on a segment of the data. \\nFailure to notice such degradation of performance has led to catastrophic \\nproduct failures. In 2015, an automated photo-tagging system categorized \\nphotos of African American users as gorillas (see this 2015 BBC article). \\nThis is an appalling failure and a consequence of not validating a model on \\na representative set of inputs. \\nThis sort of issue can arise when updating an existing model. Say you are \\nupdating a facial recognition model, for example. The previous model had \\nan accuracy of 90%, and the new one has an accuracy of 92%. Before \\ndeploying this new model, you should benchmark its performance on a \\nfew different subsets of users. You may find that while the performance \\nhas slightly improved in aggregate, the new model’s accuracy is \\nperforming very poorly for photos of women over the age of 40, so you \\nshould abstain from deploying it. Instead, you should modify the training \\ndata to add more representative examples and retrain a model that can \\nperform well for every category. \\nOmitting such benchmarks can lead to models not working for a \\nsignificant proportion of their intended audience. Most models will never \\nwork for every possible input, but it is important to validate that they work \\nfor all expected ones. \\nConsidering Context \\nUsers will not always be aware that a given piece of information \\noriginated as a prediction from a ML model. Whenever possible, you \\nshould share the context of a prediction with a user, so they can make an \\ninformed decision as to how to leverage it. To do so, you can start by \\ndescribing to them how the model was trained. \\nThere is no industry-standard “model disclaimer” format yet, but active \\nresearch in this area has shown promising formats, such as model cards \\n(see this article by M. Mitchell et al., “Model Cards for Model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 214}, page_content='Reporting”), a documentation system for transparent model reporting. In \\nthe proposed approach, a model is accompanied by metadata about how it \\nwas trained, which data it was tested on, what its intended use is, and \\nmore. \\nIn our case study, the ML Editor provides feedback based on a specific \\ndataset of questions. If we were to deploy it as a product, we would \\ninclude a disclaimer about the types of inputs the model is expected to \\nperform well on. Such a disclaimer could be as simple as “This product \\nattempts to recommend better ways to phrase a question. It was trained on \\nquestions from the writing Stack Exchange and may thus reflect the \\nparticular preferences of that community.” \\nKeeping well-meaning users informed is important. Now, let’s look at \\npotential challenges that can come from less-friendly users. \\nAdversaries \\nSome ML projects need to consider the risk of having models be defeated \\nby adversaries. Fraudsters may attempt to fool a model that is tasked with \\ndetecting suspicious credit card transactions. Alternatively, adversaries \\nmay want to probe a trained model to glean information they should not be \\nallowed to access about the underlying training data, such as sensitive user \\ninformation. \\nDEFEATING A MODEL \\nMany ML models are deployed to protect accounts and transactions from \\nfraudsters. In turn, fraudsters attempt to defeat these models by fooling \\nthem into believing they are legitimate users. \\nIf you are trying to prevent fraudulent logins to an online platform, for \\nexample, you may want to consider sets of features that would include the \\nuser’s country of origin (many large-scale attacks use multiple servers \\nfrom the same region). If you train a model on such features, you risk \\nintroducing bias against nonfraudulent users in countries where fraudsters \\nlive. In addition, relying only on such a feature will make it easy for \\nmalicious actors to fool your systems by faking their location. \\nTo defend against adversaries, it is important to regularly update models. \\nAs attackers learn existing patterns of defense and adapt their behavior to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 215}, page_content='defeat them, update your models so that they can quickly classify this new \\nbehavior as fraudulent. This requires monitoring systems so that we can \\ndetect changes of patterns in activity. We will cover this in more detail \\nin Chapter 11. In many cases, defending against attackers requires \\ngenerating new features to better detect their behavior. Feel free to refer \\nto “Let Data Inform Features and Models” for a refresher on \\nfeature generation. \\nThe most common type of attack on models aims to fool them into a \\nwrong prediction, but other types of attacks exist. Some attacks aim to use \\na trained model to learn about the data it was trained on. \\nEXPLOITING A MODEL \\nMore than simply fooling a model, attackers could use it to learn private \\ninformation. A model reflects the data it was trained on, so one could use \\nits predictions to infer patterns in the original dataset. To illustrate this \\nidea, consider the example of a classification model trained on a dataset \\ncontaining two examples. Each example is of a different class, and both \\nexamples differ only by a single feature value. If you gave an attacker \\naccess to a model trained on this dataset and allowed them to observe its \\npredictions to arbitrary inputs, they could eventually infer that this feature \\nis the only predictive one in the dataset. Similarly, an attacker could infer \\nthe distribution of features within the training data. These distributions \\noften receive sensitive or private information. \\nIn the fraudulent login detection example, let’s imagine that ZIP code is \\none of the required fields at login. An attacker could attempt to log in with \\nmany different accounts, testing different ZIP codes to see which values \\nlead to a successful login. Doing so would allow them to estimate the \\ndistribution of ZIP codes in the training set and thus the geographical \\ndistribution of this website’s customers. \\nThe simplest way to limit the efficiency of such attacks is to limit the \\nnumber of requests a given user can make, thereby limiting their ability to \\nexplore feature values. This is not a silver bullet, as sophisticated attackers \\nmay be able to create multiple accounts to circumvent such a limit. \\nThe adversaries described in this section are not the only nefarious users \\nyou should be concerned with. If you choose to share your work with the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 216}, page_content='wider community, you should also ask yourself whether it could be used \\nfor dangerous applications. \\nAbuse Concerns and Dual-Use \\nDual-use describes technologies that are developed for one purpose but \\ncan be used for others. Because of ML’s ability to perform well on \\ndatasets of similar types (see Figure 2-3), ML models often present a dual-\\nuse concern. \\nIf you build a model that allows people to change their voice to sound like \\ntheir friends’, could it be misused to impersonate others without their \\nconsent? If you do choose to build it, how could you include the proper \\nguidance and resources to make sure that users understand the proper use \\nof your model? \\nSimilarly, any model that can accurately classify faces has dual-use \\nimplications for surveillance. While such a model may originally be built \\nto enable a smart doorbell, it could then be used to automatically track \\nindividuals across a city-wide network of cameras. Models are built using \\na given dataset but can present risks when retrained on other similar \\ndatasets. \\nThere are currently no clear best practices on considering dual-use. If you \\nbelieve your work could be exploited for unethical uses, I encourage you \\nto consider making it harder to reproduce for that purpose or to engage in \\nthoughtful discussion with the community. Recently, OpenAI made the \\ndecision to not release its most powerful language model because of \\nconcerns that it may make spreading disinformation online much easier \\n(see OpenAI’s announcement post, “Better Language Models and Their \\nImplications”). While this was a relatively novel decision, I wouldn’t be \\nsurprised if such concerns are raised more often going forward. \\nTo conclude this chapter, in the next section I am sharing a discussion with \\nChris Harland, currently director of engineering at Textio, who has an \\nabundance of experience deploying models to users and presenting the \\nresults with enough context to make them useful. \\nChris Harland: Shipping Experiments'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 217}, page_content='Chris has a Ph.D. in physics and worked on a variety of ML tasks \\nincluding computer vision to extract structured information from receipts \\nfor expensing software. He worked on the search team at Microsoft, where \\nhe realized the value of ML engineering. Chris then joined Textio, a \\ncompany that builds augmented writing products to help users write more \\ncompelling job descriptions. Chris and I sat down to discuss his experience \\nshipping ML-powered products and how he approaches validating results \\nbeyond accuracy metrics. \\nQ: Textio uses ML to directly guide users. How is that different from other \\nML tasks? \\nA: When you only focus on predictions, such as when to buy gold or who \\nto follow on Twitter, you can tolerate some amount of variance. When you \\ndo guidance for writing, that is not the case, because your \\nrecommendations carry a lot of subtext. \\nIf you tell me to write 200 more words, your model should be consistent \\nand allow the user to follow its advice. Once the user writes 150 words, \\nthe model can’t change its mind and recommend to lower the word count. \\nGuidance also requires clarity: “remove stop words by 50%” is a \\nconfusing instruction, but “reduce the length of these 3 sentences” may \\nhelp users in a more actionable way. A challenge then becomes \\nmaintaining performance while using features that are more human \\nunderstandable. \\nEssentially, ML writing assistants guide the user through our feature space \\nfrom an initial point to a better one according to our model. Sometimes, \\nthis can involve passing through points that are worse, which can be a \\nfrustrating user experience. The product needs to be built with these \\nconstraints in mind. \\nQ: What are good ways to perform this guidance? \\nA: For guidance, precision is much more interesting than recall. If you \\nthink of giving advice to a person, recall would be the ability to give \\nadvice in all potential relevant domains and some irrelevant ones (of which \\nthere are many), while precision would be giving advice in a few \\npromising domains ignoring potential other ones.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 218}, page_content='When giving advice, the cost of being wrong is very high, so precision is \\nthe most useful. Users will also learn from recommendations that your \\nmodel has previously given and apply them unprompted to future inputs, \\nwhich makes the precision of these recommendations even more \\nimportant. \\nIn addition, since we surface different factors, we measure whether users \\nactually take advantage of them. If not, we should understand why not. A \\npractical example is our “active to passive ratio” feature, which was \\nunderutilized. We realized that this was because of the recommendation \\nnot being actionable enough, so we improved it by highlighting the words \\nthemselves that we recommend changing. \\nQ: How do you find new ways to guide your users or new features? \\nA: Both top-down and bottom-up approaches are valuable. \\nTop-down hypothesis investigation is domain knowledge-driven and \\nbasically consists of feature matching from prior experience. This can \\ncome from product or sales teams, for example. A top-down hypothesis \\nmay look like “we believe that there is something about the mystery aspect \\nof recruiting emails that helps drive engagement.” The challenge in top-\\ndown is usually to find a practical way to extract that feature. Only then \\ncan we validate whether the feature is predictive. \\nBottom-up aims to introspect a classification pipeline to understand what it \\nfinds predictive. If we have a general representation of text such as word \\nvectors, tokens, and parts of speech annotations that we then feed to \\nensembles of models to classify as good or bad text, which features are \\nmost predictive of our classification? Domain experts will often be the best \\nequipped to identify these patterns from a model’s predictions. The \\nchallenge is then to find a way to make these features human \\nunderstandable. \\nQ: How do you decide when a model is good enough? \\nA: You shouldn’t underestimate how far a small text dataset of relevant \\nlanguage gets you. It turns out that using only a thousand documents in \\nyour domain is enough for many use cases. Having an ability to label that \\nsmall set of data is worthwhile. You can then start by testing your model \\non out-of-sample data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 219}, page_content='You should make it easy to run experiments. An overwhelming majority \\nof the ideas you have about changing your product end up having a net \\neffect that is null, which should allow you to be a little less worried about \\nnew features. \\nFinally, building a bad model is fine and is what you should start with. \\nFixing the bad models will make your product much more robust to \\nproblems and help it evolve faster. \\nQ: How do you see how a model is doing once it is in production? \\nA: When in production, expose your model’s predictions to users clearly \\nand let them override it. Log feature values, predictions, and overwrites so \\nthat you can monitor them and analyze them later. If your model produces \\na score, finding ways to compare this score to usage of your \\nrecommendations can be an additional signal. If you are predicting \\nwhether an email will be opened, for example, it can be extremely \\nvaluable to get access to the ground truth data from your users so you can \\nimprove your model. \\nThe ultimate success metric is customer success, which is the most \\ndelayed and is influenced by many other factors. \\nConclusion \\nWe started by covering concerns with using and storing data. Then, we \\ndove into causes of bias in datasets and tips to identify and reduce them. \\nNext, we looked at the challenges that models face in the wild and how to \\nreduce the risks associated with exposing them to users. Finally, we looked \\nat how to architect systems so that they are designed to be resilient to \\nerrors. \\nThese are complex issues, and the field of ML still has much to do to \\ntackle all potential forms of abuse. The first step is for all practitioners to \\nbe aware of these concerns and to be mindful of them in their own \\nprojects. \\nWe are now ready to deploy models. To start, we will explore the trade-\\noffs between different deployment options in Chapter 9. Then, we will'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 220}, page_content='cover methods to mitigate some of the risks associated with deploying \\nmodels in Chapter 10.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 221}, page_content='Chapter 9. Choose Your Deployment \\nOption \\nThe previous chapters covered the process of going from a product idea to \\nan ML implementation, as well as methods to iterate on this application \\nuntil you are ready to deploy it. \\nThis chapter covers different deployment options and the trade-offs \\nbetween each of them. Different deployment approaches are suited to \\ndifferent sets of requirements. When considering which one to choose, \\nyou’ll want to think of multiple factors such as latency, hardware and \\nnetwork requirements, as well as privacy, cost, and complexity concerns. \\nThe goal of deploying a model is to allow users to interact with it. We will \\ncover common approaches to achieve this goal, as well as tips to decide \\nbetween approaches when deploying models. \\nWe will start with the simplest way to get started when deploying models \\nand spinning up a web server to serve predictions. \\nServer-Side Deployment \\nServer-side deployment consists of setting up a web server that can accept \\nrequests from clients, run them through an inference pipeline, and return \\nthe results. This solution fits within a web development paradigm, as it \\ntreats models as another endpoint in an application. Users have requests \\nthat they send to this endpoint, and they expect results. \\nThere are two common workloads for server-side models, streaming and \\nbatch. Streaming workflows accept requests as they come and process \\nthem immediately. Batch workflows are run less frequently and process a \\nlarge number of requests all at once. Let’s start by looking at streaming \\nworkflows. \\nStreaming Application or API \\nThe streaming approach considers a model as an endpoint that users can \\nsend requests to. In this context, users can be end users of an application or \\nan internal service that relies on predictions from a model. For example, a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 222}, page_content='model that predicts website traffic could be used by an internal service that \\nis charged with adjusting the number of servers to match the predicted \\namount of users. \\nIn a streaming application, the code path for a request goes through a set of \\nsteps that are the same as the inference pipeline we covered in “Start with \\na Simple Pipeline”. As a reminder, these steps are: \\n1. Validate the request. Verify values of parameters passed, and \\noptionally check whether the user has the correct permissions for \\nthis model to be run. \\n2. Gather additional data. Query other data sources for any additional \\nneeded data we may need, such as information related to a user, for \\nexample. \\n3. Preprocess data. \\n4. Run the model. \\n5. Postprocess the results. Verify that the results are within acceptable \\nbounds. Add context to make it understandable to the user, such as \\nexplaining the confidence of a model. \\n6. Return a result. \\nYou can see this sequence of steps illustrated in Figure 9-1. \\n \\nFigure 9-1. Streaming API workflow \\nThe endpoint approach is quick to implement but requires infrastructure to \\nscale linearly with the current number of users, since each user leads to a \\nseparate inference call. If traffic increases beyond the capacity of a server \\nto handle requests, they will start to be delayed or even fail. Adapting such'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 223}, page_content='a pipeline to traffic patterns thus requires being able to easily launch and \\nshut down new servers, which will require some level of automation. \\nFor a simple demo such as the ML Editor, however, which is only meant \\nto be visited by a few users at a time, a streaming approach is usually a \\ngood choice. To deploy the ML Editor, we use a lightweight Python web \\napplication such as Flask, which makes it easy to set up an API to serve a \\nmodel with a few lines of code. \\nYou can find the deployment code for the prototype in the book’s GitHub \\nrepository, but I’ll give a high-level overview here. The Flask application \\nconsists of two parts, an API that takes in requests and sends them to a \\nmodel for processing using Flask, and a simple website built in HTML for \\nusers to input their text and to display results. Defining such an API does \\nnot require much code. Here, you can see two functions that handle the \\nbulk of the work to serve the v3 of the ML Editor: \\nfrom flask import Flask, render_template, request \\n \\n@app.route(\"/v3\", methods=[\"POST\", \"GET\"]) \\ndef v3(): \\n    return handle_text_request(request, \"v3.html\") \\n \\ndef handle_text_request(request, template_name): \\n    if request.method == \"POST\": \\n        question = request.form.get(\"question\") \\n        suggestions = get_recommendations_from_input(question) \\n        payload = {\"input\": question, \"suggestions\": suggestions} \\n        return render_template(\"results.html\", ml_result=payload) \\n    else: \\n        return render_template(template_name) \\nThe v3 function defines a route, which allows it to determine the HTML to \\ndisplay when a user accesses the /v3 page. It uses the \\nfunction handle_text_request to decide what to display. When a user first \\naccesses the page, the request type is GET and so the function displays an \\nHTML template. A screenshot of this HTML page is shown in Figure 9-2. \\nIf a user clicks the “Get recommendation” button, the request type is POST, \\nso handle_text_request retrieves the question data, passes it to a model, and \\nreturns the model output.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 224}, page_content='Figure 9-2. Simple webpage to use a model \\nA streaming application is required when strong latency constraints exist. \\nIf the information a model needs will be available only at prediction time \\nand the model’s prediction is required immediately, you will need a \\nstreaming approach. For example, a model that predicts the price for a \\nspecific trip in a ride hailing app requires information about the user’s \\nlocation and the current availability of drivers to make a prediction, which \\nis available only at request time. Such a model also needs to output a \\nprediction immediately, since it must be displayed to the user for them to \\ndecide whether to use the service. \\nIn some other cases, the information required to compute predictions is \\navailable ahead of time. In those cases, it can be easier to process a large \\nnumber of requests at once rather than processing them as they arrive. This \\nis called batch prediction, and we will cover it next. \\nBatch Predictions \\nThe batch approach considers the inference pipeline as a job that can be \\nrun on multiple examples at once. A batch job runs a model on many \\nexamples and stores predictions so they can be used when needed. Batch \\njobs are appropriate when you have access to the features needed for a \\nmodel before the model’s prediction is required. \\nFor example, let’s say you’d like to build a model to provide each \\nsalesperson on your team with a list of companies that are the most \\nvaluable prospects to contact. This is a common ML problem called lead \\nscoring. To train such a model, you could use features such as historical \\nemail conversations and market trends. Such features are available before \\na salesperson is deciding which prospect to contact, which is when a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 225}, page_content='prediction is required. This means you could compute a list of prospects in \\na nightly batch job and have the results ready to be displayed by the \\nmorning, when they will be needed. \\nSimilarly, an app that uses ML to prioritize and rank the most important \\nmessage notifications to read in the morning does not have strong latency \\nrequirements. An appropriate workflow for this app would be to process \\nall unread emails in a batch in the morning and save the prioritized list for \\nwhen the user needs it. \\nA batch approach requires as many inference runs as a streaming \\napproach, but it can be more resource efficient. Because predictions are \\ndone at a predetermined time and the number of predictions is known at \\nthe start of a batch, it is easier to allocate and parallelize resources. In \\naddition, a batch approach can be faster at inference time since results \\nhave been precomputed and only need to be retrieved. This provides \\nsimilar gains to caching. \\nFigure 9-3 shows the two sides of this workflow. At batch time, we \\ncompute predictions for all the data points and store the results we \\nproduce. At inference time, we retrieve the precomputed results. \\n \\nFigure 9-3. Example of batch workflow \\nIt is also possible to use a hybrid approach. Precompute in as many cases \\nas possible and at inference time either retrieve precomputed results or \\ncompute them on the spot if they are not available or are outdated. Such an \\napproach produces results as rapidly as possible, since anything that can be \\ncomputed ahead of time will be. It comes with the cost of having to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 226}, page_content='maintain both a batch pipeline and a streaming pipeline, which \\nsignificantly increases the complexity of a system. \\nWe’ve covered two common ways of deploying applications on a server, \\nstreaming and batch. Both of these approaches require hosting servers to \\nrun inference for customers, which can quickly become costly if a product \\nbecomes popular. In addition, such servers represent a central failure point \\nfor your application. If the demand for predictions increases suddenly, \\nyour servers may not be able to accommodate all of the requests. \\nAlternatively, you could process requests directly on the devices of the \\nclients making them. Having models run on users’ devices reduces \\ninference costs and allows you to maintain a constant level of service \\nregardless of the popularity of your application, since clients are providing \\nthe necessary computing resources. This is called client-side deployment. \\nClient-Side Deployment \\nThe goal of deploying models on the client side is to run all computations \\non the client, eliminating the need for a server to run models. Computers, \\ntablets, modern smartphones, and some connected devices such as smart \\nspeakers or doorbells have enough computing power to run models \\nthemselves. \\nThis section only covers trained models being deployed on device for \\ninference, not training a model on the device. Models are still trained in \\nthe same manner and are then sent to the device for inference. The model \\ncan make its way to the device by being included in an app, or it can be \\nloaded from a web browser. See Figure 9-4 for an example workflow to \\npackage a model in an application.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 227}, page_content='Figure 9-4. A model running inference on device (we can still train on a server) \\nPocket-sized devices offer more limited compute power than powerful \\nservers, so this approach limits the complexity of the models that can be \\nused, but having models run on device can offer multiple advantages. \\nFirst, this reduces the need to build infrastructure that can run inference for \\nevery single user. In addition, running models on devices reduces the \\nquantity of data that needs to be transferred between the device and the \\nserver. This reduces network latency and can even allow an application to \\nrun with no access to the network. \\nFinally, if the data required for inference contains sensitive information, \\nhaving a model run on device removes the need for this data to be \\ntransferred to a remote server. Not having sensitive data on servers lowers \\nthe risk of an unauthorized third party accessing this data (see “Data \\nConcerns” for why this can be a serious risk). \\nFigure 9-5 compares the workflow for getting a prediction to a user for \\nserver-side models and client-side models. At the top, you can see that the \\nlongest delay for a server-side workflow is often the time it takes to \\ntransfer data to the server. On the bottom, you can see that while client-\\nside models incur next to no latency, they often process examples slower \\nthan servers because of hardware constraints.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 228}, page_content='Figure 9-5. Running on a server, or locally \\nJust like for server-side deployment, there are multiple ways to deploy \\napplications client side. In the following sections, we will cover two \\nmethods, deploying models natively and running them through the \\nbrowser. These approaches are relevant for smartphones and tablets, which \\nhave access to an app store and web browser, but not for other connected \\ndevices such as microcontrollers, which we will not cover here. \\nOn Device \\nProcessors in laptops and phones are not usually optimized to run ML \\nmodels and so will execute an inference pipeline slower. For a client-side \\nmodel to run quickly and without draining too much power, it should be as \\nsmall as possible. \\nReducing model size can be done by using a simpler model, reducing a \\nmodel’s number of parameters or the precision of calculations. In neural \\nnetworks, for example, weights are often pruned (removing those with \\nvalues close to zero) and quantized (lowering the precision of weights). \\nYou may also want to reduce the number of features your model uses to \\nfurther increase efficiency. In recent years, libraries such as Tensorflow \\nLite have started providing useful tools to reduce the size of models and \\nhelp make them more easily deployable on mobile devices. \\nBecause of these requirements, most models will suffer a slight \\nperformance loss by being ported on device. Products that cannot tolerate'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 229}, page_content='model performance degradation such as ones that rely on cutting-edge \\nmodels that are too complex to be run on a device such as a smartphone \\nshould be deployed on a server. In general, if the time it would take to run \\ninference on device is larger than the time it would take to transmit data to \\nthe server to be processed, you should consider running your model in the \\ncloud. \\nFor other applications such as predictive keyboards on smartphones that \\noffer suggestions to help type faster, the value of having a local model that \\ndoes not need access to the internet outweighs the accuracy loss. Similarly, \\na smartphone application built to help hikers identify plants by taking a \\nphoto of them should work offline so that it can be used on a hike. Such an \\napplication would require a model to be deployed on device, even if it \\nmeans sacrificing prediction accuracy. \\nA translation app is another example of an ML-powered product that \\nbenefits from functioning locally. Such an app is likely to be used abroad \\nwhere users may not have network access. Having a translation model that \\ncan run locally becomes a requirement, even if it isn’t as precise as a more \\ncomplex one that could run only on a server. \\nIn addition to network concerns, running models in the cloud adds a \\nprivacy risk. Sending user data to the cloud and storing it even temporarily \\nincreases the odds of an attacker getting access to it. Consider an \\napplication as benign as superimposing filters on photos. Many users may \\nnot feel comfortable with their photos being transmitted to a server for \\nprocessing and stored indefinitely. Being able to guarantee to users that \\ntheir photos never leave the device is an important differentiator in an \\nincreasingly privacy conscious world. As we saw in “Data Concerns”, the \\nbest way to avoid putting sensitive data at risk is making sure it never \\nleaves the device or gets stored on your servers. \\nOn the other hand, quantizing pruning and simplifying a model is a time-\\nconsuming process. On-device deployment is only worthwhile if the \\nlatency, infrastructure, and privacy benefits are valuable enough to invest \\nthe engineering effort. For the ML Editor, we will limit ourselves to a \\nweb-based streaming API. \\nFinally, optimizing models specifically so they run on a certain type of \\ndevice can be time-consuming, as the optimization process may differ \\nbetween devices. More options exist to run models locally, including ones'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 230}, page_content='that leverage commonalities between devices to reduce required \\nengineering work. An exciting area in this domain is ML in the browser. \\nBrowser Side \\nMost smart devices have access to a browser. These browsers have often \\nbeen optimized to support fast graphical calculations. This has led to rising \\ninterest in libraries that use browsers to have the client perform ML tasks. \\nThe most popular of these frameworks is Tensorflow.js, which makes it \\npossible to train and run inference in JavaScript in the browser for most \\ndifferentiable models, even ones that were trained in different languages \\nsuch as Python. \\nThis allows users to interact with models through the browser without \\nneeding to install any additional applications. In addition, since models run \\nin the browser using JavaScript, computations are done on the user’s \\ndevice. Your infrastructure only needs to serve the web page that includes \\nthe model weights. Finally, Tensorflow.js supports WebGL, which allows \\nit to leverage GPUs on the clients’ device if they are available to make \\ncomputations faster. \\nUsing a JavaScript framework makes it easier to deploy a model on the \\nclient side without requiring as much device-specific work as the previous \\napproach. This approach does come with the drawback of increasing \\nbandwidth costs, since the model will need to be downloaded by clients \\neach time they open the page as opposed to once when they install the \\napplication. \\nAs long as the models you use are a few megabytes or smaller and can be \\ndownloaded quickly, using JavaScript to run them on the client can be a \\nuseful way to lower server costs. If server costs ever became an issue for \\nthe ML Editor, deploying the model using a framework like Tensorflow.js \\nwould be one of the first methods I would recommend exploring. \\nSo far, we’ve considered clients purely to deploy models that have already \\nbeen trained, but we could also decide to train models on clients. In the \\nnext part, we will explore when this could be useful. \\nFederated Learning: A Hybrid Approach'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 231}, page_content='We have mostly covered different ways to deploy models that we have \\nalready trained (ideally by following the guidelines in the previous \\nchapters) and that we are now choosing how to deploy. We have looked at \\ndifferent solutions for getting a unique model in front of all our users, but \\nwhat if we wanted each user to have a different model? \\nFigure 9-6 shows the difference between a system at the top that has a \\ncommon trained model for all users and one at the bottom where each user \\nhas a slightly different version of the model. \\n \\nFigure 9-6. One big model or many individual ones \\nFor many applications such as content recommendation, giving writing \\nadvice, or healthcare, a model’s most important source of information is \\nthe data it has about the user. We can leverage this fact by generating user-\\nspecific features for a model, or we can decide that each user should have \\ntheir own model. These models can all share the same architecture, but \\neach user’s model will have different parameter values that reflect their \\nindividual data. \\nThis idea is at the core of federated learning, an area of deep learning that \\nhas been getting increasing attention recently with projects such \\nas OpenMined. In federated learning, each client has their own model. \\nEach model learns from their user’s data and sends aggregated (and \\npotentially anonymized) updates to the server. The server leverages all \\nupdates to improve its model and distills this new model back to individual \\nclients. \\nEach user receives a model that is personalized to their needs, while still \\nbenefiting from aggregate information about other users. Federated \\nlearning improves privacy for users because their data is never transferred'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 232}, page_content='to the server, which only receives aggregated model updates. This stands \\nin contrast to training a model the traditional way by collecting data about \\neach user and storing all of it on a server. \\nFederated learning is an exciting direction for ML, but it does add an \\nadditional layer of complexity. Making sure that each individual model is \\nperforming well and that the data transmitted back to the server is properly \\nanonymized is more complicated than training a single model. \\nFederated learning is already used in practical applications by teams that \\nhave the resources to deploy it. For example, as described in this article by \\nA. Hard et al., “Federated Learning for Mobile Keyboard Prediction”, \\nGoogle’s GBoard uses federated learning to provide next-word predictions \\nfor smartphone users. Because of the diversity of writing styles among \\nusers, building a unique model that performs well for all users proved \\nchallenging. Training models at the user level allows GBoard to learn \\nabout user-specific patterns and to provide better predictions. \\nWe’ve covered multiple ways to deploy models on servers, on devices, or \\neven on both. You should consider each approach and its trade-offs based \\non the requirements of your application. As with other chapters in this \\nbook, I encourage you to start with a simple approach and move to a more \\ncomplex one only once you’ve validated that it is necessary. \\nConclusion \\nThere are multiple ways to serve an ML-powered application. You can set \\nup a streaming API to allow a model to process examples as they arrive. \\nYou can use a batch workflow that will process multiple data points at \\nonce on a regular schedule. Alternatively, you can choose to deploy your \\nmodels on the client side by either packaging them in an application or \\nserving them through a web browser. Doing so would lower your \\ninference costs and infrastructure needs but make your deployment process \\nmore complex. \\nThe right approach depends on your application’s needs, such as latency \\nrequirements, hardware, network and privacy concerns, and inference \\ncosts. For a simple prototype like the ML Editor, start with an endpoint or \\na simple batch workflow and iterate from there.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 233}, page_content='Deploying a model comes with more than just exposing it to users, \\nhowever. In Chapter 10, we will cover methods to build safeguards around \\nmodels to mitigate errors, engineering tools to make the deployment \\nprocess more effective, and approaches to validate that models are \\nperforming the way they should be.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 234}, page_content='Chapter 10. Build Safeguards \\nfor Models \\nWhen designing databases or distributed systems, software engineers \\nconcern themselves with fault tolerance, the ability for a system to \\ncontinue working when some of its components fail. In software, the \\nquestion is not whether a given part of the system will fail, but when. The \\nsame principles can be applied to ML. No matter how good a model is, it \\nwill fail on some examples, so you should engineer a system that can \\ngracefully handle such failures. \\nIn this chapter, we will cover different ways to help prevent or mitigate \\nfailures. First, we’ll see how to verify the quality of the data that we \\nreceive and produce and use this verification to decide how to display \\nresults to users. Then, we will take a look at ways to make a modeling \\npipeline more robust to be able to serve many users efficiently. After that, \\nwe’ll take a look at options to leverage user feedback and judge how a \\nmodel is performing. We’ll end the chapter with an interview with Chris \\nMoody about deployment best practices. \\nEngineer Around Failures \\nLet’s cover some of the most likely ways for an ML pipeline to fail. The \\nobservant reader will notice that these failure cases are somewhat similar \\nto the debugging tips we saw in “Debug Wiring: Visualizing and Testing”. \\nIndeed, exposing a model to users in production comes with a set of \\nchallenges that mirrors the ones that come with debugging a model. \\nBugs and errors can show up anywhere, but three areas in particular are \\nmost important to verify: the inputs to a pipeline, the confidence of a \\nmodel, and the outputs it produces. Let’s address each in order. \\nInput and Output Checks \\nAny given model was trained on a specific dataset that exhibited particular \\ncharacteristics. The training data had a certain number of features, and \\neach of these features was of a certain type. Furthermore, each feature'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 235}, page_content='followed a given distribution that the model learned in order to perform \\naccurately. \\nAs we saw in “Freshness and Distribution Shift”, if production data is \\ndifferent from the data a model was trained on, a model may struggle to \\nperform. To help with this, you should check the inputs to your pipeline. \\nCHECK INPUTS \\nSome models may still perform well when faced with small differences in \\ndata distributions. However, if a model receives data that is very different \\nfrom its training data or if some features are missing or of an unexpected \\ntype, it will struggle to perform. \\nAs we saw previously, ML models are able to run even when given \\nincorrect inputs (as long as these inputs are of the right shape and type). \\nModels will produce outputs, but these outputs may be widely incorrect. \\nConsider the example illustrated in Figure 10-1. A pipeline classifies a \\nsentence into one of two topics by first vectorizing it and applying a \\nclassification model on the vectorized representation. If the pipeline \\nreceives a string of random characters, it will still transform it into a \\nvector, and the model will make a prediction. This prediction is absurd, but \\nthere is no way to know it only by looking at the results of the model. \\n \\nFigure 10-1. Models will still output a prediction for random inputs \\nTo prevent a model from running on incorrect outputs, we need to detect \\nthat these inputs are incorrect before passing them to the model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 236}, page_content='CHECKS VERSUS TESTS \\nIn this section, we are speaking of input checks, as opposed to the input tests we \\nsaw in “Test Your ML Code”. The difference is subtle but important. Tests \\nvalidate that code behaves as expected given a known, predetermined input. Tests \\nare commonly run every time code or models change to validate that a pipeline still \\nworks properly. The input checks in this section are part of the pipeline itself and \\nchange the control flow of a program based on the quality of inputs. Input checks \\nthat fail may result in running a different model or not running a model at all. \\nThe checks cover similar domains to the tests in “Test Your ML Code”. In \\norder of importance, they will: \\n1. Verify that all necessary features are present \\n2. Check every feature type \\n3. Validate feature values \\nVerifying feature values in isolation can be hard, as feature distributions \\ncan be complex. A simple way to perform such validation is to define a \\nreasonable range of values a feature could take and validate that it falls \\nwithin that range. \\nIf any of the input checks fail, the model should not run. What you should \\ndo depends on the use case. If the data that is missing represents a core \\npiece of information, you should return an error specifying the source of \\nthe error. If you estimate that you can still provide a result, you can replace \\na model call with a heuristic. This is an additional reason to start any ML \\nproject by building a heuristic; it provides you with an option to fall back \\non! \\nIn Figure 10-2, you can see an example of this logic, where the path taken \\ndepends on the results of the input checks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 237}, page_content='Figure 10-2. Example branching logic for input checks \\nFollowing is an example of some control flow logic from the ML Editor \\nthat checks for missing features and feature types. Depending on the \\nquality of the input, it either raises an error or runs a heuristic. I’ve copied \\nthe example here, but you can also find it on this book’s GitHub \\nrepository with the rest of the ML Editor code. \\ndef validate_and_handle_request(question_data): \\n    missing = find_absent_features(question_data) \\n    if len(missing) > 0: \\n        raise ValueError(\"Missing feature(s) %s\" % missing) \\n \\n    wrong_types = check_feature_types(question_data) \\n    if len(wrong_types) > 0: \\n        # If data is wrong but we have the length of the question, run \\nheuristic \\n        if \"text_len\" in question_data.keys(): \\n            if isinstance(question_data[\"text_len\"], float): \\n                return run_heuristic(question_data[\"text_len\"]) \\n        raise ValueError(\"Incorrect type(s) %s\" % wrong_types) \\n \\n    return run_model(question_data) \\nVerifying model inputs allows you to narrow down failure modes and \\nidentify data input issues. Next, you should validate a model’s outputs. \\nMODEL OUTPUTS \\nOnce a model makes a prediction, you should determine whether it should \\nbe displayed to the user. If the prediction falls outside of an acceptable \\nrange of answers for a model, you should consider not displaying it. \\nFor example, if you are predicting the age of a user from a photo, output \\nvalues should be between zero to a little over 100 years old (if you are'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 238}, page_content='reading this book in the year 3000, feel free to adjust the bounds). If a \\nmodel outputs a value outside of this range, you should not display it. \\nIn this context, an acceptable outcome is not only defined by an outcome \\nthat is plausible. It also depends on your estimation of the kind of outcome \\nthat would be useful to our user. \\nFor our ML editor, we want to only provide recommendations that are \\nactionable. If a model predicts that everything a user wrote should be \\nentirely deleted, this would consist of a rather useless (and insulting) \\nrecommendation. Here is an example snippet validating model outputs and \\nreverting to a heuristic if necessary: \\ndef validate_and_correct_output(question_data, model_output): \\n    # Verify type and range and raise errors accordingly \\n    try: \\n        # Raises value error if model output is incorrect \\n        verify_output_type_and_range(model_output) \\n    except ValueError: \\n        # We run a heuristic, but could run a different model here \\n        run_heuristic(question_data[\"text_len\"]) \\n \\n    # If we did not raise an error, we return our model result \\n    return model_output \\nWhen a model fails, you can revert to a heuristic just as we saw earlier or \\nto a simpler model you may have built earlier. Trying an earlier type of \\nmodel can often be worthwhile because different models may have \\nuncorrelated errors. \\nI’ve illustrated this on a toy example in Figure 10-3. On the left, you can \\nsee a better-performing model with a more complex decision boundary. \\nOn the right, you can see a worse, simpler model. The worse model makes \\nmore mistakes, but its mistakes are different from the complex model \\nbecause of the different shape of its decision boundary. Because of this, \\nthe simpler model gets some examples right that the complex model gets \\nwrong. This is the intuition for why using a simple model as a backup is a \\nreasonable idea when a primary model fails. \\nIf you do use a simpler model as a backup, you should also validate its \\noutputs in the same manner and fall back to a heuristic or display an error \\nif they do not pass your checks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 239}, page_content='Validating that the outputs of a model are in a reasonable range is a good \\nstart, but it isn’t sufficient. In the next section, we will cover additional \\nsafeguards we can build around a model. \\n \\nFigure 10-3. A simpler model often makes different errors \\nModel Failure Fallbacks \\nWe have built safeguards to detect and correct erroneous inputs and \\noutputs. In some cases, however, the input to our model can be correct, \\nand our model’s output can be reasonable while being entirely wrong. \\nTo go back to the example of predicting a user’s age from a photo, \\nguaranteeing that the age predicted by the model is a plausible human age \\nis a good start, but ideally we’d like to predict the correct age for this \\nspecific user. \\nNo model will be right 100 percent of the time, and slight mistakes can \\noften be acceptable, but as much as possible, you should aim to detect \\nwhen a model is wrong. Doing so allows you to potentially flag a given \\ncase as too hard and encourages users to provide an easier input (in the \\nform of a well-lit photo, for example). \\nThere are two main approaches to detecting errors. The simplest one is to \\ntrack the confidence of a model to estimate whether an output will be \\naccurate. The second one is to build an additional model that is tasked with \\ndetecting examples a main model is likely to fail on.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 240}, page_content='For the first method, classification models can output a probability that can \\nbe used as an estimate of the model’s confidence in its output. If those \\nprobabilities are well calibrated (see “Calibration Curve”), they can be \\nused to detect instances where a model is uncertain and decide not to \\ndisplay results to a user. \\nSometimes, models are wrong despite assigning a high probability to an \\nexample. This is where the second approach comes in: using a model to \\nfilter out the hardest inputs. \\nFILTERING MODEL \\nOn top of not always being trustworthy, using a model’s confidence score \\ncomes with another strong drawback. To get this score, the entire inference \\npipeline needs to be run regardless of whether its predictions will be used. \\nThis is especially wasteful when using more complex models that need to \\nbe run on a GPU, for example. Ideally, we would like to estimate how well \\na model will perform on an example without running the model on it. \\nThis is the idea behind filtering models. Since you know some inputs will \\nbe hard for a model to handle, you should detect them ahead of time and \\nnot bother running a model on them at all. A filtering model is the ML \\nversion of input tests. It is a binary classifier that is trained to predict \\nwhether a model will perform well on a given example. The core \\nassumption between such a model is that there are trends in the kind of \\ndata points that are hard for the main model. If such hard examples have \\nenough in common, the filtering model can learn to separate them from \\neasier inputs. \\nHere are some types of inputs you may want a filtering model to catch: \\n\\uf0b7 Inputs that are qualitatively different from ones the main model \\nperforms well on \\n\\uf0b7 Inputs that the model was trained on but struggled with \\n\\uf0b7 Adversarial inputs that are meant to fool the main model \\nIn Figure 10-4, you can see an updated example of the logic in Figure 10-\\n2, which now includes a filtering model. As you can see, the filtering \\nmodel is only run if the input checks pass, because you only need to filter \\nout inputs that could have made their way to the “Run Model” box.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 241}, page_content='To train a filtering model, you simply need to gather a dataset containing \\ntwo categories of examples; categories that your main model succeeded on \\nand others that it failed on. This can be done using our training data and \\nrequires no additional data collection! \\n \\nFigure 10-4. Adding a filtering step to our input checks (bolded) \\nIn Figure 10-5, I show how to do this by leveraging a trained model and its \\nresult on a dataset, as seen in the chart on the left. Sample some data points \\nthat the model predicted correctly and some that the model failed on. You \\ncan then train a filtering model to predict which of the data points are ones \\nthat the original model failed on.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 242}, page_content='Figure 10-5. Getting training data for a filtering model \\nOnce you have a trained classifier, training a filtering model can be \\nrelatively straightforward. Given a test set and a trained classifier, the \\nfollowing function will do just that. \\ndef get_filtering_model(classifier, features, labels): \\n    \"\"\" \\n    Get prediction error for a binary classification dataset \\n    :param classifier: trained classifier \\n    :param features: input features \\n    :param labels: true labels \\n    \"\"\" \\n    predictions = classifier.predict(features) \\n    # Create labels where errors are 1, and correct guesses are 0 \\n    is_error = [pred != truth for pred, truth in zip(predictions, \\nlabels)] \\n \\n    filtering_model = RandomForestClassifier() \\n    filtering_model.fit(features, is_error) \\n    return filtering_model \\nThis approach is used by Google for their Smart Reply feature, which \\nsuggests a few short responses to an incoming email (see this article by A. \\nKanan et al., “Smart Reply: Automated Response Suggestion for Email”). \\nThey use what they call a triggering model, responsible for deciding \\nwhether to run the main model that suggests responses. In their case, only \\nabout 11% of emails are suitable for this model. By using a filtering \\nmodel, they reduce their infrastructure needs by an order of magnitude. \\nA filtering model generally needs to satisfy two criteria. It should be fast \\nsince its whole purpose is to reduce the computational burden, and it \\nshould be good at eliminating hard cases. \\nA filtering model that tries to identify hard cases doesn’t need to be able to \\ncatch all of them; it simply needs to detect enough to justify the added cost \\nof running it on each inference. Generally, the faster your filtering model \\nis, the less effective it needs to be. Here is why: \\nLet’s say your average inference time using only one model is ii. \\nYour average inference time using a filtering model will be f+i(1−b)f+i(1-\\nb) where f is the execution time of your filtering model, and b is the \\naverage proportion of examples it filters out (b for block).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 243}, page_content='To reduce your average inference time by using a filtering model, you thus \\nneed to have f+i(1−b)<if+i(1-b)<i, which translates to fi<bfi<b. \\nThis means the proportion of cases your model filters out needs to be \\nhigher than the ratio between its inference speed and the speed of your \\nlarger model. \\nFor example, if your filtering model is 20 times faster than your regular \\nmodel ( fi=5%fi=5% ), it would need to block more than 5% of cases \\n( 5%<b5%<b ) to be useful in production. \\nOf course, you would also need to make sure that the precision of your \\nfiltering model is good, meaning that the majority of the inputs it blocks \\nare actually too hard for your main model. \\nOne way to do this would be to regularly let a few examples through that \\nyour filtering model would have blocked and examine how your main \\nmodel does on them. We will cover this in more depth in “Choose What to \\nMonitor”. \\nSince the filtering model is different from the inference model and trained \\nspecifically to predict hard cases, it can detect these cases more accurately \\nthan by relying on the main model’s probability output. Using a filtering \\nmodel thus helps both decreasing the likelihood of poor results and \\nimproving resource usage. \\nFor these reasons, adding filtering models to existing input and output \\nchecks can significantly increase the robustness of a production pipeline. \\nIn the next section, we will tackle more ways to make pipelines robust by \\ndiscussing how to scale ML applications to more users and how to \\norganize complex training processes. \\nEngineer for Performance \\nMaintaining performance when deploying models to production is a \\nsignificant challenge, especially as a product becomes more popular and \\nnew versions of a model get deployed regularly. We will start this section \\nby discussing methods to allow models to process large amounts of \\ninference requests. Then, we will cover features that make it easier to \\nregularly deploy updated model versions. Finally, we will discuss methods'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 244}, page_content='to reduce variance in performance between models by making training \\npipelines more reproducible. \\nScale to Multiple Users \\nMany software workloads are horizontally scalable, meaning that spinning \\nup additional servers is a valid strategy to keep response time reasonable \\nwhen the number of requests increases. ML is no different in this aspect, \\nas we can simply spin up new servers to run our models and handle the \\nextra capacity. \\nNOTE \\nIf you use a deep learning model, you may need a GPU to serve results in \\nan acceptable time. If that is the case and you are expecting to have \\nenough requests to require more than a single GPU-enabled machine, you \\nshould run your application logic and your model inference on two \\ndifferent servers. \\nBecause GPU instances are often an order of magnitude more expensive \\nthan regular instances for most cloud providers, having one cheaper \\ninstance scale out your application and GPU instances tackling only \\ninference will significantly lower your compute costs. When using this \\nstrategy, you should keep in mind that you are introducing some \\ncommunication overhead and make sure that this is not too detrimental to \\nyour use case. \\nIn addition to increasing resource allocation, ML lends itself to efficient \\nways to handle additional traffic, such as caching. \\nCACHING FOR ML \\nCaching is the practice of storing results to function calls so that future \\ncalls to this function with the same parameters can be run faster by simply \\nretrieving the stored results. Caching is a common practice to speed up \\nengineering pipelines and is very useful for ML. \\nCaching inference results \\nA least recently used (LRU) cache is a simple caching approach, which \\nentails keeping track of the most recent inputs to a model and their'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 245}, page_content='corresponding outputs. Before running the model on any new input, look \\nup the input in the cache. If a corresponding entry is found, serve the \\nresults directly from the cache. Figure 10-6 shows an example of such a \\nworkflow. The first row represents the caching step when an input is \\ninitially encountered. The second row depicts the retrieval step once the \\nsame input is seen again. \\n \\nFigure 10-6. Caching for an image captioning model \\nThis sort of caching strategy works well for applications where users will \\nprovide the same kind of input. It is not appropriate if each input is unique. \\nIf an application takes in photos of paw prints to predict which animal they \\nbelong to, it should rarely receive two identical photos, so a LRU cache \\nwould not help. \\nWhen using caching, you should only cache functions with no side effects. \\nIf a run_model function also stores results to a database, for example, using \\nan LRU cache will cause duplicate function calls to not be saved, which \\nmay not be the intended behavior. \\nIn Python, the functools module proposes a default implementation of an \\nLRU cache that you can use with a simple decorator, as shown here: \\nfrom functools import lru_cache \\n \\n@lru_cache(maxsize=128) \\ndef run_model(question_data): \\n    # Insert any slow model inference below \\n    pass'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 246}, page_content='Caching is most useful when retrieving features, processing them, and \\nrunning inference is slower than accessing a cache. Depending on your \\napproach to caching (in memory versus on disk, for example) and the \\ncomplexity of the model you are using, caching will have different degrees \\nof usefulness. \\nCaching by indexing \\nWhile the caching method described is not appropriate when receiving \\nunique inputs, we can cache other aspects of the pipeline that can be \\nprecomputed. This is easiest if a model does not only rely on user inputs. \\nLet’s say we are building a system that allows users to search for content \\nthat is related to either a text query or an image they provide. It is unlikely \\nthat caching user queries would boost performance by much if we expect \\nqueries to vary significantly. Since we are building a search system, \\nhowever, we have access to a list of potential items in our catalog that we \\ncould return. This list is known to us in advance, whether we are an online \\nretailer or a document indexing platform. \\nThis means that we could precompute modeling aspects that depend only \\non the items in our catalog. If we chose a modeling approach that allows \\nus to do this computation ahead of time, we can make inference \\nsignificantly faster. \\nFor this reason, a common approach when building a search system is to \\nfirst embed all indexed documents to a meaningful vector (refer \\nto “Vectorizing” for more on vectorization methods). Once embeddings \\nare created, they can be stored in a database. This is illustrated on the top \\nrow of Figure 10-7. When a user submits a search query, it is embedded at \\ninference time, and a lookup is performed in the database to find the most \\nsimilar embeddings and return the products that correspond to these \\nembeddings. You can see this illustrated in the bottom row of Figure 10-7. \\nThis approach significantly speeds up inference since most of the \\ncalculations have been done ahead of time. Embeddings have been \\nsuccessfully used in large-scale production pipelines at companies such as \\nTwitter (see this post on Twitter’s blog) and Airbnb (see this article by M. \\nHaldar et al., “Applying Deep Learning To Airbnb Search”.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 247}, page_content='Figure 10-7. A search query with cached embeddings \\nCaching can improve performance, but it adds a layer of complexity. The \\nsize of the cache becomes an additional hyperparameter to tune depending \\non your application’s workload. In addition, any time a model or the \\nunderlying data is updated, the cache needs to be cleared in order to \\nprevent it from serving outdated results. More generally, updating a model \\nrunning in production to a new version often requires care. In the next \\nsection, we will cover a few domains that can help make such updates \\neasier. \\nModel and Data Life Cycle Management \\nKeeping caches and models up-to-date can be challenging. Many models \\nrequire regular retraining to maintain their level of performance. While we \\nwill cover when to retrain your models in Chapter 11, I’d like to briefly \\ntalk about how to deploy updated models to users. \\nA trained model is usually stored as a binary file containing information \\nabout its type and architecture, as well as its learned parameters. Most \\nproduction applications load a trained model in memory when they start \\nand call it to serve results. A simple way to replace a model with a newer \\nversion is to replace the binary file the application loads. This is illustrated \\nin Figure 10-8, where the only aspect of the pipeline that is impacted by a \\nnew model is the bolded box. \\nIn practice, however, this process is often much more involved. Ideally, an \\nML application produces reproducible results, is resilient to model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 248}, page_content='updates, and is flexible enough to handle significant modeling and data \\nprocessing changes. Guaranteeing this involves a few additional steps that \\nwe will cover next. \\n \\nFigure 10-8. Deploying an updated version of the same model can seem like a simple change \\nREPRODUCIBILITY \\nTo track down and reproduce errors, you’ll need to know which model is \\nrunning in production. To do so requires keeping an archive of trained \\nmodels and the datasets they were trained on. Each model/dataset pair \\nshould be assigned a unique identifier. This identifier should be logged \\neach time a model is used in production. \\nIn Figure 10-9, I’ve added these requirements to the load and save boxes \\nto represent the complexity this adds to an ML pipeline.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 249}, page_content='Figure 10-9. Adding crucial metadata when saving and loading \\nIn addition to being able to serve different versions of existing models, a \\nproduction pipeline should aim to update models without significant \\ndowntime. \\nRESILIENCE \\nEnabling an application to load a new model once it is updated requires \\nbuilding a process to load a newer model, ideally without disrupting \\nservice to your users. This can consist of launching a new server serving \\nthe updated model and slowly transition traffic to it, but it quickly \\nbecomes more complex for larger systems. If a new model performs \\npoorly, we’d like to be able to roll back to the previous one. Doing both of \\nthese tasks properly is challenging and would traditionally be categorized \\nin the realm of DevOps. While we won’t cover this domain in depth, we \\nwill introduce monitoring in Chapter 11. \\nProduction changes can be more complex than updating a model. They can \\ninclude large changes to data processing, which should also be deployable. \\nPIPELINE FLEXIBILITY \\nWe previously saw that the best way to improve a model is often by \\niterating on data processing and feature generation. This means that new \\nversions of a model will often require additional preprocessing steps or \\ndifferent features.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 250}, page_content='This kind of change is reflected in more than just the model binary and \\nwould often be tied to a new version of your application. For this reason, \\nthe application version should also be logged when a model makes a \\nprediction in order to make this prediction reproducible. \\nDoing so adds another level of complexity to our pipeline, depicted with \\nthe added preprocessing and postprocessing boxes in Figure 10-10. These \\nnow also need to be reproducible and modifiable. \\nDeploying and updating models is challenging. When building a serving \\ninfrastructure, the most important aspect is to be able to reproduce the \\nresults of a model running in production. This means tying each inference \\ncall to the model that was run, the dataset that model was trained on, and \\nthe version of the data pipeline that served this model. \\n \\nFigure 10-10. Adding model and application version \\nData Processing and DAGs \\nTo produce reproducible results as described earlier, a training pipeline \\nshould also be reproducible and deterministic. For a given combination of \\ndataset, preprocessing steps, and model, a training pipeline should produce \\nthe same trained model on every training run. \\nMany successive transformation steps are required to building a model, so \\npipelines will often break at different locations. This makes guaranteeing \\nthat each part was run successfully and that they were all run in the right \\norder.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 251}, page_content='One way to make this challenge easier is by representing our process of \\ngoing from raw data to trained model as a directed acyclic graph (DAG), \\nwith each node representing a processing step and each step representing a \\ndependency between two nodes. This idea is at the core of dataflow \\nprogramming, a programming paradigm that the popular ML library \\nTensorFlow is based on. \\nDAGs can be a natural way to visualize preprocessing. In Figure 10-11, \\neach arrow represents a task that depends on another one. The \\nrepresentation allows us to keep each task simple, using the graph \\nstructure to express complexity. \\n \\nFigure 10-11. An example of a DAG for our application \\nOnce we have a DAG, we can then guarantee that we follow the same set \\nof operations for each model that we produce. There are multiple solutions \\nto define DAGs for ML, including active opensource projects such \\nas Apache Airflow or Spotify’s Luigi. Both packages allow you to define \\nDAGs and provide a set of dashboards to allow you to monitor the \\nprogress of your DAGs and any associated logs. \\nWhen first building an ML pipeline, using a DAG can be unnecessarily \\ncumbersome, but once a model becomes a core part of a production \\nsystem, reproducibility requirements make DAGs very compelling. Once \\nmodels are being regularly retrained and deployed, any tool that helps \\nsystematize, debug, and version a pipeline will become a crucial time-\\nsaver.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 252}, page_content='To conclude this chapter, I will cover an additional and direct way to \\nguarantee that a model is performing well—asking users. \\nAsk for Feedback \\nThis chapter covered systems that can help ensure we give every user an \\naccurate result in a timely manner. To guarantee the quality of results, we \\ncovered tactics to detect whether a model’s predictions are inaccurate. \\nWhy don’t we ask users? \\nYou can gather feedback from users both by explicitly asking for feedback \\nand by measuring implicit signals. You can ask for explicit feedback when \\ndisplaying a model’s prediction, by accompanying it with a way for users \\nto judge and correct a prediction. This can be as simple as a dialog asking \\n“was this prediction useful?” or something more subtle. \\nThe budgeting application Mint, for example, categorizes each transaction \\non an account automatically (categories include Travel, Food, etc.). As \\ndepicted in Figure 10-12, each category is shown in the UI as a field the \\nuser can edit and correct if needed. Such systems allow valuable feedback \\nto be collected to continuously improve models in a way that is less \\nintrusive than a satisfaction survey, for example.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 253}, page_content='Figure 10-12. Let users fix mistakes directly \\nUsers cannot provide feedback for each prediction a model makes, so \\ngathering implicit feedback is an important way to judge ML performance. \\nGathering such feedback consists of looking at actions users perform to \\ninfer whether a model provided useful results. \\nImplicit signals are useful but harder to interpret. You shouldn’t hope to \\nfind an implicit signal that always correlates with model quality, only one \\nthat does so in aggregate. For example, in a recommendation system, if a \\nuser clicks on a recommended item, you can reasonably assume that the \\nrecommendation was valid. This will not be true in all cases (people click \\non the wrong things sometimes!), but as long as it is true more often than \\nnot, it is a reasonable implicit signal. \\nBy collecting this information, as shown in Figure 10-13, you can then \\nestimate how often users found results useful. The collection of such \\nimplicit signals is useful but comes with the added risk of collecting and \\nstoring this data and potentially introducing negative feedback loops as we \\ndiscussed in Chapter 8.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 254}, page_content='Figure 10-13. User actions as a source for feedback \\nBuilding implicit feedback mechanisms in your product can be a valuable \\nway to gather additional data. Many actions can be considered a mix of \\nimplicit and explicit feedback. \\nLet’s say we added a “Ask the question on Stack Overflow” button to the \\nrecommendations of our ML editor. By analyzing which predictions led to \\nusers clicking this button, we could measure the proportion of \\nrecommendations that were good enough to be posted as questions. By \\nadding this button, we aren’t directly asking users whether the suggestion \\nis good, but we allow them to act on it, thus giving us a “weak label” \\n(see “Data types” for a reminder on weakly labeled data) of question \\nquality. \\nIn addition to being a good source of training data, implicit and explicit \\nuser feedback can be the first way to notice a degradation in performance \\nin an ML product. While ideally errors should be caught before being \\ndisplayed to users, monitoring such feedback helps detect and fix bugs \\nquicker. We will cover this in more detail in Chapter 11.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 255}, page_content='Strategies for deploying and updating models vary tremendously \\ndepending on the size of a team and their experience with ML. Some of \\nthe solutions in this chapter are excessively complex for a prototype such \\nas the ML Editor. On the other hand, some teams that have invested a \\nsignificant amount of resources into ML have built complex systems that \\nallow them to simplify their deployment process and guarantee a high \\nlevel of quality to users. Next, I’ll share an interview with Chris Moody, \\nwho leads Stitch Fix’s AI Instruments team and will take us through their \\nphilosophy when it comes to deploying ML models. \\nChris Moody: Empowering Data Scientists \\nto Deploy Models \\nChris Moody came from a physics background from Caltech and UCSC \\nand is now leading Stitch Fix’s AI Instruments team. He has an avid \\ninterest in NLP and has dabbled in deep learning, variational methods, and \\nGaussian processes. He’s contributed to the Chainer deep learning library, \\ncontributed to the super fast Barnes–Hut version of t-SNE to scikit-learn, \\nand written (one of the few!) sparse tensor factorization libraries \\nin Python. He also built his own NLP model, lda2vec. \\nQ: What part of the model life cycle do data scientists work on at Stitch \\nFix? \\nA: At Stitch Fix, data scientists own the entire modeling pipeline. This \\npipeline is broad and includes things such as ideation, prototyping, design \\nand debugging, ETL, and model training in languages and frameworks \\nsuch as scikit-learn, pytorch, and R. In addition, data scientists are in \\ncharge of setting up systems to measure metrics and building “sanity \\nchecks” for their models. Finally, data scientists run the A/B test, monitor \\nerrors and logs, and redeploy updated model versions as needed based on \\nwhat they observe. To be able to do this, they leverage the work done by \\nthe platform and engineering team. \\nQ: What does the platform team do to make data science work easier? \\nA: The goal of engineers on the platform team is to find the right \\nabstractions for modeling. This means they need to understand how a data \\nscientist works. Engineers don’t build individual data pipelines for data \\nscientists working on a given project. They build solutions that enable data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 256}, page_content='scientists to do so themselves. More generally, they build tools to \\nempower data scientists to own the entire workflow. This empowers \\nengineers to spend more time making the platform better and less time \\nbuilding one-off solutions. \\nQ: How do you judge the performance of models once they are deployed? \\nA: A big part of Stitch Fix’s strength is in making humans and algorithms \\nwork together. For example, Stitch Fix spends a lot of time thinking about \\nthe right way to present information to their stylists. Fundamentally, if you \\nhave an API that exposes your model on one end and a user such as a \\nstylist or merchandise buyer on the other hand, how should you design \\ninteractions between them? \\nAt first glance, you could be tempted to build a frontend to simply present \\nthe results of your algorithm to users. Unfortunately, this can lead users to \\nfeel like they have no control over the algorithm and the overall system \\nand can lead to frustration when it isn’t performing well. Instead, you \\nshould think about this interaction as a feedback loop, allowing users to \\ncorrect and adjust results. Doing so lets users train algorithms and have a \\nmuch larger impact on the entire process by being able to give feedback. \\nIn addition, this allows you to gather labeled data to judge the performance \\nof your models. \\nTo do this well, data scientists should ask themselves how they can expose \\na model to a user in order to both make their job easier and empower them \\nto make the model better. This means that since data scientists know best \\nwhat kind of feedback would be the most useful for their models, it is \\nintegral for them to own the process end-to-end up to this point. They can \\ncatch any errors because they can see the entire feedback loop. \\nQ: How do you monitor and debug models? \\nA: When your engineering team builds great tooling, monitoring and \\ndebugging get much easier. Stitch Fix has built an internal tool that takes \\nin a modeling pipeline and creates a Docker container, validates arguments \\nand return types, exposes the inference pipeline as an API, deploys it on \\nour infrastructure, and builds a dashboard on top of it. This tooling allows \\ndata scientists to directly fix any errors that happen during or after \\ndeployment. Because data scientists are now in charge of troubleshooting \\nmodels, we have also found that this setup incentivizes simple and robust'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 257}, page_content='models that tend to break more rarely. Ownership of the entire pipeline \\nleads individuals to optimize for impact and reliability, rather than model \\ncomplexity. \\nQ: How do you deploy new model versions? \\nA: In addition, data scientists run experiments by using a custom-built A/B \\ntesting service that allows them to define granular parameters. They then \\nanalyze test results, and if they are deemed conclusive by the team, they \\ndeploy the new version themselves. \\nWhen it comes to deployment, we use a system similar to canary \\ndevelopment where we start by deploying the new version to one instance \\nand progressively update instances while monitoring performance. Data \\nscientists have access to a dashboard that shows the number of instances \\nunder each version and continuous performance metrics as the deployment \\nprogresses. \\nConclusion \\nIn this chapter, we’ve covered ways to make our responses more resilient \\nby detecting potential failures of our model proactively and finding ways \\nto mitigate them. This has included both deterministic validation strategies \\nand the use of filtering models. We also covered a few of the challenges \\nthat come with keeping a production model up-to-date. Then, we discussed \\nsome of the ways that we can estimate how well a model is performing. \\nFinally, we took a look at a practical example of a company that deploys \\nML frequently and at large scale, and the processes they have built to do \\nso. \\nIn Chapter 11, we will cover additional methods to keep an eye on the \\nperformance of models and leverage a variety of metrics to diagnose the \\nhealth of an ML-powered application.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 258}, page_content='Chapter 11. Monitor and Update Models \\nOnce a model is deployed, its performance should be monitored just like \\nany other software system. As they did in “Test Your ML Code”, regular \\nsoftware best practices apply. And just like in “Test Your ML Code”, there \\nare additional things to consider when dealing with ML models. \\nIn this chapter, we will describe key aspects to keep in mind when \\nmonitoring ML models. More specifically, we will answer three questions: \\n1. Why should we monitor our models? \\n2. How do we monitor our models? \\n3. What actions should our monitoring drive? \\nLet’s start by covering how monitoring models can help decide when to \\ndeploy a new version or surface problems in production. \\nMonitoring Saves Lives \\nThe goal of monitoring is to track the health of a system. For models, this \\nmeans monitoring their performance and the quality of their predictions. \\nIf a change in user habits suddenly causes a model to produce subpar \\nresults, a good monitoring system will allow you to notice and react as \\nsoon as possible. Let’s cover some key issues that monitoring can help us \\ncatch. \\nMonitoring to Inform Refresh Rate \\nWe saw in “Freshness and Distribution Shift” that most models need to be \\nregularly updated to maintain a given level of performance. \\nMonitoring can be used to detect when a model is not fresh anymore and \\nneeds to be retrained. \\nFor example, let’s say that we use the implicit feedback that we get from \\nour users (whether they click on recommendations, for example) to \\nestimate the accuracy of a model. If we continuously monitor the accuracy \\nof the model, we can train a new model as soon as accuracy drops below a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 259}, page_content='defined threshold. Figure 11-1 shows a timeline of this process, with \\nretraining events happening when accuracy dips below a threshold. \\n \\nFigure 11-1. Monitoring to trigger redeploy \\nBefore redeploying an updated model, we would need to verify that the \\nnew model is better. We will cover how to do this later in this \\nsection, “CI/CD for ML”. First, let’s tackle other aspects to monitor, such \\nas potential abuse. \\nMonitor to Detect Abuse \\nIn some cases such as when building abuse prevention or fraud detection \\nsystems, a fraction of users are actively working to defeat models. In these \\ncases, monitoring becomes a key way to detect attacks and estimate their \\nsuccess rate. \\nA monitoring system can use anomaly detection to detect attacks. When \\ntracking every attempt to log in to a bank’s online portal, for example, a \\nmonitoring system could raise an alert if the number of login attempts \\nsuddenly increased tenfold, which could be a sign of an attack. \\nThis monitoring could raise an alert based on a threshold value being \\ncrossed, as you can see in Figure 11-2, or include more nuanced metrics \\nsuch as the rate of increase of login attempts. Depending on the \\ncomplexity of attacks, it may be valuable to build a model to detect such \\nanomalies with more nuance than a simple threshold could.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 260}, page_content='Figure 11-2. An obvious anomaly on a monitoring dashboard. You could build an additional ML model \\nto automatically detect it. \\nIn addition to monitoring freshness and detecting anomalies, which other \\nmetrics should we monitor? \\nChoose What to Monitor \\nSoftware applications commonly monitor metrics such as the average time \\nit takes to process a request, the proportion of requests that fail to be \\nprocessed, and the amount of available resources. These are useful to track \\nin any production service and allow for proactive remediation before too \\nmany users are impacted. \\nNext, we will cover more metrics to monitor to detect when a model’s \\nperformance is starting to decline. \\nPerformance Metrics \\nA model can become stale if the distribution of data starts to change. You \\ncan see this illustrated in Figure 11-3.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 261}, page_content='Figure 11-3. Example of drift in a feature’s distribution \\nWhen it comes to distribution shifts, both the input and the output \\ndistribution of data can change. Consider the example of a model that tries \\nto guess which movie a user will watch next. Given the same user history \\nas an input, the model’s prediction should change based on new entries in \\na catalog of available movies. \\n\\uf0b7 Tracking changes in the input distribution (also called feature drift) \\nis easier than tracking the output distribution, since it can be \\nchallenging to access the ideal value of outputs to satisfy users. \\n\\uf0b7 Monitoring the input distribution can be as simple as monitoring \\nsummary statistics such as the mean and variance of key features and \\nraising an alert if these statistics drift away from the values in the \\ntraining data by more than a given threshold. \\n\\uf0b7 Monitoring distribution shifts can be more challenging. A first \\napproach is to monitor the distribution of model outputs. Similarly to \\ninputs, a significant change in the distribution of outputs may be a \\nsign that model performance has degraded. The distribution of the \\nresults users would have liked to see, however, can be harder to \\nestimate. \\nOne of the reasons for why estimating ground truth can be hard is that a \\nmodel’s actions can often prevent us from observing it. To see why that \\nmay be the case, consider the illustration of a credit card fraud detection \\nmodel in Figure 11-4. The distribution of the data that the model will'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 262}, page_content='receive is on the left side. As the model makes predictions on the data, \\napplication code acts on these predictions by blocking any transaction \\npredicted as fraudulent. \\nOnce a transaction is blocked, we are thus unable to observe what would \\nhave happened if we had let it through. This means that we are not be able \\nto know whether the blocked transaction was actually fraudulent or not. \\nWe are only able to observe and label the transactions we let through. \\nBecause of having acted on a model’s predictions, we are only able to \\nobserve a skewed distribution of nonblocked transactions, represented on \\nthe right side. \\n \\nFigure 11-4. Taking action based on a model’s predictions can bias the observed distribution of data  \\nOnly having access to a skewed sample of the true distribution makes it \\nimpossible to correctly evaluate a model’s performance. This is the focus \\nof counterfactual evaluation, which aims to evaluate what would have \\nhappened if we hadn’t actioned a model. To perform such evaluation in \\npractice, you can withhold running a model on a small subset of examples \\n(see the article by Lihong Li et al., “Counterfactual Estimation and \\nOptimization of Click Metrics for Search Engines”). Not acting on a \\nrandom subset of examples will then allow us to observe an unbiased \\ndistribution of fraudulent transactions. By comparing model predictions to \\ntrue outcomes for the random data, we can begin to estimate a model’s \\nprecision and recall. \\nThis approach provides a way to evaluate models but comes at the cost of \\nletting a proportion of fraudulent transactions go through. In many cases, \\nthis trade-off can be favorable since it allows for model benchmarking and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 263}, page_content='comparisons. In some cases, such as in medical domains where outputting \\na random prediction is not acceptable, this approach should not be used. \\nIn “CI/CD for ML”, we’ll cover other strategies to compare models and \\ndecide which ones to deploy, but first, let’s cover the other key types of \\nmetrics to track. \\nBusiness Metrics \\nAs we’ve seen throughout this book, the most important metrics are the \\nones related to product and business goals. They are the yardstick against \\nwhich we can judge our model’s performance. If all of the other metrics \\nare in the green and the rest of the production system is performing well \\nbut users don’t click on search results or use recommendations, then a \\nproduct is failing by definition. \\nFor this reason, product metrics should be closely monitored. For systems \\nsuch as search or recommendation systems, this monitoring could track the \\nCTR, the ratio at which people that have seen a model’s recommendation \\nclicked on it. \\nSome applications may benefit from modifications to the product to more \\neasily track product success, similarly to the feedback examples we saw \\nin “Ask for Feedback”. We discussed adding a share button, but we could \\ntrack feedback at a more granular level. If we can have users click on \\nrecommendations in order to implement them, we can track whether each \\nrecommendation was used and use this data to train a new version of the \\nmodel. Figure 11-5 shows an illustrated comparison between the aggregate \\napproach on the left side and the granular one on the right.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 264}, page_content='Figure 11-5. Proposing word-level suggestions gives us more opportunities to collect user feedback \\nSince I do not expect the ML Editor prototype to be used frequently \\nenough for the method described to provide a large enough dataset, we \\nwill abstain from building it here. If we were building a product we were \\nintending to maintain, collecting such data would allow us to get precise \\nfeedback about which recommendations the user found the most useful. \\nNow that we have discussed reasons and methods to monitor models, let’s \\ncover ways to address any issues detected by monitoring. \\nCI/CD for ML \\nCI/CD stands for continuous integration (CI) and continuous delivery \\n(CD). Roughly speaking, CI is the process of letting multiple developers \\nregularly merge their code back into a central codebase, while CD focuses \\non improving the speed at which new versions of software can be released. \\nAdopting CI/CD practices allows individuals and organizations to quickly \\niterate and improve on an application, whether they are releasing new \\nfeatures or fixing existing bugs. \\nCI/CD for ML thus aims to make it easier to deploy new models or update \\nexisting ones. Releasing updates quickly is easy; the challenge comes in \\nguaranteeing their quality. \\nWhen it comes to ML, we saw that having a test suite is not enough to \\nguarantee that a new model improves upon a previous one. Training a new \\nmodel and testing that it performs well on held-out data is a good first step,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 265}, page_content='but ultimately, as we saw earlier, there is no substitute for live \\nperformance to judge the quality of a model. \\nBefore deploying a model to users, teams will often deploy them in what \\nSchelter et al., in their paper, “On Challenges in Machine Learning Model \\nManagement”, refer to as shadow mode. This refers to the process of \\ndeploying a new model in parallel to an existing one. When running \\ninference, both models’ predictions are computed and stored, but the \\napplication only uses the prediction of the existing model. \\nBy logging the new predicted value and comparing it both to the old \\nversion and to ground truth when it is available, engineers can estimate a \\nnew model’s performance in a production environment without changing \\nthe user experience. This approach also allows to test the infrastructure \\nrequired to run inference for a new model that may be more complex than \\nthe existing one. The only thing shadow mode doesn’t provide is the \\nability to observe the user’s response to the new model. The only way to \\ndo that is to actually deploy it. \\nOnce a model has been tested, it is a candidate for deployment. Deploying \\na new model comes with the risk of exposing users to a degradation of \\nperformance. Mitigating that risk requires some care and is the focus of the \\nfield of experimentation. \\nFigure 11-6 shows a visualization of each of the three approaches we \\ncovered here, from the safest one of evaluating a mode on a test set to the \\nmost informative and dangerous one of deploying a model live in \\nproduction. Notice that while shadow mode does require engineering \\neffort in order to be able to run two models for each inference step, it \\nallows for the evaluation of a model to be almost as safe as using a test set \\nand provides almost as much information as running it in production.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 266}, page_content='Figure 11-6. Ways to evaluate a model, from safest and least accurate to riskiest and most accurate \\nSince deploying models in production can be a risky process, engineering \\nteams have developed methods to deploy changes incrementally, starting \\nby showing new results to only a subset of users. We will cover this next. \\nA/B Testing and Experimentation \\nIn ML, the goal of experimentation is to maximize chances of using the \\nbest model, while minimizing the cost of trying out suboptimal models. \\nThere are many experimentation approaches, the most popular being A/B \\ntesting. \\nThe principle behind A/B testing is simple: expose a sample of users to a \\nnew model, and the rest to another. This is commonly done by having a \\nlarger “control” group being served the current model and a smaller \\n“treatment” group being served a new version that we want to test. Once \\nwe have run an experiment for a sufficient amount of time, we compare \\nthe results for both groups and choose the better model. \\nIn Figure 11-7, you can see how to randomly sample users from a total \\npopulation to allocate them to a test set. At inference time, the model used \\nfor a given user is determined by their allocated group. \\nThe idea behind A/B testing is simple, but experimental design concerns \\nsuch as choosing the control and the treatment group, deciding which \\namount of time is sufficient, and evaluating which model performs better \\nare all challenging issues.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 267}, page_content='Figure 11-7. An example of an A/B test \\nIn addition, A/B testing requires the building of additional infrastructure to \\nsupport the ability to serve different models to different users. Let’s cover \\neach of these challenges in more detail. \\nCHOOSING GROUPS AND DURATION \\nDeciding which users should be served which model comes with a few \\nrequirements. Users in both groups should be as similar as possible so that \\nany observed difference in outcome can be attributed to our model and not \\nto a difference in cohorts. If all users in group A are power users and \\ngroup B contains only occasional users, the results of an experiments will \\nnot be conclusive. \\nIn addition, the treatment group B should be large enough to draw a \\nstatistically meaningful conclusion, but as small as possible to limit \\nexposure to a potentially worse model. The duration of the test presents a \\nsimilar trade-off: too short and we risk not having enough information, too \\nlong and we risk losing users. \\nThese two constraints are challenging enough, but consider for a minute \\nthe case of large companies with hundreds of data scientists who run \\ndozens of A/B tests in parallel. Multiple A/B tests may be testing the same \\naspect of the pipeline at the same time, making it harder to determine the \\neffect of an individual test accurately. When companies get to this scale, \\nthis leads them to building experimentation platforms to handle the \\ncomplexity. See Airbnb’s ERF, as described in Jonathan Parks’s \\narticle, “Scaling Airbnb’s Experimentation Platform”; Uber’s XP as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 268}, page_content='described in A. Deb et al.’s post, “Under the Hood of Uber’s \\nExperimentation Platform”; or the GitHub repo for Intuit’s open \\nsource Wasabi. \\nESTIMATING THE BETTER VARIANT \\nMost A/B tests choose a metric they would like to compare between \\ngroups such as CTR. Unfortunately, estimating which version performed \\nbetter is more complex than selecting the group with the highest CTR. \\nSince we expect there to be natural fluctuations in any metric results, we \\nfirst need to determine whether results are statistically significant. Since \\nwe are estimating a difference between two populations, the most common \\ntests that are used are two-sample hypothesis tests. \\nFor an experiment to be conclusive, it needs to be run on a sufficient \\namount of data. The exact quantity depends on the value of the variable we \\nare measuring and the scale of the change we are aiming to detect. For a \\npractical example, see Evan Miller’s sample size calculator. \\nIt is also important to decide on the size of each group and the length of \\nthe experiment before running it. If you instead continuously test for \\nsignificance while an A/B test is ongoing and declare the test successful as \\nsoon as you see a significant result, you will be committing a repeated \\nsignificance testing error. This kind of error consists of severely \\noverestimating the significance of an experiment by opportunistically \\nlooking for significance (once again, Evan Miller has a great \\nexplanation here). \\nNOTE \\nWhile most experiments focus on comparing the value of a single metric, \\nit is important to also consider other impacts. If the average CTR increases \\nbut the number of users who stop using the product doubles, we probably \\nshould not consider a model to be better. \\nSimilarly, results of A/B tests should take into account results for different \\nsegments of users. If the average CTR increases but the CTR for a given \\nsegment plummets, it may be better to not deploy the new model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 269}, page_content='Implementing an experiment requires the ability to assign users to a group, \\ntrack each user’s assignment, and present different results based on it. This \\nnecessitates building additional infrastructure, which we cover next. \\nBUILDING THE INFRASTRUCTURE \\nExperiments also come with infrastructure requirements. The simplest way \\nto run an A/B test is to store each user’s associated group with the rest of \\nuser-related information, such as in a database. \\nThe application can then rely on branching logic that decides which model \\nto run depending on the given field’s value. This simple approach works \\nwell for systems where users are logged in but becomes significantly \\nharder if a model is accessible to logged-out users. \\nThis is because experiments usually assume that each group is independent \\nand exposed to only one variant. When serving models to logged out users, \\nit becomes harder to guarantee that a given user was always served the \\nsame variant across each session. If most users are exposed to multiple \\nvariants, this could invalidate the results of an experiment. \\nOther information to identify users such as browser cookies and IP \\naddresses can be used to identify users. Once again, however, such \\napproaches require building new infrastructure, which may be hard for \\nsmall, resource-constrained teams. \\nOther Approaches \\nA/B testing is a popular experimentation method, but other approaches \\nexist that try to address some of A/B testing’s limitations. \\nMultiarmed bandits are a more flexible approach that can test variants \\ncontinually and on more than two alternatives. They dynamically update \\nwhich model to serve based on how well each option is performing. I’ve \\nillustrated how multiarmed bandits work in Figure 11-8. Bandits \\ncontinuously keep a tally of how each alternative is performing based on \\nthe success of each request they route. Most requests are simply routed to \\nthe current best alternative, as shown on the left. A small subset of \\nrequests gets routed to a random alternative, as you can see on the right. \\nThis allows bandits to update their estimate of which model is the best and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 270}, page_content='detect if a model that is currently not being served is starting to perform \\nbetter. \\n \\nFigure 11-8. Multiarmed bandits in practice \\nContextual multiarmed bandits take this process even further, by learning \\nwhich model is a better option for each particular user. For more \\ninformation, I recommend this overview by the Stitch Fix team. \\nNOTE \\nWhile this section covered the usage of experimentation to validate \\nmodels, companies increasingly use experimentation methods to validate \\nany significant change they make to their applications. This allows them to \\ncontinuously evaluate which functionality users are finding useful and how \\nnew features are performing. \\nBecause experimentation is such a hard and error-prone process, multiple \\nstartups have started offering “optimization services” allowing customers \\nto integrate their applications with a hosted experimentation platform to \\ndecide which variants perform best. For organizations without a dedicated \\nexperimentation team, such solutions may be the easiest way to test new \\nmodel versions. \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 271}, page_content='Overall, deploying and monitoring models is still a relatively new practice. \\nIt is a crucial way to verify that models are producing value but often \\nrequires significant efforts both in terms of infrastructure work and careful \\nproduct design. \\nAs the field has started to mature, experimentation platforms such \\nas Optimizely have emerged to make some of this work easier. Ideally, \\nthis should empower builders of ML applications to make them \\ncontinuously better, for everyone. \\nLooking back at all the systems described in this book, only a small subset \\naims to train models. The majority of work involved with building ML \\nproducts consists of data and engineering work. Despite this fact, most of \\nthe data scientists I have mentored found it easier to find resources \\ncovering modeling techniques and thus felt unprepared to tackle work \\noutside of this realm. This book is my attempt at helping bridge that gap. \\nBuilding an ML application requires a broad set of skills in diverse \\ndomains such as statistics, software engineering, and product management. \\nEach part of the process is complex enough to warrant multiple books \\nbeing written about it. The goal of this book is to provide you with a broad \\nset of tools to help you build such applications and let you decide which \\ntopics to explore more deeply by following the recommendations outlined \\nin “Additional Resources”, for example. \\nWith that in mind, I hope this book gave you tools to more confidently \\ntackle the majority of the work involved with building ML-powered \\nproducts. We’ve covered every part of the ML product life cycle, starting \\nby translating a product goal to an ML approach, then finding and curating \\ndata and iterating on models, before validating their performance and \\ndeploying them. \\nWhether you’ve read this book cover to cover or dove into specific \\nsections that were most relevant to your work, you should now have the \\nrequired knowledge to start building your own ML-powered applications. \\nIf this book has helped you to build something or if you have any \\nquestions or comments about its content, please reach out to me by \\nemailing mlpoweredapplications@gmail.com. I look forward to hearing \\nfrom you and seeing your ML work.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 272}, page_content='Index \\nA \\n\\uf0b7 A/B testing, A/B Testing and Experimentation \\n\\uf0b7 accuracy \\n\\uf0a7 defined, Judge Performance \\n\\uf0a7 looking beyond, Evaluate Your Model: Look Beyond \\nAccuracy \\n\\uf0b7 adversarial actors \\n\\uf0a7 abuse and dual-use, Abuse Concerns and Dual-Use, Monitor to \\nDetect Abuse \\n\\uf0a7 defeating ML models, Defeating a model \\n\\uf0a7 exploiting models, Exploiting a model \\n\\uf0b7 aggregate metrics, Going beyond aggregate metrics, Inclusive Model \\nPerformance \\n\\uf0b7 algorithms \\n\\uf0a7 being the algorithm, The Simplest Approach: Being the \\nAlgorithm, Be the Algorithm \\n\\uf0a7 classification and regression, Classification and regression \\n\\uf0a7 clustering analysis, Explore and Label Efficiently \\n\\uf0a7 k-means, Clustering \\n\\uf0a7 supervised versus unsupervised, Models, Freshness and \\nDistribution Shift \\n\\uf0b7 anomaly detection, Classification and regression, Monitor to Detect \\nAbuse \\n\\uf0b7 Apache Airflow, Data Processing and DAGs \\n\\uf0b7 applications (see also ML-powered applications)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 273}, page_content='\\uf0a7 building practical, Use ML to Build Practical Applications \\n\\uf0a7 challenges of applying ML to, The Goal of Using Machine \\nLearning Powered Applications \\n\\uf0a7 determining what is possible, Estimate What Is \\nPossible, Monica Rogati: How to Choose and Prioritize ML \\nProjects \\n\\uf0a7 process of building ML–powered, The ML Process \\n\\uf0b7 AUC (area under the curve), ROC Curve \\n\\uf0b7 automated photo tagging systems, Inclusive Model Performance \\n\\uf0b7 AutoRegressive Integrated Moving Average (ARIMA), Our data has \\na temporal aspect \\n\\uf0b7 autoregressive models, Trying to Do It All with ML: An End-to-End \\nFramework \\nB \\n\\uf0b7 bag of words, Text data \\n\\uf0b7 batch predictions, Batch Predictions \\n\\uf0b7 BERT, Text data \\n\\uf0b7 bias \\n\\uf0a7 data bias, Data Bias \\n\\uf0a7 systemic bias, Systemic Bias \\n\\uf0b7 bias-variance trade-off, Judge Performance-Bias variance trade-off \\n\\uf0b7 black-box explainers, Black-Box Explainers, Extracting Local \\nFeature Importance \\n\\uf0b7 BLEU Score, Model Performance \\n\\uf0b7 Bokeh, Be the Algorithm \\n\\uf0b7 bounding boxes, Knowledge extraction from unstructured data \\nC \\n\\uf0b7 caching'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 274}, page_content='\\uf0a7 defined, Caching for ML \\n\\uf0a7 by indexing, Caching by indexing \\n\\uf0a7 inference results, Caching inference results \\n\\uf0b7 calibration curves, Calibration Curve, Using a Model’s Score \\n\\uf0b7 case study (writing assistant tool) \\n\\uf0a7 assessing data quality, ML editor data inspection-ML editor \\ndata inspection \\n\\uf0a7 data ethics, Data Ownership \\n\\uf0a7 extracting recommendations, Extracting Recommendations \\nfrom Models-Extracting Local Feature Importance \\n\\uf0a7 feature generation, ML Editor Features \\n\\uf0a7 framing in ML paradigm, Framing the ML Editor-Middle \\nGround: Learning from Our Experience \\no being the algorithm, The Simplest Approach: Being the \\nAlgorithm \\no end-to-end framework, Trying to Do It All with ML: An \\nEnd-to-End Framework \\no learning from data (classifier approach), Middle Ground: \\nLearning from Our Experience \\n\\uf0a7 initial prototype \\no building, Prototype of an ML Editor-Generating \\nFeatures \\no evaluating, ML Editor Prototype Evaluation-User \\nExperience \\n\\uf0a7 mock-up diagram, Find the Correct ML Approach \\n\\uf0a7 model comparison, Comparing Models-Version 3: \\nUnderstandable Recommendations \\n\\uf0a7 model evaluation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 275}, page_content='o feature importance, Directly from a Classifier \\no top-k method, The k worst performing examples, Top-k \\nmethod for the ML Editor \\n\\uf0a7 model selection, ML Editor model \\n\\uf0a7 model training \\no judging performance, Judge Performance \\no split datasets, ML Editor Data Split \\n\\uf0a7 overview of, Our Case Study: ML–Assisted Writing \\n\\uf0a7 performance metrics \\no freshness and distribution shift, Freshness and \\nDistribution Shift \\no recommendation usefulness, Business Performance \\n\\uf0a7 planning \\no initial editor plan, Initial Plan for an Editor \\no levels of increasing complexity, Measuring Success \\no simple pipeline, Pipeline for the ML Editor \\no simple starting model, Always Start with a Simple \\nModel \\n\\uf0a7 visualizing and exploring data \\no inspecting and labeling using Bokeh, Be the Algorithm \\no summary statistics, Summary statistics for ML editor \\n\\uf0b7 catalog organization, Catalog organization \\n\\uf0b7 CI/CD (continuous improvement/continuous delivery) \\n\\uf0a7 A/B testing and experimentation, A/B Testing and \\nExperimentation \\n\\uf0a7 defined, CI/CD for ML \\n\\uf0a7 shadow mode deployment, CI/CD for ML'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 276}, page_content='\\uf0b7 classification and regression, Classification and regression, Middle \\nGround: Learning from Our Experience, Contrast Data and \\nPredictions \\n\\uf0b7 cleaning data, Parse and Clean Data \\n\\uf0b7 click-through rate (CTR), Business Performance, Calibration \\nCurve, Business Metrics \\n\\uf0b7 client-side deployment, Client-Side Deployment \\n\\uf0b7 clustering analysis, Explore and Label \\nEfficiently, Clustering, Dimensionality Reduction for Errors \\n\\uf0b7 code examples, obtaining and using, Using Code Examples \\n\\uf0b7 code quality estimation, The Simplest Scaffolding \\n\\uf0b7 collaborative filtering recommendation systems, Catalog \\norganization \\n\\uf0b7 comments and questions, How to Contact Us \\n\\uf0b7 Common Crawl, Be Efficient, Start Small \\n\\uf0b7 computer vision, Classification and regression, Chris Harland: \\nShipping Experiments \\n\\uf0b7 confidence scores, Filtering model \\n\\uf0b7 confusion matrices, Contrast Data and Predictions \\n\\uf0b7 considering application context, Considering Context \\n\\uf0b7 content-based recommendation systems, Catalog \\norganization, Feedback Loops \\n\\uf0b7 convolutional neural networks (CNNs), From Product Goal to ML \\nFraming, Each data point is a combination of patterns \\n\\uf0b7 corrupted data, Data Bias \\n\\uf0b7 count vectors, Text data \\n\\uf0b7 counterfactual evaluation, Performance Metrics \\n\\uf0b7 cross-entropy, Model Performance, Optimization Problems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 277}, page_content='\\uf0b7 cross-validation, Validation set \\nD \\n\\uf0b7 data and datasets \\n\\uf0a7 acquiring initial datasets, Data availability, Explore Your First \\nDataset \\n\\uf0a7 assessing quality of, Insights Versus Products-ML editor data \\ninspection \\n\\uf0a7 assuring representative training data, Test set, Test sets \\n\\uf0a7 common sources of errors in, Data Bias \\n\\uf0a7 data leakage, Tabular data, Data leakage-ML Editor Data \\nSplit, Data Leakage \\n\\uf0a7 data mismatch, Start with One Example \\n\\uf0a7 data types, Data types \\n\\uf0a7 determining available data, Models, Data availability \\n\\uf0a7 expert advice on finding, labeling, and leveraging, Robert \\nMunro: How Do You Find, Label, and Leverage Data? \\n\\uf0a7 freshness and distribution shift, Freshness and Distribution \\nShift, Performance Metrics \\n\\uf0a7 imperfect datasets, Data availability \\n\\uf0a7 iterative nature of, Datasets are iterative, Iterate on Datasets \\n\\uf0a7 life cycle management, Model and Data Life Cycle \\nManagement-Pipeline flexibility \\n\\uf0a7 normalizing data, Tabular data \\n\\uf0a7 parsing and cleaning data, Parse and Clean Data \\n\\uf0a7 preprocessing data, ML editor data inspection, Validation \\nset, Data Processing and DAGs \\n\\uf0a7 publicly accessible data, Stand on the Shoulders of Giants, Be \\nEfficient, Start Small'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 278}, page_content='\\uf0a7 quality, quantity, and diversity of data, Data quality, quantity, \\nand diversity \\n\\uf0a7 removing bias from, Systemic Bias \\n\\uf0a7 role in machine learning, Do Data Science \\n\\uf0a7 split datasets, Split Your Dataset-Sample contamination \\n\\uf0a7 Stack Exchange, Initial Plan for an Editor \\n\\uf0a7 structured versus unstructured data, Knowledge extraction \\nfrom unstructured data \\n\\uf0a7 tangential datasets, Open data \\n\\uf0a7 time series data, Classification and regression, Our data has a \\ntemporal aspect \\n\\uf0a7 training speech recognition systems, Data \\n\\uf0a7 validating data flow, Debug Wiring: Visualizing and Testing-\\nTest model outputs \\n\\uf0a7 visualizing and exploring data \\no clustering analysis, Explore and Label \\nEfficiently, Clustering \\no dimensionality reduction, Dimensionality reduction \\no exploratory data analysis, Examining the data \\no finding data trends, Label to Find Data Trends, Data \\nTrends \\no process of labeling data, Be the Algorithm \\no summary statistics, Summary Statistics \\no vectorization, Vectorizing-Image data \\n\\uf0a7 weakly labeled data, Data availability \\n\\uf0b7 data augmentation strategies, Data augmentation \\n\\uf0b7 data ethics \\n\\uf0a7 data bias, Data Bias'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 279}, page_content='\\uf0a7 data ownership, Data Ownership \\n\\uf0a7 questions you should ask, Considerations When Deploying \\nModels \\n\\uf0a7 resources for, Considerations When Deploying Models \\n\\uf0a7 systemic bias, Systemic Bias \\n\\uf0b7 debugging \\n\\uf0a7 generalization, Debug Generalization: Make Your Model \\nUseful-Consider the Task at Hand \\no data augmentation, Data augmentation \\no data leakage, Data Leakage \\no dataset redesign, Dataset redesign \\no overfitting, Overfitting \\no overly complex tasks, Consider the Task at Hand \\no regularization, Regularization \\n\\uf0a7 ML-specific best practices, ML-Specific Best Practices \\n\\uf0a7 resources for, Software Best Practices \\n\\uf0a7 software best practices, Software Best Practices \\n\\uf0a7 testing ML code, Test Your ML Code-Test model outputs \\no data ingestion, Test data ingestion \\no data processing, Test data processing \\no model outputs, Test model outputs \\n\\uf0a7 training process, Debug Training: Make Your Model Learn-\\nOptimization Problems \\no optimization problems, Optimization Problems \\no task difficulty, Task Difficulty \\n\\uf0a7 validating data flow, Debug Wiring: Visualizing and Testing-\\nTest model outputs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 280}, page_content='o cleaning and feature selection, Cleaning and feature \\nselection \\no data formatting, Data formatting \\no data loading, Data loading \\no feature generation, Feature generation \\no initial steps, Start with One Example \\no model output, Model output \\no modular organization, Separate your concerns \\no systematizing visual validation, Systematizing our visual \\nvalidation \\no visualization steps, Visualization steps \\n\\uf0b7 decision threshold, ROC Curve \\n\\uf0b7 decision trees, We want to ignore feature scale \\n\\uf0b7 deployment and monitoring \\n\\uf0a7 deployability, Deployable \\n\\uf0a7 deploying updated models to users, Model and Data Life \\nCycle Management \\n\\uf0a7 deployment options \\no client-side deployment, Client-Side Deployment \\no hybrid approach, Federated Learning: A Hybrid \\nApproach \\no server-side deployment, Server-Side Deployment-Batch \\nPredictions \\n\\uf0a7 deployment planning \\no data concerns, Data Concerns-Systemic Bias \\no expert advice on shipping ML-powered products, Chris \\nHarland: Shipping Experiments'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 281}, page_content='o modeling concerns, Modeling Concerns-Abuse \\nConcerns and Dual-Use \\no questions you should ask, Considerations When \\nDeploying Models \\n\\uf0a7 monitoring and updating models \\no CI/CD for ML, CI/CD for ML-Other Approaches \\no key issues discovered by, Monitoring Saves Lives \\no selecting metrics to monitor, Choose What to Monitor-\\nBusiness Metrics \\n\\uf0a7 safeguards for models \\no creating robust pipelines, Engineer for Performance-\\nData Processing and DAGs \\no expert advice on deploying ML models, Chris Moody: \\nEmpowering Data Scientists to Deploy Models \\no incorporating user feedback, Ask for Feedback \\no model failure fallbacks, Model Failure Fallbacks-\\nFiltering model \\no verifying data quality, Input and Output Checks-Model \\noutputs \\n\\uf0a7 shadow mode deployment, CI/CD for ML \\n\\uf0b7 detecting attacks, Monitor to Detect Abuse \\n\\uf0b7 dimensionality reduction, Dimensionality reduction, Dimensionality \\nReduction for Errors \\n\\uf0b7 directed acyclic graphs (DAGs), Data Processing and DAGs-Data \\nProcessing and DAGs \\n\\uf0b7 discounted cumulative gain (DCG), Modeling Results \\n\\uf0b7 distribution shift, Freshness and Distribution Shift-Freshness and \\nDistribution Shift, Performance Metrics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 282}, page_content='\\uf0b7 domain expertise, Leverage Domain Expertise \\n\\uf0b7 dropout, Regularization \\n\\uf0b7 dual-use technologies, Abuse Concerns and Dual-Use \\nE \\n\\uf0b7 Elbow method, Clustering \\n\\uf0b7 end-to-end frameworks \\n\\uf0a7 challenges of, Trying to Do It All with ML: An End-to-End \\nFramework \\n\\uf0a7 debugging (see debugging) \\n\\uf0a7 inference pipeline, The Simplest Scaffolding \\n\\uf0a7 testing workflow, Test Your Workflow-Finding the impact \\nbottleneck \\n\\uf0b7 error analysis \\n\\uf0a7 detecting errors, Model Failure Fallbacks \\n\\uf0a7 dimensionality reduction, Dimensionality Reduction for Errors \\n\\uf0a7 modeling versus product domains, Finding the impact \\nbottleneck \\n\\uf0b7 experimentation, A/B Testing and Experimentation \\n\\uf0b7 explainability, Understandable \\n\\uf0b7 exploratory data analysis (EDA), Examining the data \\nF \\n\\uf0b7 f1 score, defined, Judge Performance \\n\\uf0b7 fairness constraints, Systemic Bias \\n\\uf0b7 false positive rate (FPR), ROC Curve \\n\\uf0b7 fastText, Text data \\n\\uf0b7 feasibility, determining, Estimate What Is Possible, Monica Rogati: \\nHow to Choose and Prioritize ML Projects-Monica Rogati: How to \\nChoose and Prioritize ML Projects, Measuring Success'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 283}, page_content='\\uf0b7 feature crosses, Feature crosses \\n\\uf0b7 feature drift, Performance Metrics \\n\\uf0b7 feature generation \\n\\uf0a7 building features out of patterns, Build Features Out of \\nPatterns-Giving your model the answer \\n\\uf0a7 defined, Classification and regression \\n\\uf0a7 example of, Generating Features \\n\\uf0a7 inspecting feature values, Feature generation \\n\\uf0a7 visualizing hard examples, Dimensionality Reduction for \\nErrors \\n\\uf0b7 feature importance \\n\\uf0a7 evaluating, The Simplest Approach: Being the \\nAlgorithm, Evaluate Feature Importance \\n\\uf0a7 extracting, Extracting Global Feature Importance \\n\\uf0b7 feature scale, ignoring, We want to ignore feature scale \\n\\uf0b7 feature selection, Classification and regression, Cleaning and feature \\nselection \\n\\uf0b7 feature statistics, Using feature statistics \\n\\uf0b7 federated learning, Federated Learning: A Hybrid Approach \\n\\uf0b7 feedback loops, Feedback Loops \\n\\uf0b7 feedback, requesting, Ask for Feedback \\n\\uf0b7 filtering models, Filtering model \\n\\uf0b7 forecasting, Classification and regression, Insights Versus Products \\n\\uf0b7 fraud detection systems \\n\\uf0a7 classifying individual examples, Classification and regression \\n\\uf0a7 defending against attacks, Defeating a model, Monitor to \\nDetect Abuse \\n\\uf0a7 illustration of, Performance Metrics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 284}, page_content='\\uf0a7 labeled or unlabeled data for, Models \\n\\uf0a7 seasonality aspect of, Insights Versus Products \\n\\uf0b7 freshness, Freshness and Distribution Shift-Freshness and \\nDistribution Shift \\nG \\n\\uf0b7 generative models, Generative models \\n\\uf0b7 global feature importance, Extracting Global Feature Importance \\n\\uf0b7 gradient descent, We want to ignore feature scale, Optimization \\nProblems \\n\\uf0b7 gradient-boosted decision trees, We want to ignore feature scale \\n\\uf0b7 GroupShuffleSplit class, ML Editor Data Split \\n\\uf0b7 guardrail metrics, Business Performance \\nH \\n\\uf0b7 Harland, Chris, Chris Harland: Shipping Experiments \\n\\uf0b7 heuristics \\n\\uf0a7 benefits of building, Measuring Success \\n\\uf0a7 examples of great, The Simplest Scaffolding \\n\\uf0a7 identifying good, Leverage Domain Expertise \\nI \\n\\uf0b7 image data \\n\\uf0a7 automated photo tagging systems, Inclusive Model \\nPerformance \\n\\uf0a7 knowledge extraction, Knowledge extraction from \\nunstructured data \\n\\uf0a7 selecting models based on, Each data point is a combination of \\npatterns \\n\\uf0a7 vectorizing, Image data-Image data \\n\\uf0b7 ImageNet, Image data, Image data \\n\\uf0b7 impact bottlenecks'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 285}, page_content='\\uf0a7 benefits of prototypes for identifying, Build Your First End-to-\\nEnd Pipeline \\n\\uf0a7 defined, Monica Rogati: How to Choose and Prioritize ML \\nProjects \\n\\uf0a7 finding, Finding the impact bottleneck \\n\\uf0b7 implementation \\n\\uf0a7 challenges of end-to-end models, Trying to Do It All with ML: \\nAn End-to-End Framework \\n\\uf0a7 quick to implement models, Quick to implement \\n\\uf0b7 implicit signals, measuring, Ask for Feedback \\n\\uf0b7 inclusive model performance, Inclusive Model Performance \\n\\uf0b7 indexing, Caching by indexing \\n\\uf0b7 inference pipeline, The Simplest Scaffolding \\n\\uf0b7 informing users, Considering Context \\n\\uf0b7 input checks, Check inputs \\n\\uf0b7 input distribution, Performance Metrics \\n\\uf0b7 Insight Data Science, The Simplest Scaffolding \\n\\uf0b7 interpretability, Understandable \\nJ \\n\\uf0b7 Jaccard index, Other Models \\nK \\n\\uf0b7 k-means algorithm, Clustering \\n\\uf0b7 Kaggle, Be Efficient, Start Small \\n\\uf0b7 Keras, Image data \\n\\uf0b7 KISS (Keep It Stupid Simple), Software Best Practices \\n\\uf0b7 knowledge extraction \\n\\uf0a7 from images, Knowledge extraction from unstructured data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 286}, page_content='\\uf0a7 structured versus unstructured data, Knowledge extraction \\nfrom unstructured data \\nL \\n\\uf0b7 L1 and L2 regularization, Regularization \\n\\uf0b7 labels \\n\\uf0a7 applying efficiently, Explore and Label Efficiently \\n\\uf0a7 labeled versus unlabeled data, Models \\n\\uf0a7 levels of data availability based on, Data availability \\n\\uf0a7 poorly formatted, Data formatting \\n\\uf0a7 process of labeling data, Be the Algorithm \\n\\uf0b7 lead scoring, Batch Predictions \\n\\uf0b7 least recently used (LRU) cache, Caching inference results \\n\\uf0b7 life cycle management, Model and Data Life Cycle Management-\\nPipeline flexibility \\n\\uf0b7 LIME, Black-Box Explainers, Extracting Local Feature Importance \\n\\uf0b7 linear regression, Our predicted variable is a linear combination of \\npredictors \\n\\uf0b7 local feature importance, Extracting Local Feature Importance \\n\\uf0b7 logistic regression, Our predicted variable is a linear combination of \\npredictors \\n\\uf0b7 loss functions, Optimization Problems \\n\\uf0b7 Luigi, Data Processing and DAGs \\nM \\n\\uf0b7 machine learning (ML) \\n\\uf0a7 additional resources, Additional Resources \\n\\uf0a7 approach to learning, A technical, practical case study \\n\\uf0a7 best practices, ML-Specific Best Practices'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 287}, page_content='\\uf0a7 challenges of applying to applications, The Goal of Using \\nMachine Learning Powered Applications \\n\\uf0a7 data ethics and, Considerations When Deploying Models \\n\\uf0a7 defined, Practical ML \\n\\uf0a7 identifying the right ML approach, Find the Correct ML \\nApproach-Conclusion, Measuring Success \\n\\uf0a7 predicting success of applications using, Estimate What Is \\nPossible \\n\\uf0a7 prerequisites to learning, Prerequisites \\n\\uf0a7 process of building ML–powered applications, The entire \\nprocess of ML, The ML Process \\n\\uf0a7 products powered by, The Goal of Using Machine Learning \\nPowered Applications, Practical ML \\n\\uf0a7 versus traditional programming, From Product Goal to ML \\nFraming, Iterate on Datasets \\n\\uf0b7 mapping inputs to outputs, Data types \\n\\uf0b7 measurement errors, Data Bias \\n\\uf0b7 metrics (see performance metrics) \\n\\uf0b7 metrics for binary classification, ROC Curve \\n\\uf0b7 Minimum Viable Product (MVP), Build a Working Pipeline \\n\\uf0b7 ML writing assistants, Chris Harland: Shipping Experiments \\n\\uf0b7 ML-powered applications \\n\\uf0a7 building initial prototype, Build a Working Pipeline-\\nConclusion \\no first end-to-end pipeline, Build Your First End-to-End \\nPipeline-Conclusion \\no initial dataset, Acquire an Initial Dataset-Conclusion \\n\\uf0a7 deployment and monitoring, Deploy and Monitor-Conclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 288}, page_content='o building model safeguards, Build Safeguards for \\nModels-Conclusion \\no deployment options, Choose Your Deployment Option-\\nConclusion \\no deployment planning, Considerations When Deploying \\nModels-Conclusion \\no monitoring and updating models, Monitor and Update \\nModels-Conclusion \\n\\uf0a7 determining what is possible, Estimate What Is \\nPossible, Monica Rogati: How to Choose and Prioritize ML \\nProjects-Monica Rogati: How to Choose and Prioritize ML \\nProjects \\n\\uf0a7 identifying the right ML approach, Find the Correct ML \\nApproach-Conclusion \\no creating a plan, Create a Plan-Conclusion \\no from product goal to ML framing, From Product Goal to \\nML Framing-Conclusion \\n\\uf0a7 iterating on models, The Simplest Scaffolding, Iterate on \\nModels-Conclusion \\no core parts of process, The Simplest Scaffolding \\no debugging, Debug Your ML Problems-Conclusion \\no training and evaluation, Train and Evaluate Your Model-\\nConclusion \\no using classifiers for writing recommendations, Using \\nClassifiers for Writing Recommendations-Conclusion \\n\\uf0b7 ML–powered applications \\n\\uf0a7 build challenges, The Goal of Using Machine Learning \\nPowered Applications'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 289}, page_content='\\uf0a7 build process overview, The entire process of ML-The ML \\nProcess \\n\\uf0b7 ML–powered editors, Our Case Study: ML–Assisted \\nWriting (see also case study) \\n\\uf0b7 model disclaimers, Considering Context \\n\\uf0b7 model metrics, Model Performance \\n\\uf0b7 models \\n\\uf0a7 debugging (see debugging) \\n\\uf0a7 evaluating \\no calibration curves, Calibration Curve \\no confusion matrices, Confusion Matrix \\no contrasting data and predictions, Contrast Data and \\nPredictions \\no error analysis, Dimensionality Reduction for Errors \\no feature importance, Evaluate Feature Importance-Black-\\nBox Explainers \\no looking beyond accuracy, Evaluate Your Model: Look \\nBeyond Accuracy \\no ROC curves, ROC Curve \\no top-k method, The Top-k Method-Top-k method for the \\nML Editor \\n\\uf0a7 identifying failure modes and resolutions, Finding the impact \\nbottleneck, Test set, Engineer Around Failures-Filtering model \\n\\uf0a7 key to building successful, Going beyond aggregate metrics \\n\\uf0a7 levels of increasing complexity, Measuring Success \\n\\uf0a7 performance metrics, Model Performance-Model Performance \\no generating additional model metrics, Model Performance \\no making modeling tasks easier, Model Performance'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 290}, page_content='o model precision, Model Performance \\no reasonable expectations for, Model Performance \\n\\uf0a7 selecting, From Product Goal to ML Framing-Estimate What \\nIs Possible, The Simplest Appropriate Model-Going beyond \\naggregate metrics \\no based on data patterns, From Patterns to Models \\no benefits of prototypes for, Modeling Results \\no data driven assumptions, Be the Algorithm \\no determining deployability, Deployable \\no estimating what is possible, Estimate What Is Possible \\no flowchart for, Each data point is a combination of \\npatterns \\no quick to implement, Quick to implement \\no scoring models based on simplicity, Deployable \\no understandable models, Understandable \\n\\uf0a7 taxonomy of \\no autoregressive, Trying to Do It All with ML: An End-to-\\nEnd Framework \\no catalog organization, Catalog organization \\no classification and regression, Classification and \\nregression \\no end-to-end, Trying to Do It All with ML: An End-to-End \\nFramework \\no generative, Generative models \\no knowledge extraction from unstructured \\ndata, Knowledge extraction from unstructured data \\no labeled versus unlabeled data, Models, Data availability \\no list of approaches, Models'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 291}, page_content='o sequence-to-sequence, Trying to Do It All with ML: An \\nEnd-to-End Framework \\no structured versus unstructured data, Knowledge \\nextraction from unstructured data \\no supervised versus unsupervised, Models, Freshness and \\nDistribution Shift \\n\\uf0a7 training \\no data leakage, Data leakage \\no eliminating undesirable bias in, Modeling Concerns-\\nAbuse Concerns and Dual-Use \\no judging performance, Judge Performance-Going beyond \\naggregate metrics \\no relative proportions of data, Relative proportions \\no split datasets, Split Your Dataset \\no test datasets, Test set, Test sets \\no validation datasets, Validation set \\n\\uf0b7 Moody, Chris, Chris Moody: Empowering Data Scientists to Deploy \\nModels \\n\\uf0b7 movie recommendations, Catalog organization \\n\\uf0b7 multi-layer neural networks, Our predicted variable is a linear \\ncombination of predictors \\n\\uf0b7 Munro, Robert, Robert Munro: How Do You Find, Label, and \\nLeverage Data? \\nN \\n\\uf0b7 naive Bayes classifiers, Our predicted variable is a linear \\ncombination of predictors \\n\\uf0b7 natural language processing, Text data, Robert Munro: How Do You \\nFind, Label, and Leverage Data?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 292}, page_content='\\uf0b7 neural networks, Optimization Problems \\n\\uf0b7 neural style transfer, Generative models \\n\\uf0b7 nonlinear models, Our predicted variable is a linear combination of \\npredictors \\nO \\n\\uf0b7 object detection, Knowledge extraction from unstructured \\ndata, Other Models \\n\\uf0b7 offline metrics, Model Performance \\n\\uf0b7 one-hot encoding, Tabular data \\n\\uf0b7 open source code, Stand on the Shoulders of Giants, Open source \\ncode \\n\\uf0b7 optimization problems, Optimization Problems \\n\\uf0b7 outliers, Data quality, quantity, and diversity \\n\\uf0b7 output checks, Model outputs \\n\\uf0b7 overfitting, Bias variance trade-off, Overfitting \\nP \\n\\uf0b7 parsing and cleaning data, Parse and Clean Data \\n\\uf0b7 patterns \\n\\uf0a7 building features out of, Build Features Out of Patterns-Giving \\nyour model the answer \\n\\uf0a7 selecting models based on, From Patterns to Models \\n\\uf0b7 PCA (Principal Component Analysis), Dimensionality reduction \\n\\uf0b7 performance metrics \\n\\uf0a7 business performance, Business Performance, Business \\nMetrics \\n\\uf0a7 deceptive nature of, Going beyond aggregate metrics \\n\\uf0a7 freshness and distribution shift, Freshness and Distribution \\nShift-Freshness and Distribution Shift, Performance Metrics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 293}, page_content='\\uf0a7 measuring success, Measuring Success \\n\\uf0a7 model performance, Model Performance-Model Performance \\no degradation of performance, Inclusive Model \\nPerformance \\no generating additional model metrics, Model Performance \\no making modeling tasks easier, Model Performance \\no model precision, Model Performance \\no reasonable expectations for, Model Performance \\n\\uf0a7 speed, Speed \\n\\uf0b7 performance, engineering for \\n\\uf0a7 data processing and DAGs, Data Processing and DAGs-Data \\nProcessing and DAGs \\n\\uf0a7 model and data life cycle management, Model and Data Life \\nCycle Management-Pipeline flexibility \\n\\uf0a7 scaling to multiple users, Scale to Multiple Users-Caching by \\nindexing \\n\\uf0b7 pipelines \\n\\uf0a7 creating robust, Engineer for Performance-Data Processing \\nand DAGs \\n\\uf0a7 debugging (see debugging) \\n\\uf0a7 flexibility of, Pipeline flexibility \\n\\uf0a7 types of, The Simplest Scaffolding \\n\\uf0b7 planning \\n\\uf0a7 estimating scope and challenges, Estimate Scope and \\nChallenges \\n\\uf0a7 exploratory data analysis (EDA), Examining the data \\n\\uf0a7 leveraging domain expertise, Leverage Domain Expertise'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 294}, page_content='\\uf0a7 simple starting model, To Make Regular Progress: Start \\nSimple-Pipeline for the ML Editor \\n\\uf0a7 understanding and reproducing existing results, Stand on the \\nShoulders of Giants-Bring both together \\n\\uf0b7 precision, defined, Judge Performance \\n\\uf0b7 preprocessing data, ML editor data inspection, Validation set, Data \\nProcessing and DAGs \\n\\uf0b7 private information, protecting, Exploiting a model \\n\\uf0b7 product goals \\n\\uf0a7 estimating scope and challenges, Estimate Scope and \\nChallenges \\n\\uf0a7 evaluating ML feasibility, Estimate What Is \\nPossible, Measuring Success \\n\\uf0a7 framing in ML paradigm, Framing the ML Editor-Middle \\nGround: Learning from Our Experience \\no being the algorithm, The Simplest Approach: Being the \\nAlgorithm, Be the Algorithm \\no end-to-end framework, Trying to Do It All with ML: An \\nEnd-to-End Framework \\no expert advice on, Monica Rogati: How to Choose and \\nPrioritize ML Projects \\no learning from data (classifier approach), Middle Ground: \\nLearning from Our Experience \\no overview of, Conclusion \\no steps for, Estimate What Is Possible \\n\\uf0a7 identifying failure modes and resolutions, Finding the impact \\nbottleneck, Test set \\n\\uf0a7 misalignment with model metrics, Create a Plan'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 295}, page_content='\\uf0b7 product metrics, Business Performance \\n\\uf0b7 prototypes \\n\\uf0a7 benefits of building, Modeling Results \\n\\uf0a7 building initial, Build Your First End-to-End \\nPipeline, Prototype of an ML Editor-Generating Features \\n\\uf0a7 evaluating, ML Editor Prototype Evaluation \\n\\uf0a7 inference pipeline, The Simplest Scaffolding \\n\\uf0a7 selecting heuristics, The Simplest Scaffolding \\n\\uf0a7 testing workflow, Test Your Workflow-Finding the impact \\nbottleneck \\n\\uf0b7 publicly accessible data, Open data \\nQ \\n\\uf0b7 questions and comments, How to Contact Us \\nR \\n\\uf0b7 random forests, We want to ignore feature scale, Regularization \\n\\uf0b7 recall, defined, Judge Performance \\n\\uf0b7 Recurrent Neural Networks (RNN), Our data has a temporal aspect \\n\\uf0b7 refresh rate, Monitoring to Inform Refresh Rate \\n\\uf0b7 regression and classification, Classification and regression, Middle \\nGround: Learning from Our Experience, Contrast Data and \\nPredictions \\n\\uf0b7 regularization, Regularization \\n\\uf0b7 representation, Data Bias \\n\\uf0b7 reproducibility, Reproducibility \\n\\uf0b7 resilience, Resilience \\n\\uf0b7 RGB images, Image data \\n\\uf0b7 ROC (receiver operating characteristic) curves, ROC Curve'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 296}, page_content='\\uf0b7 Rogati, Monica, Monica Rogati: How to Choose and Prioritize ML \\nProjects, Be the Algorithm \\nS \\n\\uf0b7 sample contamination, Sample contamination \\n\\uf0b7 seasonality, Build Features Out of Patterns \\n\\uf0b7 segmentation masks, Knowledge extraction from unstructured \\ndata, Data formatting \\n\\uf0b7 self-reinforcing feedback loops, Feedback Loops \\n\\uf0b7 sentence simplification model, Other Models \\n\\uf0b7 sentiment classification, Classification and regression, Summary \\nStatistics \\n\\uf0b7 sequence-to-sequence models, Trying to Do It All with ML: An \\nEnd-to-End Framework \\n\\uf0b7 server-side deployment, Server-Side Deployment-Batch Predictions \\n\\uf0b7 shadow mode deployment, CI/CD for ML \\n\\uf0b7 SHAP, Black-Box Explainers \\n\\uf0b7 Silhouette plots, Clustering \\n\\uf0b7 simple models \\n\\uf0a7 characteristics of, Simple Models \\n\\uf0a7 scoring models based on simplicity, Deployable \\n\\uf0b7 Smart Reply feature, Filtering model \\n\\uf0b7 software best practices, Software Best Practices \\n\\uf0b7 spaCy, Text data \\n\\uf0b7 spam filters, Classification and regression \\n\\uf0b7 speech recognition systems, Data, Each data point is a combination \\nof patterns, Confusion Matrix \\n\\uf0b7 speed, Speed \\n\\uf0b7 Stack Exchange, Initial Plan for an Editor'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 297}, page_content='\\uf0b7 Stack Overflow, Framing the ML Editor \\n\\uf0b7 standard score, Tabular data \\n\\uf0b7 streaming applications, Streaming Application or API \\n\\uf0b7 structured data, Knowledge extraction from unstructured data \\n\\uf0b7 subtitle generation, Generative models \\n\\uf0b7 summarization tasks, Generative models \\n\\uf0b7 summary statistics, Summary Statistics, Performance Metrics \\n\\uf0b7 supervised algorithms, Models, Freshness and Distribution Shift \\n\\uf0b7 systemic bias, Systemic Bias \\nT \\n\\uf0b7 t-SNE, Dimensionality reduction \\n\\uf0b7 tabular data, vectorizing, Tabular data-Tabular data \\n\\uf0b7 tangential datasets, Open data \\n\\uf0b7 task difficulty \\n\\uf0a7 data quality, quantity, and diversity, Data quality, quantity, \\nand diversity \\n\\uf0a7 data representation, Data representation, Test sets \\n\\uf0a7 model capacity, Model capacity \\n\\uf0a7 overview of, Task Difficulty \\n\\uf0b7 temporal data leakage, Temporal data leakage \\n\\uf0b7 Tensorboard, Optimization Problems \\n\\uf0b7 tensors, Image data \\n\\uf0b7 test sets, Test set, Test sets \\n\\uf0b7 text data \\n\\uf0a7 classifying, Each data point is a combination of patterns \\n\\uf0a7 properly formatted, Data formatting \\n\\uf0a7 tokenizing, Tokenizing Text \\n\\uf0a7 vectorizing, Text data-Text data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 298}, page_content='\\uf0b7 TF-IDF (Term Frequency-Inverse Document Frequency), Text data \\n\\uf0b7 time series data, Classification and regression, Our data has a \\ntemporal aspect, Temporal data leakage \\n\\uf0b7 tokenization, Tokenizing Text \\n\\uf0b7 top-k method \\n\\uf0a7 identifying challenging data points with, The Top-k Method \\n\\uf0a7 implementation tips, Top-k implementation tips \\n\\uf0a7 k best performing examples, The k best performing examples \\n\\uf0a7 k most uncertain examples, The k most uncertain examples \\n\\uf0a7 k worst performing examples, The k worst performing \\nexamples \\n\\uf0b7 training loss, Data Leakage \\n\\uf0b7 transfer learning, Image data \\n\\uf0b7 translation invariant filters, Each data point is a combination of \\npatterns \\n\\uf0b7 translation tasks, Generative models, Trying to Do It All with ML: \\nAn End-to-End Framework \\n\\uf0b7 triggering models, Filtering model \\n\\uf0b7 true positive rate (TPR), ROC Curve \\nU \\n\\uf0b7 ULMFit, Text data \\n\\uf0b7 UMAP, Dimensionality reduction \\n\\uf0b7 underfitting, Bias variance trade-off, Overfitting \\n\\uf0b7 understandable models, Understandable \\n\\uf0b7 Unix time, Raw datetime \\n\\uf0b7 unstructured data, Knowledge extraction from unstructured data \\n\\uf0b7 unsupervised algorithms, Models \\n\\uf0b7 usage metrics, Model Performance'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 299}, page_content='\\uf0b7 user experience, evaluating, User Experience, User Experience, Ask \\nfor Feedback \\nV \\n\\uf0b7 validation loss, Data Leakage \\n\\uf0b7 validation sets, Validation set \\n\\uf0b7 vectorization \\n\\uf0a7 data leakage and, Tabular data \\n\\uf0a7 defined, Vectorizing \\n\\uf0a7 image data, Image data-Image data \\n\\uf0a7 tabular data, Tabular data-Tabular data \\n\\uf0a7 text data, Text data-Text data \\n\\uf0b7 VGG, Image data \\n\\uf0b7 visualization libraries, Be the Algorithm \\nW \\n\\uf0b7 weak labels, Always Start with a Simple Model \\n\\uf0b7 weakly labeled data, Data availability \\n\\uf0b7 weakly supervised algorithms, Models \\n\\uf0b7 weight histograms, Optimization Problems \\n\\uf0b7 WikiText, Image data \\n\\uf0b7 window size, Text data \\n\\uf0b7 word embeddings, pretrained, Text data \\n\\uf0b7 Word2Vec, Text data \\n\\uf0b7 workflow, testing, Test Your Workflow-Finding the impact \\nbottleneck \\n\\uf0b7 writing assistant tool, Our Case Study: ML–Assisted Writing, Chris \\nHarland: Shipping Experiments (see also case study) \\n\\uf0b7 writing recommendation systems \\n\\uf0a7 best way to iterate, Other Models'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 300}, page_content=\"\\uf0a7 comparing models, Comparing Models-Version 3: \\nUnderstandable Recommendations \\no heuristic-based version, Version 1: The Report Card \\no key metrics, Comparing Models \\no leveraging features explainable to users, Version 3: \\nUnderstandable Recommendations \\no simple model, Version 2: More Powerful, More Unclear \\n\\uf0a7 extracting recommendations from models, Extracting \\nRecommendations from Models-Extracting Local Feature \\nImportance \\no using a model's score, Using a Model’s Score \\no using feature statistics, Using feature statistics \\no using global feature importance, Extracting Global \\nFeature Importance \\no using local feature importance, Extracting Local Feature \\nImportance \\n\\uf0a7 generating editing recommendations, Generating Editing \\nRecommendations-Generating Editing Recommendations \\n\\uf0a7 judging performance of, Model Performance \\n\\uf0a7 monitoring, Business Metrics \\nX \\n\\uf0b7 XGBoost, We want to ignore feature scale\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 301}, page_content='About the Author \\nEmmanuel Ameisen has been building ML-powered products for years. \\nHe is currently working on ML engineering at Stripe. Previously, he was \\nhead of AI at Insight Data Science, where he led more than 150 ML \\nprojects. Before, Emmanuel was a data scientist at Zipcar, where he \\nworked on on-demand prediction and building frameworks and services to \\nhelp deploy ML models in production. He has a background at the \\nintersection of ML and business, with an M.S. in AI from Université Paris-\\nSud, an M.S. in Engineering from CentraleSupélec, and an M.S. in \\nManagement from ESCP Europe.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2020-01-22T18:20:50+03:00', 'source': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Building_Machine_Learning_Powered_Applications_Going_From_Idea_to.pdf', 'total_pages': 303, 'format': 'PDF 1.5', 'title': '', 'author': 'artemenovs', 'subject': '', 'keywords': '', 'moddate': '2020-01-22T18:20:50+03:00', 'trapped': '', 'modDate': \"D:20200122182050+03'00'\", 'creationDate': \"D:20200122182050+03'00'\", 'page': 302}, page_content='Colophon \\nThe animal on the cover of Building Machine Learning Powered \\nApplications is the poplar admiral butterfly (Limenitis populi). This \\nbutterfly is a large but increasingly rare butterfly of North Africa, northern \\nAsia, the Middle East, and Europe. \\nPoplar admirals have a three-inch wingspan. Its wings are white-spotted \\ndark brown above with margins of slate and orange, and undersides \\nmarked in orange. \\nIn late August, butterflies lay their eggs on the leaves of poplar trees \\n(mostly trembling aspen, Populus tremula), as the caterpillars eat only \\nthese leaves. Caterpillars have horn-like appendages, and molt through \\nshades of green and brown as they grow, for camouflage. They begin \\ngrowing in late summer, but in the fall start spinning silk for a protective \\ncocoon where they overwinter. In spring, they re-emerge as the first poplar \\nleaves bud and immediately begin feeding, to complete their final \\ntransition: in late May, they attach themselves with silk to a leaf and grow \\na hard surface skin in which they pupate, emerging as butterflies in June \\nand July. \\nUnlike most butterflies, poplar admirals do not visit flowers for nectar, \\ninstead using their probing proboscis to draw food and nutrients from \\ncarrion, animal droppings, tree sap, and the salts in mud (and at times the \\nsalts in human sweat). They seem attracted to smells of decomposition. \\nPoplar admiral numbers are declining as deciduous forests are cleared for \\ndevelopment, though its current IUCN Red List is “Least Concern.” Many \\nof the animals on O’Reilly covers are endangered; all of them are \\nimportant to the world. \\nThe color illustration on the cover is by Jose Marzan Jr., based on a black-\\nand-white engraving from the Meyers Kleines Lexicon. The cover fonts are \\nGilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; \\nthe heading font is Adobe Myriad Condensed; and the code font is Dalton \\nMaag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 0}, page_content='PySpark \\nRecipes\\nA Problem-Solution Approach  \\nwith PySpark2\\n—\\nRaju Kumar Mishra\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 1}, page_content='PySpark Recipes\\nA Problem-Solution Approach  \\nwith PySpark2\\nRaju Kumar Mishra\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 2}, page_content='PySpark Recipes\\nRaju Kumar Mishra\\t\\t\\n\\t\\n\\t\\nBangalore, Karnataka, India\\t \\t\\n\\t\\nISBN-13 (pbk): 978-1-4842-3140-1\\t\\n\\t\\nISBN-13 (electronic): 978-1-4842-3141-8\\nhttps://doi.org/10.1007/978-1-4842-3141-8\\nLibrary of Congress Control Number: 2017962438\\nCopyright © 2018 by Raju Kumar Mishra\\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole \\nor part of the material is concerned, specifically the rights of translation, reprinting, reuse of \\nillustrations, recitation, broadcasting, reproduction on microfilms or in any other physical \\nway, and transmission or information storage and retrieval, electronic adaptation, computer \\nsoftware, or by similar or dissimilar methodology now known or hereafter developed.\\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark \\nsymbol with every occurrence of a trademarked name, logo, or image, we use the names, logos, \\nand images only in an editorial fashion and to the benefit of the trademark owner, with no \\nintention of infringement of the trademark.\\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if \\nthey are not identified as such, is not to be taken as an expression of opinion as to whether or not \\nthey are subject to proprietary rights.\\nWhile the advice and information in this book are believed to be true and accurate at the \\ndate of publication, neither the authors nor the editors nor the publisher can accept any legal \\nresponsibility for any errors or omissions that may be made. The publisher makes no warranty, \\nexpress or implied, with respect to the material contained herein.\\nCover image by Freepik (www.freepik.com)\\nManaging Director: Welmoed Spahr\\nEditorial Director: Todd Green\\nAcquisitions Editor: Celestin Suresh John\\nDevelopment Editor: Laura Berendson\\nTechnical Reviewer: Sundar Rajan\\nCoordinating Editor: Sanchita Mandal\\nCopy Editor: Sharon Wilkey\\nCompositor: SPi Global\\nIndexer: SPi Global\\nArtist: SPi Global \\nDistributed to the book trade worldwide by Springer Science + Business Media New York, \\n233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, \\ne-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, \\nLLC is a California LLC, and the sole member (owner) is Springer Science + Business Media \\nFinance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\\nFor information on translations, please e-mail rights@apress.com, or visit  \\nwww.apress.com/rights-permissions. \\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook \\nversions and licenses are also available for most titles. For more information, reference our \\nPrint and eBook Bulk Sales web page at www.apress.com/bulk-sales.\\nAny source code or other supplementary material referenced by the author in this book is \\navailable to readers on GitHub via the book’s product page, located at www.apress.com.  \\nFor more detailed information, please visit www.apress.com/source-code.\\nPrinted on acid-free paper\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 3}, page_content='To the Almighty, who guides me in every aspect of my life.  \\nAnd to my mother, Smt. Savitri Mishra, and  \\nmy lovely wife, Smt. Smita Rani Pathak.\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 4}, page_content='v\\nContents\\nAbout the Author\\x08........................................................................... xvii\\nAbout the Technical Reviewer\\x08........................................................ xix \\nAcknowledgments\\x08.......................................................................... xxi \\nIntroduction \\x08................................................................................. xxiii \\n■\\n■\\x07Chapter 1: The Era of Big Data, Hadoop, and Other Big Data \\nProcessing Frameworks\\x08................................................................. 1\\nBig Data\\x08................................................................................................... 2\\nVolume\\x08......................................................................................................................2\\nVelocity\\x08.....................................................................................................................3\\nVariety\\x08.......................................................................................................................3\\nVeracity\\x08.....................................................................................................................3\\nHadoop\\x08.................................................................................................... 3\\nHDFS\\x08.........................................................................................................................4\\nMapReduce\\x08...............................................................................................................5\\nApache Hive\\x08............................................................................................ 6\\nApache Pig\\x08.............................................................................................. 7\\nApache Kafka\\x08.......................................................................................... 8\\nProducer\\x08...................................................................................................................8\\nBroker\\x08.......................................................................................................................8\\nConsumer\\x08.................................................................................................................8\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 5}, page_content='■ Contents\\nvi\\nApache Spark\\x08.......................................................................................... 9\\nCluster Managers\\x08.................................................................................. 10\\nStandalone Cluster Manager\\x08..................................................................................11\\nApache Mesos Cluster Manager\\x08.............................................................................11\\nYARN Cluster Manager\\x08............................................................................................11\\nPostgreSQL\\x08............................................................................................ 12\\nHBase\\x08.................................................................................................... 12\\n■\\n■Chapter 2: Installation\\x08.................................................................. 15\\nRecipe 2-1. Install Hadoop on a Single Machine\\x08................................... 16\\nProblem\\x08..................................................................................................................16\\nSolution\\x08..................................................................................................................16\\nHow It Works\\x08...........................................................................................................16\\nRecipe 2-2. Install Spark on a Single Machine\\x08..................................... 23\\nProblem\\x08..................................................................................................................23\\nSolution\\x08..................................................................................................................23\\nHow It Works\\x08...........................................................................................................23\\nRecipe 2-3. Use the PySpark Shell\\x08........................................................ 25\\nProblem\\x08..................................................................................................................25\\nSolution\\x08..................................................................................................................25\\nHow It Works\\x08...........................................................................................................25\\nRecipe 2-4. Install Hive on a Single Machine\\x08........................................ 27\\nProblem\\x08..................................................................................................................27\\nSolution\\x08..................................................................................................................27\\nHow It Works\\x08...........................................................................................................27\\nRecipe 2-5. Install PostgreSQL\\x08.............................................................. 30\\nProblem\\x08..................................................................................................................30\\nSolution\\x08..................................................................................................................30\\nHow It Works\\x08...........................................................................................................30\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 6}, page_content='■ Contents\\nvii\\nRecipe 2-6. Configure the Hive Metastore on PostgreSQL\\x08.................... 31\\nProblem\\x08..................................................................................................................31\\nSolution\\x08..................................................................................................................31\\nHow It Works\\x08...........................................................................................................32\\nRecipe 2-7. Connect PySpark to Hive\\x08.................................................... 37\\nProblem\\x08..................................................................................................................37\\nSolution\\x08..................................................................................................................37\\nHow It Works\\x08...........................................................................................................37\\nRecipe 2-8. Install Apache Mesos\\x08......................................................... 38\\nProblem\\x08..................................................................................................................38\\nSolution\\x08..................................................................................................................38\\nHow It Works\\x08...........................................................................................................38\\nRecipe 2-9. Install HBase\\x08...................................................................... 42\\nProblem\\x08..................................................................................................................42\\nSolution\\x08..................................................................................................................42\\nHow It Works\\x08...........................................................................................................42\\n■\\n■Chapter 3: Introduction to Python and NumPy\\x08............................. 45\\nRecipe 3-1. Create Data and Verify the Data Type\\x08................................. 46\\nProblem\\x08..................................................................................................................46\\nSolution\\x08..................................................................................................................46\\nHow It Works\\x08...........................................................................................................46\\nRecipe 3-2. Create and Index a Python String\\x08...................................... 48\\nProblem\\x08..................................................................................................................48\\nSolution\\x08..................................................................................................................48\\nHow It Works\\x08...........................................................................................................49\\nRecipe 3-3. Typecast from One Data Type to Another\\x08........................... 51\\nProblem\\x08..................................................................................................................51\\nSolution\\x08..................................................................................................................51\\nHow It Works\\x08...........................................................................................................51\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 7}, page_content='■ Contents\\nviii\\nRecipe 3-4. Work with a Python List\\x08..................................................... 54\\nProblem\\x08..................................................................................................................54\\nSolution\\x08..................................................................................................................54\\nHow It Works\\x08...........................................................................................................54\\nRecipe 3-5. Work with a Python Tuple\\x08................................................... 58\\nProblem\\x08..................................................................................................................58\\nSolution\\x08..................................................................................................................58\\nHow It Works\\x08...........................................................................................................58\\nRecipe 3-6. Work with a Python Set\\x08...................................................... 60\\nProblem\\x08..................................................................................................................60\\nSolution\\x08..................................................................................................................60\\nHow It Works\\x08...........................................................................................................60\\nRecipe 3-7. Work with a Python Dictionary\\x08........................................... 62\\nProblem\\x08..................................................................................................................62\\nSolution\\x08..................................................................................................................62\\nHow It Works\\x08...........................................................................................................63\\nRecipe 3-8. Work with Define and Call Functions\\x08................................. 64\\nProblem\\x08..................................................................................................................64\\nSolution\\x08..................................................................................................................64\\nHow It Works\\x08...........................................................................................................65\\nRecipe 3-9. Work with Create and Call Lambda Functions\\x08................... 66\\nProblem\\x08..................................................................................................................66\\nSolution\\x08..................................................................................................................66\\nHow It Works\\x08...........................................................................................................66\\nRecipe 3-10. Work with Python Conditionals\\x08........................................ 67\\nProblem\\x08..................................................................................................................67\\nSolution\\x08..................................................................................................................67\\nHow It Works\\x08...........................................................................................................67\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 8}, page_content='■ Contents\\nix\\nRecipe 3-11. Work with Python “for” and  \\n“while” Loops\\x08........................................................................................ 68\\nProblem\\x08..................................................................................................................68\\nSolution\\x08..................................................................................................................68\\nHow It Works\\x08...........................................................................................................69\\nRecipe 3-12. Work with NumPy\\x08............................................................. 70\\nProblem\\x08..................................................................................................................70\\nSolution\\x08..................................................................................................................70\\nHow It Works\\x08...........................................................................................................71\\nRecipe 3-13. Integrate IPython and IPython Notebook with PySpark\\x08... 78\\nProblem\\x08..................................................................................................................78\\nSolution\\x08..................................................................................................................79\\nHow It Works\\x08...........................................................................................................79\\n■\\n■\\x07Chapter 4: Spark Architecture and the Resilient  \\nDistributed Dataset\\x08.............................................................................85\\nRecipe 4-1. Create an RDD\\x08.................................................................... 89\\nProblem\\x08..................................................................................................................89\\nSolution\\x08..................................................................................................................89\\nHow It Works\\x08...........................................................................................................89\\nRecipe 4-2. Convert Temperature Data\\x08................................................. 91\\nProblem\\x08..................................................................................................................91\\nSolution\\x08..................................................................................................................91\\nHow It Works\\x08...........................................................................................................92\\nRecipe 4-3. Perform Basic Data Manipulation\\x08...................................... 94\\nProblem\\x08..................................................................................................................94\\nSolution\\x08..................................................................................................................94\\nHow It Works\\x08...........................................................................................................95\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 9}, page_content='■ Contents\\nx\\nRecipe 4-4. Run Set Operations\\x08............................................................ 99\\nProblem\\x08..................................................................................................................99\\nSolution\\x08..................................................................................................................99\\nHow It Works\\x08.........................................................................................................100\\nRecipe 4-5. Calculate Summary Statistics\\x08.......................................... 103\\nProblem\\x08................................................................................................................103\\nSolution\\x08................................................................................................................103\\nHow It Works\\x08.........................................................................................................104\\nRecipe 4-6. Start PySpark Shell on Standalone Cluster Manager\\x08....... 109\\nProblem\\x08................................................................................................................109\\nSolution\\x08................................................................................................................109\\nHow It Works\\x08.........................................................................................................109\\nRecipe 4-7. Start PySpark Shell on Mesos\\x08.......................................... 113\\nProblem\\x08................................................................................................................113\\nSolution\\x08................................................................................................................113\\nHow It Works\\x08.........................................................................................................113\\n■\\n■Chapter 5: The Power of Pairs: Paired RDDs\\x08............................... 115\\nRecipe 5-1. Create a Paired RDD\\x08......................................................... 115\\nProblem\\x08................................................................................................................115\\nSolution\\x08................................................................................................................115\\nHow It Works\\x08.........................................................................................................116\\nRecipe 5-2. Aggregate data\\x08................................................................. 119\\nProblem\\x08................................................................................................................119\\nSolution\\x08................................................................................................................119\\nHow It Works\\x08.........................................................................................................120\\nRecipe 5-3. Join Data\\x08.......................................................................... 126\\nProblem\\x08................................................................................................................126\\nSolution\\x08................................................................................................................127\\nHow It Works\\x08.........................................................................................................128\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 10}, page_content='■ Contents\\nxi\\nRecipe 5-4. Calculate Page Rank\\x08........................................................ 132\\nProblem\\x08................................................................................................................132\\nSolution\\x08................................................................................................................132\\nHow It Works\\x08.........................................................................................................133\\n■\\n■Chapter 6: I/O in PySpark\\x08........................................................... 137\\nRecipe 6-1. Read a Simple Text File\\x08.................................................... 137\\nProblem\\x08................................................................................................................137\\nSolution\\x08................................................................................................................138\\nHow It Works\\x08.........................................................................................................138\\nRecipe 6-2. Write an RDD to a Simple Text File\\x08................................... 141\\nProblem\\x08................................................................................................................141\\nSolution\\x08................................................................................................................141\\nHow It Works\\x08.........................................................................................................142\\nRecipe 6-3. Read a Directory\\x08.............................................................. 143\\nProblem\\x08................................................................................................................143\\nSolution\\x08................................................................................................................143\\nHow It Works\\x08.........................................................................................................144\\nRecipe 6-4. Read Data from HDFS\\x08...................................................... 145\\nProblem\\x08................................................................................................................145\\nSolution\\x08................................................................................................................145\\nHow It Works\\x08.........................................................................................................145\\nRecipe 6-5. Save RDD Data to HDFS\\x08................................................... 146\\nProblem\\x08................................................................................................................146\\nSolution\\x08................................................................................................................146\\nHow It Works\\x08.........................................................................................................146'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 11}, page_content='■ Contents\\nxii\\nRecipe 6-6. Read Data from a Sequential File\\x08.................................... 147\\nProblem\\x08................................................................................................................147\\nSolution\\x08................................................................................................................147\\nHow It Works\\x08.........................................................................................................148\\nRecipe 6-7. Write Data to a Sequential File\\x08......................................... 148\\nProblem\\x08................................................................................................................148\\nSolution\\x08................................................................................................................148\\nHow It Works\\x08.........................................................................................................149\\nRecipe 6-8. Read a CSV File\\x08................................................................ 150\\nProblem\\x08................................................................................................................150\\nSolution\\x08................................................................................................................150\\nHow It Works\\x08.........................................................................................................151\\nRecipe 6-9. Write an RDD to a CSV File\\x08............................................... 152\\nProblem\\x08................................................................................................................152\\nSolution\\x08................................................................................................................152\\nHow It Works\\x08.........................................................................................................152\\nRecipe 6-10. Read a JSON File\\x08........................................................... 154\\nProblem\\x08................................................................................................................154\\nSolution\\x08................................................................................................................154\\nHow It Works\\x08.........................................................................................................155\\nRecipe 6-11. Write an RDD to a JSON File\\x08.......................................... 156\\nProblem\\x08................................................................................................................156\\nSolution\\x08................................................................................................................156\\nHow It Works\\x08.........................................................................................................157\\nRecipe 6-12. Read Table Data from HBase by Using PySpark\\x08............. 159\\nProblem\\x08................................................................................................................159\\nSolution\\x08................................................................................................................159\\nHow It Works\\x08.........................................................................................................160'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 12}, page_content='■ Contents\\nxiii\\n■\\n■Chapter 7: Optimizing PySpark and PySpark Streaming\\x08............ 163\\n\\x07Recipe 7-1. Optimize the Page-Rank Algorithm by Using  \\nPySpark Code\\x08...................................................................................... 164\\n\\x07Problem\\x08................................................................................................................164\\n\\x07Solution\\x08................................................................................................................164\\n\\x07How It Works\\x08.........................................................................................................164\\n\\x07Recipe 7-2. Implement the k-Nearest Neighbors Algorithm by  \\nUsing PySpark\\x08..................................................................................... 166\\n\\x07Problem\\x08................................................................................................................166\\n\\x07Solution\\x08................................................................................................................166\\n\\x07How It Works\\x08.........................................................................................................171\\n\\x07Recipe 7-3. Read Streaming Data from the Console Using  \\nPySpark Streaming\\x08.............................................................................. 174\\n\\x07Problem\\x08................................................................................................................174\\n\\x07Solution\\x08................................................................................................................174\\n\\x07How It Works\\x08.........................................................................................................175\\n\\x07Recipe 7-4. Integrate PySpark Streaming with Apache Kafka,  \\nand Read and Analyze the Data\\x08........................................................... 178\\n\\x07Problem\\x08................................................................................................................178\\n\\x07Solution\\x08................................................................................................................178\\n\\x07How It Works\\x08.........................................................................................................179\\n\\x07Recipe 7-5. Execute a PySpark Script in Local Mode\\x08......................... 182\\n\\x07Problem\\x08................................................................................................................182\\n\\x07Solution\\x08................................................................................................................182\\n\\x07How It Works\\x08.........................................................................................................183\\n\\x07Recipe 7-6. Execute a PySpark Script Using Standalone Cluster \\nManager and Mesos Cluster Manager\\x08................................................ 184\\n\\x07Problem\\x08................................................................................................................184\\n\\x07Solution\\x08................................................................................................................184\\n\\x07How It Works\\x08.........................................................................................................185'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 13}, page_content='■ Contents\\nxiv\\n■\\n■Chapter 8: PySparkSQL\\x08............................................................... 187\\nRecipe 8-1. Create a DataFrame\\x08......................................................... 188\\nProblem\\x08................................................................................................................188\\nSolution\\x08................................................................................................................188\\nHow It Works\\x08.........................................................................................................188\\nRecipe 8-2. Perform Exploratory Data Analysis  \\non a DataFrame\\x08................................................................................... 195\\nProblem\\x08................................................................................................................195\\nSolution\\x08................................................................................................................195\\nHow It Works\\x08.........................................................................................................195\\nRecipe 8-3. Perform Aggregation Operations  \\non a DataFrame\\x08................................................................................... 200\\nProblem\\x08................................................................................................................200\\nSolution\\x08................................................................................................................200\\nHow It Works\\x08.........................................................................................................201\\nRecipe 8-4. Execute SQL and HiveQL Queries  \\non a DataFrame\\x08................................................................................... 207\\nProblem\\x08................................................................................................................207\\nSolution\\x08................................................................................................................207\\nHow It Works\\x08.........................................................................................................207\\nRecipe 8-5. Perform Data Joining on DataFrames\\x08.............................. 210\\nProblem\\x08................................................................................................................210\\nSolution\\x08................................................................................................................210\\nHow It Works\\x08.........................................................................................................210\\nRecipe 8-6. Perform Breadth-First Search Using GraphFrames\\x08......... 220\\nProblem\\x08................................................................................................................220\\nSolution\\x08................................................................................................................221\\nHow It Works\\x08.........................................................................................................222'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 14}, page_content='■ Contents\\nxv\\nRecipe 8-7. Calculate Page Rank Using GraphFrames\\x08........................ 226\\nProblem\\x08................................................................................................................226\\nSolution\\x08................................................................................................................226\\nHow It Works\\x08.........................................................................................................226\\nRecipe 8-8. Read Data from Apache Hive\\x08........................................... 230\\nProblem\\x08................................................................................................................230\\nSolution\\x08................................................................................................................230\\nHow It Works\\x08.........................................................................................................232\\n■\\n■Chapter 9: PySpark MLlib and Linear Regression\\x08...................... 235\\nRecipe 9-1. Create a Dense Vector\\x08...................................................... 236\\nProblem\\x08................................................................................................................236\\nSolution\\x08................................................................................................................236\\nHow It Works\\x08.........................................................................................................236\\nRecipe 9-2. Create a Sparse Vector\\x08.................................................... 237\\nProblem\\x08................................................................................................................237\\nSolution\\x08................................................................................................................237\\nHow It Works\\x08.........................................................................................................238\\nRecipe 9-3. Create Local Matrices\\x08...................................................... 239\\nProblem\\x08................................................................................................................239\\nSolution\\x08................................................................................................................239\\nHow It Works\\x08.........................................................................................................239\\nRecipe 9-4. Create a Row Matrix\\x08........................................................ 241\\nProblem\\x08................................................................................................................241\\nSolution\\x08................................................................................................................241\\nHow It Works\\x08.........................................................................................................241\\nRecipe 9-5. Create a Labeled Point\\x08..................................................... 242\\nProblem\\x08................................................................................................................242\\nSolution\\x08................................................................................................................242\\nHow It Works\\x08.........................................................................................................242'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 15}, page_content='■ Contents\\nxvi\\nRecipe 9-6. Apply Linear Regression\\x08.................................................. 243\\nProblem\\x08................................................................................................................243\\nSolution\\x08................................................................................................................243\\nHow It Works\\x08.........................................................................................................244\\nRecipe 9-7. Apply Ridge Regression\\x08................................................... 251\\nProblem\\x08................................................................................................................251\\nSolution\\x08................................................................................................................251\\nHow It Works\\x08.........................................................................................................252\\nRecipe 9-8. Apply Lasso Regression\\x08................................................... 257\\nProblem\\x08................................................................................................................257\\nSolution\\x08................................................................................................................257\\nHow It Works\\x08.........................................................................................................258\\nIndex\\x08.............................................................................................. 261'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 16}, page_content='xvii\\nAbout the Author\\nRaju Kumar Mishra has a strong interest in data \\nscience and systems that have the capability of \\nhandling large amounts of data and operating \\ncomplex mathematical models through computational \\nprogramming. He was inspired to pursue a Master of \\nTechnology degree in computational sciences from the \\nIndian Institute of Science in Bangalore, India. Raju \\nprimarily works in the areas of data science and its \\nvarious applications. Working as a corporate trainer, \\nhe has developed unique insights that help him in \\nteaching and explaining complex ideas with ease. Raju \\nis also a data science consultant who solves complex \\nindustrial problems. He works on programming tools \\nsuch as R, Python, scikit-learn, Statsmodels, Hadoop, \\nHive, Pig, Spark, and many others.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 17}, page_content='xix\\nAbout the Technical \\nReviewer\\nSundar Rajan Raman is an artificial intelligence \\npractitioner currently working for Bank of America. \\nHe holds a Bachelor of Technology degree from the \\nNational Institute of Technology in India. Being a \\nseasoned Java and J2EE programmer, he has worked \\nat companies such as AT&T, Singtel, and Deutsche \\nBank. He is a messaging platform specialist with vast \\nexperience on SonicMQ, WebSphere MQ, and TIBCO \\nsoftware, with respective certifications. His current \\nfocus is on artificial intelligence, including machine \\nlearning and neural networks. More information is \\navailable at https://in.linkedin.com/pub/sundar-\\nrajan-raman/7/905/488.\\nI would like to thank my wife, Hema, and my \\ndaughter, Shriya, for their patience during the review \\nprocess.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 18}, page_content='xxi\\nAcknowledgments\\nMy heartiest thanks to the Almighty. I also would like to thank my mother, Smt. Savitri \\nMishra; my sisters, Mitan and Priya; my cousins, Suchitra and Chandni; and my maternal \\nuncle, Shyam Bihari Pandey; for their support and encouragement. I am very grateful to \\nmy sweet and beautiful wife, Smt. Smita Rani Pathak, for her continuous encouragement \\nand love while I was writing this book. I thank my brother-in-law, Mr. Prafull Chandra \\nPandey, for his encouragement to write this book. I am very thankful to my sisters-in-law, \\nRinky, Reena, Kshama, Charu, Dhriti, Kriti, and Jyoti for their encouragement as well.  \\nI am grateful to Anurag Pal Sehgal, Saurabh Gupta, Devendra Mani Tripathi, and all my \\nfriends. Last but not least, thanks to Coordinating Editor Sanchita Mandal, Acquisitions \\nEditor Celestin Suresh John, and Development Editor Laura Berendson at Apress; without \\nthem, this book would not have been possible.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 19}, page_content='xxiii\\nIntroduction\\nThis book will take you on an interesting journey to learn about PySpark and big  \\ndata through a problem-solution approach. Every problem is followed by a detailed,  \\nstep-by-step answer, which will improve your thought process for solving big data \\nproblems with PySpark. This book is divided into nine chapters. Here’s a brief description \\nof each chapter:\\nChapter 1, “The Era of Big Data, Hadoop, and Other Big Data Processing \\nFrameworks,” covers many big data processing tools such as Apache Hadoop, Apache Pig, \\nApache Hive, and Apache Spark. The shortcomings of Hadoop and the evolution of Spark \\nare discussed. Apache Kafka is explained as a publish-subscribe system. This chapter also \\nsheds light on HBase, a NoSQL database.\\nChapter 2, “Installation,” will take you to the real battleground. You’ll learn how to \\ninstall many big data processing tools such as Hadoop, Hive, Spark, Apache Mesos, and \\nApache HBase.\\nChapter 3, “Introduction to Python and NumPy,” is for newcomers to Python. \\nYou will learn about the basics of Python and NumPy by following a problem-solution \\napproach. Problems in this chapter are data-science oriented.\\nChapter 4, “Spark Architecture and the Resilient Distributed Dataset,” explains the \\narchitecture of Spark and introduces resilient distributed datasets. You’ll learn about \\ncreating RDDs and using data-analysis algorithms for data aggregation, data filtering, and \\nset operations on RDDs.\\nChapter 5, “The Power of Pairs: Paired RDD,” shows how to create paired RDDs and \\nhow to perform data aggregation, data joining, and other algorithms on these paired \\nRDDs.\\nChapter 6, “I/O in PySpark,” will teach you how to read data from various types of \\nfiles and save the result as an RDD.\\nChapter 7, “Optimizing PySpark and PySpark Streaming,” is one of the most \\nimportant chapters. You will start by optimizing a page-rank algorithm. Then you’ll \\nimplement a k-nearest neighbors algorithm and optimize it by using broadcast variables \\nprovided by the PySpark framework. Learning PySpark Streaming will finally lead us into \\nintegrating Apache Kafka with the PySpark Streaming framework.\\nChapter 8, “PySparkSQL,” is paradise for readers who use SQL. But newcomers \\nwill also learn PySparkSQL in order to write SQL-like queries on DataFrames by using a \\nproblem-solution approach. Apart from DataFrames, we will also implement the graph \\nalgorithms breadth-first search and page rank by using the GraphFrames library.\\nChapter 9, “PySpark MLlib and Linear Regression,” describes PySpark’s machine-\\nlearning library, MLlib. You will see many recipes on various data structures provided \\nby PySpark MLlib. You’ll also implement linear regression. Recipes on lasso and ridge \\nregression are included in the chapter.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 20}, page_content='1\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_1 \\nCHAPTER 1\\nThe Era of Big Data, \\nHadoop, and Other Big Data \\nProcessing Frameworks\\nWhen I first joined Orkut, I was happy. With Orkut, I had a new platform enabling me get \\nto know the people around me, including their thoughts, their views, their purchases, \\nand the places they visited. We were all gaining more knowledge than ever before and \\nfelt more connected to the people around us. Uploading pictures helped us share good \\nideas of places to visit. I was becoming more and more addicted to understanding \\nand expressing sentiments. After a few years, I joined Facebook. And day by day, I was \\nintroduced to what became an infinite amount of information from all over world. Next, \\nI started purchasing items online, and I liked it more than shopping offline. I could easily \\nget a lot of information about products, and I could compare prices and features. And I \\nwasn’t the only one; millions of people were feeling the same way about the Web.\\nMore and more data was flooding in from every corner of the world to the Web. And \\nthanks to all those inventions related to data storage systems, people could store this huge \\ninflow of data.\\nMore and more users joined the Web from all over the world, and therefore \\nincreased the amount of data being added to these storage systems. This data was in the \\nform of opinions, pictures, videos, and other forms of data too. This data deluge forced \\nusers to adopt distributed systems. Distributed systems require distributed programming. \\nAnd we also know that distributed systems require extra care for fault-tolerance and \\nefficient algorithms. Distributed systems always need two things: reliability of the system \\nand availability of all its components.\\nApache Hadoop was introduced, ensuring efficient computation and fault-tolerance \\nfor distributed systems. Mainly, it concentrated on reliability and availability. Because \\nApache Hadoop was easy to program, many people became interested in big data. Big \\ndata became a popular topic for discussion everywhere. E-commerce companies wanted \\nto know more about their customers, and the health-care industry was interested in \\ngaining insights from the data collected, for example. More data metrics were defined. \\nMore data points started to be collected.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 21}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n2\\nMany open source big data tools emerged, including Apache Tez and Apache Storm. \\nThis was also a time that many NoSQL databases emerged to deal with this huge data inflow. \\nApache Spark also evolved as a distributed system and became very popular during this time.\\nIn this chapter, we are going to discuss big data as well as Hadoop as a distributed \\nsystem for processing big data. In covering the components of Hadoop, we will also \\ndiscuss Hadoop ecosystem frameworks such as Apache Hive and Apache Pig. The \\nusefulness of the components of the Hadoop ecosystem is also discussed to give you \\nan overview. Throwing light on some of the shortcomings of Hadoop will give you \\nbackground on the development of Apache Spark. The chapter will then move through \\na description of Apache Spark. We will also discuss various cluster managers that work \\nwith Apache Spark. The chapter wouldn’t be complete without discussing NoSQL, so \\ndiscussion on the NoSQL database HBase is also included. Sometimes we read data from \\na relational database management system (RDBMS); this chapter discusses PostgreSQL.\\nBig Data\\nBig data is one of the hot topics of this era. But what is big data? Big data describes a \\ndataset that is huge and increasing with amazing speed. Apart from this volume and \\nvelocity, big data is also characterized by its variety of data and veracity. Let’s explore \\nthese terms—volume, velocity, variety, and veracity—in detail. These are also known as \\nthe 4V characteristics of big data, as illustrated in Figure\\xa01-1.\\nVolume\\nThe volume specifies the amount of data to be processed. A large amount of data requires \\nlarge machines or distributed systems. And the time required for computation will also \\nincrease with the volume of data. So it’s better to go for a distributed system, if we can \\nparallelize our computation. Volume might be of structured data, unstructured data, \\nVolume\\nVariety\\nVeracity\\nVelocity\\nFigure 1-1.\\u2002 Characteristcis of big data'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 22}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n3\\nor any data. If we have unstructured data, the situation becomes more complex and \\ncomputing intensive. You might wonder, how big is big? What volume of data should be \\nclassified as big data? This is again a debatable question. But in general, we can say  \\nthat an amount of data that we can’t handle via a conventional system can be considered \\nbig data.\\nVelocity\\nEvery organization is becoming more and more data conscious. A lot of data is collected \\nevery moment. This means that the velocity of data—the speed of the data flow and \\nof data processing—is also increasing. How will a single system be able to handle this \\nvelocity? The problem becomes complex when we have to analyze a large inflow of data \\nin real time. Each day, systems are being developed to deal with this huge inflow of data.\\nVariety\\nSometimes the variety of data adds enough complexity that conventional data analysis \\nsystems can’t analyze data well. What do we mean by variety? You might think data is \\njust data. But this is not the case. Image data is different from simple tabular data, for \\nexample, because of the way it is organized and saved. In addition, an infinite number of \\nfile systems are available, and every file system requires a different way of dealing with it. \\nReading and writing a JSON file, for instance, will be different from the way we deal with a \\nCSV file. Nowadays, a data scientist has to handle a combination of these data types. The \\ndata you are going to deal with might be a combination of pictures, videos, and text. This \\nvariety of data makes big data more complex to analyze.\\nVeracity\\nCan you imagine a logically incorrect computer program resulting in the correct output? \\nOf course not. Similarly, data that is not accurate is going to provide misleading results. \\nThe veracity of data is one of the important concerns related to big data. When we \\nconsider the condition of big data, we have to think about any abnormalities in the data.\\nHadoop\\nHadoop is a distributed and scalable framework for solving big data problems. Hadoop, \\ndeveloped by Doug Cutting and Mark Cafarella, is written in Java. It can be installed on \\na cluster of commodity hardware, and it scales horizontally on distributed systems. Easy \\nto program Inspiration from Google research paper Hadoop was developed. Hadoop’s \\ncapability to work on commodity hardware makes it cost-effective. If we are working on \\ncommodity hardware, fault-tolerance is an inevitable issue. But Hadoop provides a fault-\\ntolerant system for data storage and computation, and this fault-tolerant capability has \\nmade Hadoop popular.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 23}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n4\\nHadoop has two components, as illustrated in Figure\\xa01-2. The first component is the \\nHadoop Distributed File System (HDFS). The second component is MapReduce. HDFS is \\nfor distributed data storage, and MapReduce is for performing computation on the data \\nstored in HDFS.\\nHDFS\\nHDFS is used to store large amounts of data in a distributed and fault-tolerant fashion. \\nHDFS is written in Java and runs on commodity hardware. It was inspired by a Google \\nresearch paper about the Google File System (GFS). It is a write-once and read-many-\\ntimes system that’s effective for large amounts of data.\\nHDFS comprises two components: NameNode and DataNode. These two \\ncomponents are Java daemon processes. A NameNode, which maintains metadata of files \\ndistributed on a cluster, works as the master for many DataNodes. HDFS divides a large \\nfile into small blocks and saves the blocks on different DataNodes. The actual file data \\nblocks reside on DataNodes.\\nHDFS provides a set of Unix shell-like commands to deal with it. But we can use \\nthe Java file system API provided by HDFS to work at a finer level on large files. Fault-\\ntolerance is implemented by using replications of data blocks.\\nWe can access the HDFS files by using a single-thread process and also in parallel. \\nHDFS provides a useful utility, distcp, which is generally used to transfer data in parallel \\nfrom one HDFS system to another. It copies data by using parallel map tasks. You can see \\nthe HDFS components in Figure\\xa01-3.\\nHadoop\\nHDFS\\nMapReduce\\nFigure 1-2.\\u2002 Hadoop components'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 24}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n5\\nMapReduce\\nThe Map-Reduce model of computation first appeared in a Google research paper. \\nThis research paper was implemented in Hadoop as Hadoop’s MapReduce. Hadoop’s \\nMapReduce is the computation engine of the Hadoop framework, which performs \\ncomputations on the distributed data in HDFS. MapReduce is horizontally scalable \\non distributed systems of commodity hardware. It also scales for large problems. In \\nMapReduce, the solution is broken into two phases: the map phase and the reduce phase. \\nIn the map phase, a chunk of data is processed, and in the reduce phase, an aggregation \\nor a reduction operation is run on the result of the map phase. Hadoop’s MapReduce \\nframework is written in Java.\\nMapReduce uses a master/slave model. In Hadoop 1, this map-reduce computation \\nwas managed by two daemon processes: Jobtracker and Tasktracker. Jobtracker is a master \\nprocess that deals with many Tasktrackers. There’s no need to say that Tasktracker is a \\nslave to Jobtracker. But in Hadoop 2, Jobtracker and Tasktracker were replaced by YARN.\\nBecause we know that Hadoop’s MapReduce framework is written in Java, we can \\nwrite our MapReduce code by using an API provided by the framework and programmed \\nin Java. The Hadoop streaming module gives further power so that a person knowing \\nanother programming language (such as Python or Ruby) can program MapReduce.\\nMapReduce algorithms are good for many algorithms. Many machine-learning \\nalgorithms are implemented as Apache Mahout. Mahout used to run on Hadoop as Pig \\nand Hive.\\nBut MapReduce wasn’t very good for iterative algorithms. At the end of every \\nHadoop job, MapReduce will save the data to HDFS and read it back again for the next \\njob. We know that reading and writing data to a file is one of the costliest activities. \\nApache Spark mitigated this shortcoming of MapReduce by providing in-memory data \\npersisting and computation.\\nHDFS\\nNameNode\\nDataNode\\nFigure 1-3.\\u2002 Components of HDFS'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 25}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n6\\n■\\n■Note\\u2003  You can read more about MapReduce and Mahout at the following web pages:\\nwww.usenix.org/legacy/publications/library/proceedings/osdi04/tech/ \\nfull_papers/dean/dean_html/index.html\\nhttps://mahout.apache.org/users/basics/quickstart.html\\nApache Hive\\nThe world of computer science is one of abstraction. Everyone knows that all data \\nultimately exists in the form of bits. Programming languages such as C enable us to \\navoid programming in machine-level language. The C language provides an abstraction \\nover machine and assembly language. More abstraction is provided by other high-level \\nlanguages. Structured Query Language (SQL) is one of the abstractions. SQL is widely \\nused all over the world by many data modeling experts. Hadoop is good for analysis \\nof big data. So how can a large population knowing SQL utilize the power of Hadoop \\ncomputational power on big data? In order to write Hadoop’s MapReduce program, users \\nmust know a programming language that can be used to program Hadoop’s MapReduce.\\nIn the real world, day-to-day problems follow patterns. In data analysis, some \\nproblems are common, such as manipulating data, handling missing values, transforming \\ndata, and summarizing data. Writing MapReduce code for these day-to-day problems is \\nhead-spinning work for a nonprogrammer. Writing code to solve a problem is not a very \\nintelligent thing. But writing efficient code that has performance scalability and can be extended \\nis something that is valuable. Having this problem in mind, Apache Hive was developed at \\nFacebook, so that general problems can be solved without writing MapReduce code.\\nAccording to the Hive wiki, “Hive is a data warehousing infrastructure based on \\nApache Hadoop.” Hive has its own SQL dialect, which is known as Hive Query Language \\n(abbreviated as HiveQL or HQL). Using HiveQL, Hive can query data in HDFS. Hive can run \\nnot only on HDFS, but also on Spark and other big data frameworks such as Apache Tez.\\nHive provides the user an abstraction that is like a relational database management \\nsystem for structured data in HDFS. We can create tables and run SQL-like queries on \\nthem. Hive saves the table schema in an RDBMS. Apache Derby is the default RDBMS, \\nwhich is shipped with the Apache Hive distribution. Apache Derby has been fully written \\nin Java; this open source RDBMS comes with the Apache License, Version 2.0.\\nHive\\nCommands\\nMapReduce\\nCode\\nRun on\\nHadoop\\nCluster\\nFigure 1-4.\\u2002 Code execution flow in Apache Hive'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 26}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n7\\nHiveQL commands are transformed into Hadoop’s MapReduce code, and then it \\nruns on Hadoop cluster. You can see the Hive command execution flow in Figure\\xa01-4.\\nA person knowing SQL can easily learn Apache Hive and HiveQL and can use the \\nbenefits of storage and the computation power of Hadoop in their day-to-day data \\nanalysis of big data. HiveQL is also supported by PySparkSQL. We can run HiveQL \\ncommands in PySparkSQL. Apart from executing HiveQL queries, we can also read data \\nfrom Hive directly to PySparkSQL and write results to Hive.\\n■\\n■Note\\u2003  You can read more about Hive and the Apache Derby RDBMS at the following  \\nweb pages:\\nhttps://cwiki.apache.org/confluence/display/Hive/Tutorial\\nhttps://db.apache.org/derby/\\nApache Pig\\nApache Pig is data-flow framework for performing data-analysis algorithms on huge \\namounts of data. It was developed by Yahoo!, open sourced to the Apache Software \\nFoundation, and is now available under the Apache License, Version 2.0. The pig \\nprogramming language is a Pig Latin scripting language. Pig is loosely connected to \\nHadoop, which means that we can connect it to Hadoop and perform analysis. But Pig \\ncan be used with other tools such as Apache Tez and Apache Spark.\\nApache Hive is used as reporting tool, whereas Apache Pig is used as an  \\nextract, transform, and load (ETL) tool. We can extend the functionality of Pig by using \\nuser-defined functions (UDFs). User-defined functions can be written in many languages, \\nincluding Java, Python, Ruby, JavaScript, Groovy, and Jython.\\nApache Pig uses HDFS to read and store the data, and Hadoop’s MapReduce to \\nexecute the data-science algorithms. Apache Pig is similar to Apache Hive in using \\nthe Hadoop cluster. As Figure\\xa01-5 depicts, on Hadoop, Pig Latin commands are first \\ntransformed into Hadoop’s MapReduce code. And then the transformed MapReduce \\ncode runs on the Hadoop cluster.\\nPig\\nCommands\\nMapReduce\\nCode\\nRun on\\nHadoop\\nCluster\\nFigure 1-5.\\u2002 Code execution flow in Apache Pig'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 27}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n8\\nThe best part of Pig is that the code is optimized and tested to work for day-to-day \\nproblems. A user can directly install Pig and start using it. Pig provides a Grunt shell to \\nrun interactive Pig commands, so anyone who knows Pig Latin can enjoy the benefits of \\nHDFS and MapReduce, without knowing an advanced programming language such as \\nJava or Python.\\n■\\n■Note\\u2003  You can read more about Apache Pig at the following sites:\\nhttp://pig.apache.org/docs/\\nhttps://en.wikipedia.org/wiki/Pig_(programming_tool)\\nhttps://cwiki.apache.org/confluence/display/PIG/Index\\nApache Kafka\\nApache Kafka is a publish-subscribe, distributed messaging platform. It was developed at \\nLinkedIn and later open sourced to the Apache Foundation. It is fault-tolerant, scalable, \\nand fast. A message, in Kafka terms, is the smallest unit of data that can flow from a \\nproducer to a consumer through a Kafka server, and that can be persisted and used at a \\nlater time. You might be confused about the terms producer and consumer. We are going \\nto discuss these terms soon. Another key term we are going to use in the context of Kafka \\nis topic. A topic is stream of messages of a similar category. Kafka comes with a built-in \\nAPI, which developers can use to build their applications. We are the ones who define the \\ntopic. Now let’s discuss the three main components of Apache Kafka.\\nProducer\\nA Kafka producer produces the message to a Kafka topic. It can publish data to more than \\none topic.\\nBroker\\nThe broker is the main Kafka server that runs on a dedicated machine. Messages are \\npushed to the broker by the producer. The broker persists topics in different partitions, \\nand these partitions are replicated to different brokers to deal with faults. The broker is \\nstateless, so the consumer has to track the message it has consumed.\\nConsumer\\nA consumer fetches messages from the Kafka broker. Remember, it fetches the messages; \\nthe Kafka broker doesn’t push messages to the consumer; rather, the consumer pulls \\ndata from the Kafka broker. Consumers are subscribed to one or more topics on the Kafka'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 28}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n9\\nbroker, and they read the messages. The consumer also keeps tracks of all the messages \\nthat it has already consumed. Data is persisted in a broker for a specified time. If the \\nconsumer fails, it can fetch the data after its restart.\\nFigure\\xa01-6 explains the message flow of Apache Kafka. The producer publishes \\na message to the topic. Then the consumer pulls data from the broker. In between \\npublishing and pulling, the message is persisted by the Kafka broker.\\nProducer\\nConsumer\\nBroker\\nPublish\\nTopics\\nFetch\\nFigure 1-6.\\u2002 Apache Kafka message flow\\nWe will integrate Apache Kafka with PySpark in Chapter 7, which discusses Kafka further.\\n■\\n■Note\\u2003  You can read more about Apache Kafka at the following sites:\\nhttps://kafka.apache.org/documentation/\\nhttps://kafka.apache.org/quickstart\\nApache Spark\\nApache Spark is a general-purpose, distributed programming framework. It is considered \\nvery good for iterative as well as batch processing of data. Developed at the AMPLab at \\nthe University of California, Berkeley, Spark is now open source software that provides \\nan in-memory computation framework. On the one hand, it is good for batch processing; \\non the other hand, it works well with real-time (or, better to say, near-real-time) data. \\nMachine learning and graph algorithms are iterative. Where Spark do magic. According to \\nits research paper, it is approximately 100 times faster than its peer, Hadoop. Data can be \\ncached in memory. Caching intermediate data in iterative algorithms provides amazingly \\nfast processing speed. Spark can be programmed with Java, Scala, Python, and R.\\nIf anyone is considering Spark as an improved Hadoop, then to some extent, that is \\nfine in my view. Because we can implement a MapReduce algorithm in Spark, Spark uses \\nthe benefit of HDFS; this means Spark can read data from HDFS and store data to HDFS \\ntoo, and Spark handles iterative computation efficiently because data can be persisted in \\nmemory. Apart from in-memory computation, Spark is good for interactive data analysis.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 29}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n10\\nWe are going to study Apache Spark with Python. This is also known as PySpark. \\nPySpark comes with many libraries for writing efficient programs, and there are some \\nexternal libraries as well. Here are some of them:\\n• \\nPySparkSQL: A PySpark library to apply SQL-like analysis on a \\nhuge amount of structured or semistructured data. We can also \\nuse SQL queries with PySparkSQL. We can connect it to Apache \\nHive, and HiveQL can be applied too. PySparkSQL is a wrapper \\nover the PySpark core. PySparkSQL introduced the DataFrame, \\nwhich is a tabular representation of structured data that is like a \\ntable in a relational database management system. Another data \\nabstraction, the DataSet, was introduced in Spark 1.6, but it does \\nnot work with PySparkSQL.\\n• \\nMLlib: MLlib is a wrapper over the PySpark core that deals \\nwith machine-learning algorithms. The machine-learning API \\nprovided by the MLlib library is easy to use. MLlib supports many \\nmachine-learning algorithms for classification, clustering, text \\nanalysis, and more.\\n• \\nGraphFrames: The GraphFrames library provides a set of APIs for \\nperforming graph analysis efficiently, using the PySpark core and \\nPySparkSQL. At the time of this writing, DataFrames is an external \\nlibrary. You have to download and install it separately. We are \\ngoing to perform graph analysis in Chapter 8.\\nCluster Managers\\nIn a distributed system, a job or application is broken into different tasks, which can run \\nin parallel on different machines of the cluster. A task, while running, needs resources \\nsuch as memory and a processor. The most important part is that if a machine fails, you \\nthen have to reschedule the task on another machine. The distributed system generally \\nfaces scalability problems due to mismanagement of resources. As another scenario, say a \\njob is already running on a cluster. Another person wants to run another job. The second \\njob has to wait until the first is finished. But in this way, we are not utilizing the resources \\noptimally. This resource management is easy to explain but difficult to implement on a \\ndistributed system.\\nCluster managers were developed to manage cluster resources optimally. There are \\nthree cluster managers available for Spark: Standalone, Apache Mesos, and YARN. The \\nbest part of these cluster managers is that they provide an abstraction layer between the \\nuser and the cluster. The user feels like he’s working on a single machine, while in reality \\nhe’s working on a cluster, due to the abstraction provided by cluster managers. Cluster \\nmanagers schedule cluster resources to running applications.\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 30}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n11\\nStandalone Cluster Manager\\nApache Spark is shipped with the Standalone Cluster Manager. It provides a master/slave \\narchitecture to the Spark cluster. It is Spark’s only cluster manager. You can run only Spark \\napplications when using the Standalone Cluster Manager. Its components are the master \\nand workers. Workers are the slaves to the master process. Standalone is the simplest \\ncluster manager. Spark Standalone Cluster Manager can be configured using scripts in the \\nsbin directory of Spark. We will configure Spark Standalone Cluster Manager in the coming \\nchapters and will deploy PySpark applications by using Standalone Cluster Manager.\\nApache Mesos Cluster Manager\\nApache Mesos is a general-purpose cluster manager. It was developed at the University of \\nCalifornia, Berkeley, AMPLab. Apache Mesos helps distributed solutions scale efficiently. \\nYou can run different applications using different frameworks on the same cluster when \\nusing Mesos. What do I mean by different applications using different frameworks? I mean \\nthat we can run a Hadoop application and a Spark application simultaneously on Mesos. \\nWhile multiple applications are running on Mesos, they share the resources of the cluster. \\nThe two important components of Apache Mesos are master and slaves. It has a master/\\nslave architecture similar to Spark Standalone Cluster Manager. The applications running \\non Mesos are known as the framework. Slaves inform the master about the resources \\navailable to it as a resource offer. Slave machines provides resource offers periodically. The \\nallocation module of the master server decides the framework that will get the resources.\\nYARN Cluster Manager\\nYARN stands for Yet Another Resource Negotiator. YARN was introduced in Hadoop \\n2 to scale Hadoop; resource management and job management were separated. \\nSeparating these two components made Hadoop scale better. YARN’s main components \\nare ResourceManager, ApplicationMaster, and NodeManager. There is one global \\nResourceManager, and many NodeManagers will be running per cluster.  \\nNodeManagers are slaves to the ResourceManager. The Scheduler, which is a component \\nof ResourceManager, allocates resources for different applications working on the cluster. \\nThe best part is, we can run a Spark application and any other applications such as Hadoop \\nor MPI simultaneously on clusters managed by YARN. There is one ApplicationMaster \\nper application, which deals with the task running in parallel on a distributed system. \\nRemember, Hadoop and Spark have their own kinds of ApplicationMaster.\\n■\\n■Note\\u2003 You can read more about Standalone, Apache Mesos, and YARN cluster managers \\nat the following web pages:\\nhttps://spark.apache.org/docs/2.0.0/spark-standalone.html\\nhttps://spark.apache.org/docs/2.0.0/running-on-mesos.html\\nhttps://spark.apache.org/docs/2.0.0/running-on-yarn.html'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 31}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n12\\nPostgreSQL\\nRelational database management systems are till very frequent in different organizations. \\nWhat is the meaning or relational here? It means tables. PostgreSQL is an RDBMS. It \\nruns on nearly all major operating systems, including Microsoft Windows, Unix-based \\noperating systems, macOS, and many more. It is open source software, and the code is \\navailable under the PostgreSQL license. Therefore, you can use it freely and modify it \\naccording to your requirements.\\nPostgreSQL databases can be connected through other programming languages \\nsuch as Java, Perl, Python, C, and C++ and through various programming interfaces. \\nIt can be also be programmed using a procedural programming language, Procedural \\nLanguage/PostgreSQL (PL/pgSQL), which is similar to PL/SQL. The user can add custom \\nfunctions to this database. We can write our custom functions in C/C++ and other \\nprogramming languages. We can read data from PostgreSQL from PySparkSQL by using \\nJava Database Connectivity (JDBC) connectors. In upcoming chapters, we are going to \\nread data tables from PostgreSQL by using PySparkSQL. We are also going to explore \\nmore facets of PostgreSQL in upcoming chapters.\\nPostgreSQL follows the ACID (Atomicity, Consistency, Isolation, and Durability) \\nprinciples. It comes with many features, and some might be unique to PostgreSQL itself. \\nIt supports updatable views, transactional integrity, complex queries, triggers, and other \\nfeatures. PostgreSQL performs its concurrency management by using a multiversion \\nconcurrency control model.\\nThere is a large community of support if you find a problem while using PostgreSQL. \\nPostgreSQL has been designed and developed to be extensible.\\n■\\n■Note\\u2003  If you want to learn PostgreSQL in depth, the following links will be helpful to you:\\nhttps://wiki.postgresql.org/wiki/Main_Page\\nhttps://en.wikipedia.org/wiki/PostgreSQL\\nhttps://en.wikipedia.org/wiki/Multiversion_concurrency_control\\nhttp://postgresguide.com/\\nHBase\\nHBase is an open source, distributed, NoSQL database. When I say NoSQL, you might \\nconsider it schemaless. And you’re right, to a certain extent, but not completely. At the \\ntime that you define a table, you have to mention the column family, so the database is \\nnot fully schemaless. We are going to create an HBase table in this section so you can \\nunderstand this semi-schemaless property. HBase is a column-oriented database. You \\nmight wonder what that means. Let me explain: in column-oriented databases, data is \\nsaved columnwise.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 32}, page_content='Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n13\\nWe are going to install HBase in the next chapter, but for now, let me show how a \\ntable is created and how data is put inside the tables. You can apply all these commands \\nafter installing HBase on your system. In the coming chapter, we are going to read the \\nsame data table by using PySpark.\\nhbase(main):001:0> list\\nTABLE\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n0 row(s) in 0.1750 seconds\\n=> []\\nhbase(main):002:0> create \\'pysparkBookTable\\',\\'btcf1\\',\\'btcf2\\'\\n0 row(s) in 2.2750 seconds\\n=> Hbase::Table - pysparkBookTable\\nhbase(main):003:0> list\\nTABLE\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\npysparkBookTable\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n1 row(s) in 0.0190 seconds\\n=> [\"pysparkBookTable\"]\\nhbase(main):004:0> put \\'pysparkBookTable\\', \\'00001\\', \\'btcf1:btc1\\',\\'c11\\'\\n0 row(s) in 0.1680 seconds\\nhbase(main):005:0> put \\'pysparkBookTable\\', \\'00001\\', \\'btcf2:btc2\\',\\'c21\\'\\n0 row(s) in 0.0240 seconds\\nhbase(main):006:0> put \\'pysparkBookTable\\', \\'00002\\', \\'btcf1:btc1\\',\\'c12\\'\\n0 row(s) in 0.0150 seconds\\nhbase(main):007:0> put \\'pysparkBookTable\\', \\'00002\\', \\'btcf2:btc2\\',\\'c22\\'\\n0 row(s) in 0.0070 seconds\\nhbase(main):008:0> put \\'pysparkBookTable\\', \\'00003\\', \\'btcf1:btc1\\',\\'c13\\'\\n0 row(s) in 0.0080 seconds\\nhbase(main):009:0> put \\'pysparkBookTable\\', \\'00003\\', \\'btcf2:btc2\\',\\'c23\\'\\n0 row(s) in 0.0060 seconds\\nhbase(main):010:0>\\xa0\\xa0put \\'pysparkBookTable\\', \\'00004\\', \\'btcf1:btc1\\',\\'c14\\'\\n0 row(s) in 0.0240 seconds\\nhbase(main):011:0>\\xa0\\xa0put \\'pysparkBookTable\\', \\'00004\\', \\'btcf2:btc2\\',\\'c24\\'\\n0 row(s) in 0.0280 seconds'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 33}, page_content=\"Chapter 1 ■ The Era of Big Data, Hadoop, and Other Big Data Processing Frameworks\\n14\\nhbase(main):012:0> scan 'pysparkBookTable'\\nROW\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0COLUMN+CELL\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa000001\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf1:btc1, timestamp=1496715394968, value=c11\\xa0\\xa0\\xa0\\n\\xa000001\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf2:btc2, timestamp=1496715408865, value=c21\\xa0\\xa0\\xa0\\n\\xa000002\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf1:btc1, timestamp=1496715423206, value=c12\\xa0\\xa0\\xa0\\n\\xa000002\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf2:btc2, timestamp=1496715436087, value=c22\\xa0\\xa0\\xa0\\n\\xa000003\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf1:btc1, timestamp=1496715450562, value=c13\\xa0\\xa0\\xa0\\n\\xa000003\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf2:btc2, timestamp=1496715463134, value=c23\\xa0\\xa0\\n\\xa000004\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf1:btc1, timestamp=1496715503014, value=c14\\xa0\\xa0\\xa0\\n\\xa000004\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0column=btcf2:btc2, timestamp=1496715516864, value=c24\\xa0\\xa0\\xa0\\n4 row(s) in 0.0770 seconds\\n■\\n■Note\\u2003  You can get a lot of information about HBase at https://hbase.apache.org/.\\nSpark can be used with three cluster managers: Standalone, Apache Mesos, and \\nYARN. Standalone cluster manager is shipped with Spark and it is Spark only cluster \\nmanager. With Apache Mesos and YARN, we can run heterogeneous applications.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 34}, page_content='15\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_2\\nCHAPTER 2\\nInstallation\\nIn the upcoming chapters, we are going to solve many problems by using PySpark. \\nPySpark also interacts with many other big data frameworks to provide end-to-end \\nsolutions. PySpark might read data from HDFS, NoSQL databases, or a relational \\ndatabase management system (RDBMS). After data analysis, we can also save the results \\ninto HDFS or databases.\\nThis chapter covers all the software installations that are required to go through \\nthis book. We are going to install all the required big data frameworks on the CentOS \\noperating system. CentOS is an enterprise-class operating system. It is free to use and \\neasily available. You can download CentOS from www.centos.org/download/ and then \\ninstall it on a virtual machine.\\nThis chapter covers the following recipes:\\n• \\nRecipe 2-1. Install Hadoop on a single machine\\n• \\nRecipe 2-2. Install Spark on a single machine\\n• \\nRecipe 2-3. Use the PySpark shell\\n• \\nRecipe 2-4. Install Hive on a single machine\\n• \\nRecipe 2-5. Install PostgreSQL\\n• \\nRecipe 2-6. Configure the Hive metastore on PostgreSQL\\n• \\nRecipe 2-7. Connect PySpark to Hive\\n• \\nRecipe 2-8. Install Apache Mesos\\n• \\nRecipe 2-9. Install HBase\\nI suggest that you install every piece of software on your own. It is a good exercise \\nand will give you a deeper understanding of the components of each software package.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 35}, page_content='Chapter 2 ■ Installation\\n16\\nRecipe 2-1. Install Hadoop on a Single Machine\\nProblem\\nYou want to install Hadoop on a single machine.\\nSolution\\nYou might be thinking, Why are we installing Hadoop while we are learning PySpark? \\nAre we going to use Hadoop MapReduce as a distributed framework for our problem \\nsolving? The answer is, Not at all. We are going to use two components of Hadoop: HDFS \\nand YARN—HDFS for data storage and YARN as cluster manager. Installation of Hadoop \\nrequires you to download it and configure it.\\nHow It Works\\nFollow these steps to complete the installation.\\nStep 2-1-1. Creating a New CentOS User\\nIn this step, we’ll create a new user. You might be thinking, Why a new user? Why can’t \\nwe install Hadoop on an existing user? The reason is that we want to provide a dedicated \\nuser for all the big data frameworks. With the following lines of code, we create the user \\npysparkbook:\\n[root@localhost pyspark]# adduser pysparkbook\\n[root@localhost pyspark]# passwd pysparkbook\\nThe output is as follows:\\nChanging password for user pysparkbook.\\nNew password:\\npasswd: all authentication tokens updated successfully.\\nIn the preceding code, you can see that the command adduser has been used to \\ncreate or add a user. The Linux command passwd has been used to provide a password to \\nour new user pysparkbook.\\nAfter creating the user, we have to add it to sudo. Sudo stands for superuser do. Using \\nsudo, we can run any code as a super user. Sudo is used to install software.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 36}, page_content='Chapter 2 ■ Installation\\n17\\nStep 2-1-2. Creating a new CentOS user\\nA new user is created. You might be thinking why new user. Why cant we install Hadoop \\nin existing user. The reason behind that is, we want to provide a dedicated user for \\nall the big data frameworks. In following lines of code we are going to create a user \\n“pysparkbook”.\\n[pyspark@localhost ~]$ su root\\n[root@localhost pyspark]# adduser pysparkbook\\n[root@localhost pyspark]# passwd pysparkbook\\nOutput:\\nChanging password for user pysparkbook.\\nNew password:\\npasswd: all authentication tokens updated successfully.\\nIn the preceding code, you can see that the command adduser has been used to \\ncreate or add a user. The command passwd has been used to provide a password for our \\nnew user pysparkbook to the sudo.\\n[root@localhost pyspark]# usermod -aG wheel pyspark\\n[root@localhost pyspark]#exit\\nThen we will enter to our user pysparkbook.\\n[pyspark@localhost ~]$ su pysparkbook\\nWe will create two directories. The binaries directory under the home directory will \\nbe used to download software, and the allPySpark directory under the root (/) directory \\nwill be used to install big data frameworks:\\n[pysparkbook@localhost ~]$ mkdir binaries\\n[pysparkbook@localhost ~]$ sudo\\xa0\\xa0mkdir /allPySpark\\nStep 2-1-3. Installing Java\\nHadoop, Hive, Spark and many big data frameworks use Java to run on. That’s why we  \\nare first going to install Java. We are going to use OpenJDK for this purpose; we’ll install \\nthe eighth version of OpenJDK. We can install Java on CentOS by using the yum installer, \\nas follows:\\n[pysparkbook@localhost binaries]$ sudo yum install java-1.8.0-openjdk.x86_64\\nAfter installation of any software, it is a good idea to check the installation to ensure \\nthat everything is fine.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 37}, page_content='Chapter 2 ■ Installation\\n18\\nTo check the Java installation, I prefer the command java -version:\\n[pysparkbook@localhost binaries]$ java -version\\nThe output is as follows:\\nopenjdk version \"1.8.0_111\"\\nOpenJDK Runtime Environment (build 1.8.0_111-b15)\\nOpenJDK 64-Bit Server VM (build 25.111-b15, mixed mode)\\nJava has been installed. Now we have to look for the environment variable JAVA_HOME, \\nwhich will be used by all the distributed frameworks. After installation, JAVA_HOME can be \\nfound by using jrunscript as follows:\\n[pysparkbook@localhostbinaries]$jrunscript -e \\'java.lang.System.out.\\nprintln(java.lang.System.getProperty(\"java.home\"));\\'\\nHere is the output:\\n/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64/jre\\nStep 2-1-4. Creating Passwordless Logging from pysparkbook\\nUse this command to create a passwordless login:\\n[pysparkbook@localhost binaries]$ ssh-keygen -t rsa\\nHere is the output:\\nGenerating public/private rsa key pair.\\nEnter file in which to save the key (/home/pysparkbook/.ssh/id_rsa):\\n/home/pysparkbook/.ssh/id_rsa already exists.\\nOverwrite (y/n)? y\\nEnter passphrase (empty for no passphrase):\\nEnter same passphrase again:\\nYour identification has been saved in /home/pysparkbook/.ssh/id_rsa.\\nYour public key has been saved in /home/pysparkbook/.ssh/id_rsa.pub.\\nThe key fingerprint is:\\nfd:9a:f3:9d:b6:66:f5:29:9f:b5:a5:bb:34:df:cd:6c pysparkbook@localhost.localdomain\\nThe key\\'s randomart image is:\\n+--[ RSA 2048]----+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0S .\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0.\\xa0\\xa0\\xa0\\xa0.|'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 38}, page_content=\"Chapter 2 ■ Installation\\n19\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. o.=|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0.o ++OE|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0oo.+XO*|\\n+-----------------+\\n[pysparkbook@localhost binaries]$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\\n[pysparkbook@localhost binaries]$ chmod 755 ~/.ssh/authorized_keys\\n[pysparkbook@localhost binaries]$ ssh localhost\\nHere is the output:\\nLast login: Wed Dec 21 16:17:45 2016 from localhost\\n[pysparkbook@localhost ~]$ exit\\nHere is the output:\\nlogout\\nConnection to localhost closed.\\nStep 2-1-5. Downloading Hadoop\\nWe are going to download Hadoop from the Apache website. As noted previously, we will \\ndownload all the software into the binaries directory. We’ll use the wget command to \\ndownload Hadoop:\\n[pysparkbook@localhost ~]$ cd\\xa0\\xa0binaries\\n[pysparkbook@localhost binaries]$ wget http://redrockdigimark.com/\\napachemirror/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz\\nHere is the output:\\n--2016-12-21 12:50:55--\\xa0\\xa0http://redrockdigimark.com/apachemirror/hadoop/\\ncommon/hadoop-2.6.5/hadoop-2.6.5.tar.gz\\nResolving redrockdigimark.com (redrockdigimark.com)... 119.18.61.94\\nConnecting to redrockdigimark.com (redrockdigimark.com)|119.18.61.94|:80... \\nconnected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 199635269 (190M) [application/x-gzip]\\nSaving to: 'hadoop-2.6.5.tar.gz'\\nStep 2-1-6. Moving Hadoop Binaries to the Installation Directory\\nOur installation directory is allPySpark. The downloaded software is hadoop-2.6.5.tar.gz, \\nwhich is a compressed directory. So at first we have to decompress it by using the tar \\ncommand as follows:\\n[pysparkbook@localhost binaries]$ tar xvzf hadoop-2.6.5.tar.gz\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 39}, page_content='Chapter 2 ■ Installation\\n20\\nNow we’ll move Hadoop under the allPySpark directory:\\npysparkbook@localhost binaries]$ sudo mv hadoop-2.6.5\\xa0\\xa0\\xa0/allPySpark/hadoop\\nStep 2-1-7. Modifying the Hadoop Environment File\\nWe have to make some changes in the Hadoop environment file. This file is found in the \\nHadoop configuration directory. In our case, the Hadoop configuration directory is  \\n/allPySpark/hadoop/etc/hadoop/. Use the following line of code to add JAVA_HOME to \\nthe hadoop-env.sh file:\\n[pysparkbook@localhost binaries]$ vim /allPySpark/hadoop/etc/hadoop/hadoop-env.sh\\nAfter opening the Hadoop environment file, add the following line:\\n# The java implementation to use.\\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64/jre\\nStep 2-1-8. Modifying the Hadoop Properties Files\\nIn this step, we are concerned with three properties files:\\n• \\nhdfs-site.xml: HDFS properties\\n• \\ncore-site.xml: Core properties related to the cluster\\n• \\nmapred-site.xml: Properties for the MapReduce framework\\nThese properties files are found in the Hadoop configuration directory. In the \\npreceding chapter, we discussed HDFS. You learned that HDFS has two components: \\nNameNode and DataNode. You also learned that HDFS uses data replication for fault-\\ntolerance. In our hdfs-site.xml file, we are going to set the NameNode directory by \\nusing the dfs.name.dir parameter, the DataNode directory by using the dfs.data.dir \\nparameter, and the replication factor by using the dfs.replication parameter.\\nLet’s modify hdfs-site.xml:\\n[pysparkbook@localhost binaries]$ vim /allPySpark/hadoop/etc/hadoop/hdfs-site.xml\\nAfter opening hdfs-site.xml, we have to put the following lines in that file:\\n<property>\\n\\xa0\\xa0<name>dfs.name.dir</name>\\n\\xa0\\xa0\\xa0\\xa0<value>file:/allPySpark/hdfs/namenode</value>\\n\\xa0\\xa0\\xa0\\xa0<description>NameNode location</description>\\n</property>'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 40}, page_content='Chapter 2 ■ Installation\\n21\\n<property>\\n\\xa0\\xa0<name>dfs.data.dir</name>\\n\\xa0\\xa0\\xa0\\xa0<value>file:/allPySpark/hdfs/datanode</value>\\n\\xa0\\xa0\\xa0\\xa0\\xa0<description>DataNode location</description>\\n</property>\\n<property>\\n\\xa0<name>dfs.replication</name>\\n\\xa0<value>1</value>\\n\\xa0<description> Number of block replication </description>\\n</property>\\nAfter updating hdfs-site.xml, we are going to update core-site.xml.  \\nIn core-site.xml, we are going to update only one property, fs.default.name.  \\nThis property is used to determine the host, port, and other details of the file system.  \\nAdd the following lines to core-site.xml:\\n<property>\\n\\xa0\\xa0<name>fs.default.name</name>\\n\\xa0\\xa0\\xa0\\xa0<value>hdfs://localhost:9746</value>\\n</property>\\nFinally, we are going to modify mapred-site.xml. We also are going to modify \\nmapreduce.framework.name, which will decide which runtime framework has to be used. \\nThe possible values are local, classic, or yarn. Add the following to mapred-site.xml:\\n[pysparkbook@localhost binaries]$ cp /allPySpark/hadoop/etc/hadoop/mapred-\\nsite.xml.template /allPySpark/hadoop/etc/hadoop/mapred-site.xml\\n[pysparkbook@localhost binaries]$vim /allPySpark/hadoop/etc/hadoop/mapred-site.xml\\n\\xa0<property>\\n\\xa0\\xa0<name>mapreduce.framework.name</name>\\n\\xa0\\xa0\\xa0<value>yarn</value>\\n\\xa0</property>\\nStep 2-1-9. Updating the .bashrc File\\nNext, we’ll add the following lines to the .bashrc file. Open the .bashrc file:\\n[pysparkbook@localhost binaries]$ vim\\xa0\\xa0~/.bashrc'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 41}, page_content='Chapter 2 ■ Installation\\n22\\nThen add the following lines:\\nexport HADOOP_HOME=/allPySpark/hadoop\\nexport PATH=$PATH:$HADOOP_HOME/sbin\\nexport PATH=$PATH:$HADOOP_HOME/bin\\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64/jre\\nexport PATH=$PATH:$JAVA_HOME/bin\\nThen we have to source the .bashrc file. After sourcing the file, the new updated \\nvalues will be reflected in the console.\\n[pysparkbook@localhost binaries]$ source ~/.bashrc\\nStep 2-1-10. Running the NameNode Format\\nWe have updated some property files. We are supposed to run in NameNode format so \\nthat all the changes are reflected in our framework. Use the following command to run \\nNameNode format:\\n[pysparkbook@localhost binaries]$ hdfs namenode -format\\nHere is the output:\\n16/12/22 02:38:15 INFO namenode.NameNode: STARTUP_MSG:\\n/************************************************************\\nSTARTUP_MSG: Starting NameNode\\nSTARTUP_MSG:\\xa0\\xa0\\xa0host = localhost/127.0.0.1\\nSTARTUP_MSG:\\xa0\\xa0\\xa0args = [-format]\\nSTARTUP_MSG:\\xa0\\xa0\\xa0version = 2.6.5\\n.\\n16/12/22 02:40:59 INFO util.ExitUtil: Exiting with status 0\\n16/12/22 02:40:59 INFO namenode.NameNode: SHUTDOWN_MSG:\\n/************************************************************\\nSHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1\\n************************************************************/\\nStep 2-1-11. Starting Hadoop\\nHadoop has been installed, and now we can start it. We can find the Hadoop starting \\nscript in /allPySpark/hadoop/sbin/. Although this script has been deprecated, we’ll use \\nit for this example:\\n[pysparkbook@localhost binaries]$ /allPySpark/hadoop/sbin/start-all.sh'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 42}, page_content='Chapter 2 ■ Installation\\n23\\nStep 2-1-12. Checking the Hadoop Installation\\nWe know that the jps command will show all the Java processes running on the machine. \\nHere is the command:\\n[pysparkbook@localhost binaries]$ jps\\nIf everything is fine, we will see the process running as shown here:\\n25720 NodeManager\\n25896 Jps\\n25625 ResourceManager\\n25195 NameNode\\n25292 DataNode\\n25454 SecondaryNameNode\\nCongratulations! We have finally installed Hadoop on our systems.\\nRecipe 2-2. Install Spark on a Single Machine\\nProblem\\nYou want to install Spark on a single machine.\\nSolution\\nWe are going to install the prebuilt spark-2.0.0 for Hadoop version 2.6. We can build Spark \\nfrom source code. But in this example, we are going to use the prebuilt Apache Spark.\\nHow It Works\\nFollow the steps in this section to complete the installation.\\nStep 2-2-1. Downloading Apache Spark\\nWe are going to download Spark from its mirror. We’ll use the wget command as follows:\\n[pysparkbook@localhost binaries]$ wget\\xa0\\xa0https://d3kbcqa49mib13.cloudfront.\\nnet/spark-2.0.0-bin-hadoop2.6.tgz\\nStep 2-2-2. Extracting a .tgz file of Spark\\nUse this command to extract the .tgz file:\\n[pysparkbook@localhost binaries]$ tar xvzf spark-2.0.0-bin-hadoop2.6.tgz'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 43}, page_content='Chapter 2 ■ Installation\\n24\\nStep 2-2-3. Moving the Extracted Spark Directory to /allPySpark\\nNow we have to move the extracted Spark directory under the /allPySpark location. Use \\nthis command:\\n[pysparkbook@localhost binaries]$ sudo mv spark-2.0.0-bin-hadoop2.6\\xa0\\xa0\\xa0 \\n/allPySpark/spark\\nStep 2-2-4. Changing the Spark Environment File\\nThe Spark environment file possesses all the environment variables required to run \\nSpark. We are going to set the following environment variables in an environment file:\\n• \\nHADOOP_CONF_DIR: Configuration directory of Hadoop\\n• \\nSPARK_CONF_DIR: Alternate configuration directory (default: \\n${SPARK_HOME}/conf)\\n• \\nSPARK_LOG_DIR: Stores log files (default: ${SPARK_HOME}/log)\\n• \\nSPARK_WORKER_DIR: Sets the working directory of worker \\nprocesses\\n• \\nHIVE_CONF_DIR: Used to read data from Hive\\nFirst we have to copy the spark-env.sh.template file to spark-env.sh. The Spark \\nenvironment file, spark-env.sh, is found inside spark/conf (the configuration directory \\nlocation). Here is the command:\\n[pysparkbook@localhost binaries]$ cp /allPySpark/spark/conf/spark-env.\\nsh.template /allPySpark/spark/conf/spark-env.sh\\nNow let’s open the spark-env.sh file:\\n[pysparkbook@localhost binaries]$ vim /allPySpark/spark/conf/spark-env.sh\\nNow append the following lines to the end of spark-env.sh:\\nexport HADOOP_CONF_DIR=/allPySpark/hadoop/etc/hadoop/\\nexport SPARK_LOG_DIR=/allPySpark/logSpark/\\nexport SPARK_WORKER_DIR=/tmp/spark\\nexport HIVE_CONF_DIR=/allPySpark/hive/conf\\nStep 2-2-5. Amending the .bashrc File\\nIn the .bashrc file, we have to add the Spark bin directory. Use the following command:\\n[pysparkbook@localhost binaries]$ vim\\xa0\\xa0~/.bashrc'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 44}, page_content='Chapter 2 ■ Installation\\n25\\nThen add the following lines in the .bashrc file:\\nexport SPARK_HOME=/allPySpark/spark\\nexport PATH=$PATH:$SPARK_HOME/bin\\nAfter this, source the .bashrc file:\\n[pysparkbook@localhost binaries]$ source\\xa0\\xa0~/.bashrc\\nStep 2-2-6. Starting PySpark\\nWe can start the PySpark shell by using the pyspark script. Discussion of the pyspark \\nscript will continue in the next recipe.\\n[pysparkbook@localhost binaries]$ pyspark\\nWe have completed one more successful installation. Other installations are required \\nto move through this book, but first let’s focus on the PySpark shell.\\nRecipe 2-3. Use the PySpark Shell\\nProblem\\nYou want to use the PySpark shell.\\nSolution\\nThe PySpark shell is an interactive shell for interacting with PySpark by using Python. The \\nPySpark shell can be started by using a PySpark script. The PySpark script can be found at \\nthe spark/bin location.\\nHow It Works\\nThe PySpark shell can be started as follows:\\n[pysparkbook@localhost binaries]$ pyspark'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 45}, page_content='Chapter 2 ■ Installation\\n26\\nAfter starting, PySpark will show the screen in Figure\\xa02-1.\\nFigure 2-1.\\u2002 Starting up the console screen in PySpark\\nYou can see that, after starting, PySpark displays a lot of information. It displays \\ninformation about the Python version it is using as well as the PySpark version.\\nThe >>> symbol is known to Python programmers. Whenever we start the Python \\nshell, we get this symbol. It tells us that we can now write our Python commands. \\nSimilarly, in PySpark, this symbol tells us that now we can write our Python or PySpark \\ncommands and see the results.\\nThe PySpark shell works similarly on both a single-machine installation and a cluster \\ninstallation of PySpark.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 46}, page_content=\"Chapter 2 ■ Installation\\n27\\nRecipe 2-4. Install Hive on a Single Machine\\nProblem\\nYou want to install Hive on a single machine.\\nSolution\\nWe discussed Hive in the first chapter. Now it is time to install Hive on our machines. We \\nare going to read data from Hive in PySpark in upcoming chapters.\\nHow It Works\\nFollow the steps in this section to complete the installation.\\nStep 2-4-1. Downloading Hive\\nWe can download Hive from the Apache Hive website. We download the Hive tar.gz file \\nby using the wget command as follows:\\n[pysparkbook@localhost binaries]$ wget http://www-eu.apache.org/dist/hive/\\nhive-2.0.1/apache-hive-2.0.1-bin.tar.gz\\nHere is the output:\\n100%[=============================================>] \\n139,856,338\\xa0\\xa0709KB/s\\xa0\\xa0\\xa0in 3m 5s\\xa0\\xa0\\n2016-12-26 09:34:21 (737 KB/s) - 'apache-hive-2.0.1-bin.tar.gz' saved \\n[139856338/139856338]\\nStep 2-4-2. Extracting Hive\\nWe have downloaded apache-hive-2.0.1-bin.tar.gz, a .tar.gz. So now we have to \\nextract it. We can extract it by using the tar command as follows:\\n[pysparkbook@localhost binaries]$ tar xvzf\\xa0\\xa0\\xa0apache-hive-2.0.1-bin.tar.gz\\nStep 2-4-3. Moving the Extracted Hive Directory\\n[pysparkbook@localhost binaries]$ sudo mv apache-hive-2.0.1-bin /allPySpark/hive\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 47}, page_content='Chapter 2 ■ Installation\\n28\\nStep 2-4-4. Updating hive-site.xml\\nHive is dispatched with the embedded Derby database for metastores. The Derby \\ndatabase is memory-less. Hence it is better to provide a definite location for it. We can \\nprovide that location in hive-site.xml. For that, we have to move hive-default.xml.\\ntemplate to hive-site.xml.\\n[pysparkbook@localhost binaries]$ mv /allPySpark/hive/conf/hive-default.xml.\\ntemplate /allPySpark/hive/conf/hive-default.xml.templatehive-site.xml\\nThen open hive-site.xml and update the following:\\n[pysparkbook@localhost binaries]$ vim /allPySpark/hive/conf/hive-site.xml\\nYou can add the following lines to the end of hive-site.xml or you can change \\njavax.jdo.option.ConnectionURL in the hive-site.xml file:\\n<name>javax.jdo.option.ConnectionURL</name>\\n\\xa0\\xa0\\xa0\\xa0<value>jdbc:derby:;databaseName=/allPySpark/hive/metastore/metastore_db; \\ncreate=true</value>\\nAfter that, we have to add HADOOP_HOME to the hive_env.sh file, as follows:\\n[pysparkbook@localhost binaries]$ mv /allPySpark/hive/conf/hive-env.\\nsh.template /allPySpark/hive/conf/hive-env.sh\\n[pysparkbook@localhost binaries]$ vim\\xa0\\xa0/allPySpark/hive/conf/hive-env.sh\\nAnd in hive-env.sh, add the following line:\\n# Set HADOOP_HOME to point to a specific hadoop install directory\\nHADOOP_HOME=/allPySpark/hadoop\\nStep 2-4-5. Updating the .bashrc File\\nOpen the .bashrc file. This file will stay in the home directory:\\n[pysparkbook@localhost binaries]$ vim\\xa0\\xa0~/.bashrc\\nAdd the following lines into the .bashrc file:\\n####################Hive Parameters ######################\\nexport HIVE_HOME=/allPySpark/hive\\nexport PATH=$PATH:$HIVE_HOME/bin'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 48}, page_content='Chapter 2 ■ Installation\\n29\\nNow source the .bashrc file by using following command:\\n[pysparkbook@localhost binaries]$ source ~/.bashrc\\nStep 2-4-6. Creating Data Warehouse Directories of Hive\\nNow we have to create data warehouse directories. This data warehouse directory is used \\nby Hive to place the data files.\\n[pysparkbook@localhost binaries]$hadoop fs -mkdir -p /user/hive/warehouse\\n[pysparkbook@localhost binaries]$hadoop fs -mkdir -p /tmp\\n[pysparkbook@localhost binaries]$hadoop fs -chmod g+w /user/hive/warehouse\\n[pysparkbook@localhost binaries]$hadoop fs -chmod g+w /tmp\\nThe /user/hive/warehouse directory is the Hive warehouse directory.\\nStep 2-4-7. Initiating the Metastore Database\\nSometimes it is necessary to initiate a schema. You might be thinking, a schema of \\nwhat? We know that Hive stores metadata of tables in a relational database. For the time \\nbeing, we are going to use a Derby database as the metastore database for Hive. And \\nthen, in upcoming recipes, we are going to connect our Hive to an external PostgreSQL. \\nIn Ubuntu, Hive installation works without this command. But in CentOS I found it \\nindispensable to run. Without the following command, Hive throws errors:\\n[pysparkbook@localhost\\xa0\\xa0binaries]$ schematool -initSchema -dbType derby\\nStep 2-4-8. Checking the Hive Installation\\nNow that Hive has been installed, we should check our work. Start the Hive shell by using \\nthe following command:\\n[pysparkbook@localhost binaries]$ hive\\nThen we will find that the Hive shell has been opened as follows:\\nhive>'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 49}, page_content='Chapter 2 ■ Installation\\n30\\nRecipe 2-5. Install PostgreSQL\\nProblem\\nYou want to install PostgreSQL.\\nSolution\\nPostgreSQL is a relational database management system developed at the University of \\nCalifornia. The PostgreSQL license provides permission to use, modify, and distribute \\nPostgreSQL. PostgreSQL can run on macOS and on Unix-like systems such as Red Hat \\nand Ubuntu. We are going to install it on CentOS.\\nWe are going to use our PostgreSQL in two ways. First, we’ll use PostgreSQL as a \\nmetastore database for Hive. After having an external database as a metastore, we will \\nbe able to easily read data from the existing Hive. Second, we are going to read data from \\nPostgreSQL, and after analysis, we will save our result to PostgreSQL.\\nInstalling PostgreSQL can be done with source code, but we are going to install it via \\nthe command-line yum installer.\\nHow It Works\\nFollow the steps in this section to complete the installation.\\nStep 2-5-1. Installing PostgreSQL\\nPostgreSQL can be installed using the yum installer. Here is the command:\\n[pysparkbook@localhost binaries]$ sudo yum install postgresql-server\\n[sudo] password for pysparkbook: \\nStep 2-5-2. Initializing the Database\\nTo use PostgreSQL, we first need to use the initdb utility to initialize the database. If we \\ndon’t initialize the database, we cannot use it. At the time of database initialization, we \\ncan also specify the data file of the database. After installing PostgreSQL, we have to \\ninitialize it. The database can be initialized using following command:\\n[pysparkbook@localhost binaries]$ sudo postgresql-setup initdb\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 50}, page_content='Chapter 2 ■ Installation\\n31\\nHere is the output:\\n[sudo] password for pysparkbook:\\nInitializing database ... OK\\nStep 2-5-3. Enabling and Starting the Database\\n[pysparkbook@localhost binaries]$ sudo systemctl enable postgresql\\n[pysparkbook@localhost binaries]$ sudo systemctl start postgresql\\n[pysparkbook@localhost binaries]$ sudo -i -u postgres\\nHere is the output:\\n[sudo] password for pysparkbook:\\n-bash-4.2$ psql\\npsql (9.2.18)\\nType \"help\" for help.\\npostgres=# \\n■\\n■Note\\u2003  The installation procedure is located at the following web page:\\nhttps://wiki.postgresql.org/wiki/YUM_Installation\\nRecipe 2-6. Configure the Hive Metastore on \\nPostgreSQL\\nProblem\\nYou want to configure the Hive metastore on PostgreSQL.\\nSolution\\nAs we know, Hive puts metadata of tables in a relational database. We have already \\ninstalled Hive. Our Hive installation has an embedded metastore. Hive uses the Derby \\nrelational database system for its metastore. In upcoming chapters, we will read existing \\nHive tables from PySpark.\\nConfiguring the Hive metastore on PostgreSQL requires us to populate tables in the \\nPostgreSQL database. These tables will hold the metadata of the Hive tables. After this, we \\nhave to configure the Hive property file.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 51}, page_content=\"Chapter 2 ■ Installation\\n32\\nHow It Works\\nIn this section, we are going to configure the Hive metastore on the PostgreSQL database. \\nThen our Hive will have metadata in PostgreSQL.\\nStep 2-6-1. Downloading the PostgreSQL JDBC Connector\\nWe need a JDBC connector so that the Hive process can connect to the external \\nPostgreSQL. We can get a JDBC connector by using the following command:\\n[pysparkbook@localhost binaries]$ wget https://jdbc.postgresql.org/download/\\npostgresql-9.4.1212.jre6.jar\\nStep 2-6-2. Copying the JDBC Connector to the Hive lib \\nDirectory\\nAfter getting the JDBC connector, we have to put it in the Hive lib directory:\\n[pysparkbook@localhost binaries]$ cp postgresql-9.4.1212.jre6.jar\\xa0\\xa0 \\n/allPySpark/hive/lib/\\nStep 2-6-3. Connecting to PostgreSQL\\nUse this command to connect to PostgreSQL:\\n[pysparkbook@localhost binaries]$ sudo -u postgres psql\\nStep 2-6-4. Creating the Required User and Database\\nIn this step, we are going to create a PostgreSQL user, pysparkBookUser. Then we are \\ngoing to create a database named pymetastore. Our database is going to hold all the \\ntables related to the Hive metastore.\\nFirst, create the user:\\npostgres=# CREATE USER pysparkBookUser WITH PASSWORD 'pbook';\\nHere is the output:\\nCREATE ROLE\\nNext, create the database:\\npostgres=# CREATE DATABASE pymetastore;\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 52}, page_content='Chapter 2 ■ Installation\\n33\\nHere is the output:\\nCREATE DATABASE\\nThe \\\\c PostgreSQL command stands for connect. We have created our database \\npymetastore. Now we are going to connect to this database by using our \\\\c command:\\npostgres=# \\\\c pymetastore;\\nYou are now connected to the pymetastore database. You can see more PostgreSQL \\ncommands at www.postgresql.org/docs/9.0/static/app-psql.html.\\nStep 2-6-5. Populating Data in the pymetastore Database\\nHive possess its own PostgreSQL scripts to populate tables for the metastore. The \\\\i \\ncommand reads commands from the PostgreSQL script and executes those commands. \\nThe following command runs the hive-txn-schema-2.0.0.postgres.sql script, which \\nwill create all the tables required for the Hive metastore:\\npymetastore=#\\xa0\\xa0\\\\i /allPySpark/hive/scripts/metastore/upgrade/postgres/ \\nhive-txn-schema-2.0.0.postgres.sql\\nHere is the output:\\npsql:/allPySpark/hive/scripts/metastore/upgrade/postgres/hive-txn-schema-\\n2.0.0.postgres.sql:30: NOTICE:\\xa0\\xa0CREATE TABLE / PRIMARY KEY will create \\nimplicit index \"txns_pkey\" for table \"txns\"\\nCREATE TABLE\\nCREATE TABLE\\nINSERT 0 1\\npsql:/allPySpark/hive/scripts/metastore/upgrade/postgres/hive-txn-schema-\\n2.0.0.postgres.sql:69: NOTICE:\\xa0\\xa0CREATE TABLE / PRIMARY KEY will create \\nimplicit index \"hive_locks_pkey\" for table \"hive_locks\"\\nCREATE TABLE\\nStep 2-6-6. Granting Permissions\\nThe following commands will grant some permissions:\\npymetastore=# grant select, insert,update,delete on public.txns to \\npysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.txn_components to \\npysparkBookUser;\\nGRANT'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 53}, page_content='Chapter 2 ■ Installation\\n34\\npymetastore=# grant select, insert,update,delete on public.completed_txn_\\ncomponents\\xa0\\xa0\\xa0to pysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.next_txn_id to \\npysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.hive_locks to \\npysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.next_lock_id to \\npysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.compaction_queue \\nto pysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.next_compaction_\\nqueue_id to pysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.completed_\\ncompactions to pysparkBookUser;\\nGRANT\\npymetastore=# grant select, insert,update,delete on public.aux_table to \\npysparkBookUser;\\nGRANT\\nStep 2-6-7. Changing the pg_hba.conf File\\nRemember that in order to update pg_hba.conf, you are supposed to be the root user. So \\nfirst become the root user. Then open the pg_hba.conf file:\\n[root@localhost binaries]# vim /var/lib/pgsql/data/pg_hba.conf\\nThen change all the peers and indent them to trust:\\n#local\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0peer\\nlocal\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0trust\\n# IPv4 local connections:\\n#host\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0127.0.0.1/32\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ident\\nhost\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0127.0.0.1/32\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0trust'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 54}, page_content='Chapter 2 ■ Installation\\n35\\n# IPv6 local connections:\\n#host\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0::1/128\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ident\\nhost\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0all\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0::1/128\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0trust\\nYou can find more details about this change at http://stackoverflow.com/\\nquestions/2942485/psql-fatal-ident-authentication-failed-for-user-postgres.\\nCome out of root user. \\nStep 2-6-8. Testing Our User\\nNext, we’ll test that we are easily able to enter our database as our created user:\\n[pysparkbook@localhost binaries]$ psql -h localhost -U pysparkbookuser -d \\npymetastore\\nHere is the output:\\npsql (9.2.18)\\nType \"help\" for help.\\npymetastore=>\\nStep 2-6-9. Modifying Our hive-site.xml\\nWe can modify the Hive-related configuration in the configuration file hive-site.xml. \\nWe have to modify the following properties:\\n• \\njavax.jdo.option.ConnectionURL: Connection URL to the \\ndatabase\\n• \\njavax.jdo.option.ConnectionDriverName: Connection JDBC \\ndriver name\\n• \\njavax.jdo.option.ConnectionUserName: Database connection user\\n• \\njavax.jdo.option.ConnectionPassword: Connection password\\nEither modify these properties or add the following lines at the end of the Hive \\nproperty file to get the required result:\\n<property>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<name>javax.jdo.option.ConnectionURL</name>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<value>jdbc:postgresql://localhost/pymetastore</value>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<description>postgreSQL server metadata store</description>\\n\\xa0</property>\\n\\xa0<property>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<name>javax.jdo.option.ConnectionDriverName</name>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<value>org.postgresql.Driver</value>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<description>Driver class of postgreSQL</description>\\n\\xa0</property>'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 55}, page_content=\"Chapter 2 ■ Installation\\n36\\n\\xa0\\xa0<property>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<name>javax.jdo.option.ConnectionUserName</name>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<value>pysparkbookuser</value>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<description>User name to connect to postgreSQL</description>\\n\\xa0</property>\\n\\xa0<property>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<name>javax.jdo.option.ConnectionPassword</name>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<value>pbook</value>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<description>password for connecting to PostgreSQL server</\\ndescription>\\n\\xa0</property>\\nStep 2-6-10. Starting Hive\\nWe have connected Hive to an external relational database management system. So now \\nit is time to start Hive and check that everything is fine. First, start Hive:\\n[pysparkbook@localhost binaries]$ hive\\nOur activities will be reflected in PostgreSQL. Let’s create a database and a table \\ninside the database. We’ll create the database apress and the table apressBooks via the \\nfollowing commands:\\nhive> create database apress;\\nHere is the output:\\nOK\\nTime taken: 1.397 seconds\\nhive> use apress;\\nHere is the output:\\nOK\\nTime taken: 0.07 seconds\\nhive> create table apressBooks (\\n\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0bookName String,\\n\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0bookWriter String\\n\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0row format delimited\\n\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fields terminated by ',';\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 56}, page_content='Chapter 2 ■ Installation\\n37\\nHere is the output:\\nOK\\nTime taken: 0.581 seconds\\nStep 2-6-11. Testing Creation of Metadata in PostgreSQL\\nThe created database and table will be reflected in PostgreSQL. We can see the updated \\ndata in the TBLS table as follows:\\npymetastore=> SELECT * from \"TBLS\";\\nTBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME |\\xa0\\xa0\\xa0\\xa0OWNER\\xa0\\xa0\\xa0\\xa0| RETENTION | \\nSD_ID |\\xa0\\xa0TBL_NAME\\xa0\\xa0\\xa0|\\n\\xa0\\xa0\\xa0TBL_TYPE\\xa0\\xa0\\xa0\\xa0| VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT\\n--------+-------------+-------+------------------+-------------+-----------\\n+-------+-------------+\\n---------------+--------------------+--------------------\\n\\xa0\\xa0\\xa0\\xa0\\xa01 |\\xa0\\xa01482892229 |\\xa0\\xa0\\xa0\\xa0\\xa06 |\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00 | pysparkbook |\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00 \\n|\\xa0\\xa0\\xa0\\xa01 | apressbooks |\\n\\xa0MANAGED_TABLE |\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n(1 row)\\nThe significant work needed to connect Hive to an external database is done. In the \\nfollowing recipe, we are going to install Apache Mesos. \\nRecipe 2-7. Connect PySpark to Hive\\nProblem\\nYou want to connect PySpark to Hive.\\nSolution\\nPySpark needs the Hive property file to know the configuration parameters of Hive. The \\nHive property file, hive-site.xml, stays in the Hive configuration directory. Copy the \\nHive property file to the Spark configuration directory. Then we will be finished and we \\ncan start PySpark.\\nHow It Works\\nTwo steps have been identified to connect PySpark to Hive.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 57}, page_content=\"Chapter 2 ■ Installation\\n38\\nStep 2-7-1. Copying the Hive Property File to the Spark Conf \\nDirectory\\nUse this command to copy the Hive property file:\\n[pysparkbook@localhost binaries]$cp /allPySpark/hive/conf/hive-site.xml /\\nallPySpark/spark/\\nStep 2-7-2. Starting PySpark\\nUse this command to start PySpark:\\n[pysparkbook@localhost binaries]$pyspark\\nRecipe 2-8. Install Apache Mesos\\nProblem\\nYou want to install Apache Mesos.\\nSolution\\nInstalling Apache Mesos requires downloading the code and then configuring it.\\nHow It Works\\nFollow the steps in this section to complete the installation.\\nStep 2-8-1. Downloading Apache Mesos\\nUse this command to obtain Apache Mesos:\\n[pysparkbook@localhost binaries]$ wget http://www.apache.org/dist/\\nmesos/1.1.0/mesos-1.1.0.tar.gz\\nThe output is as follows:\\n--2016-12-28 08:15:14--\\xa0\\xa0http://www.apache.org/dist/mesos/1.1.0/mesos-\\n1.1.0.tar.gz\\nResolving www.apache.org (www.apache.org)... 88.198.26.2, 140.211.11.105, \\n2a01:4f8:130:2192::2\\nConnecting to www.apache.org (www.apache.org)|88.198.26.2|:80... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 41929556 (40M) [application/x-gzip]\\nSaving to: 'mesos-1.1.0.tar.gz.1'\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 58}, page_content=\"Chapter 2 ■ Installation\\n39\\n100%[=============================================>] \\n41,929,556\\xa0\\xa0\\xa0226KB/s\\xa0\\xa0\\xa0in 58s\\xa0\\xa0\\xa0\\xa0\\n2016-12-28 08:16:23 (703 KB/s) - 'mesos-1.1.0.tar.gz.1' saved \\n[41929556/41929556]\\nStep 2-8-2. Extracting Mesos from .tar.gz\\nTo extract Mesos, use this command:\\n[pysparkbook@localhost binaries]$ tar xvzf mesos-1.1.0.tar.gz\\nStep 2-8-3. Installing Repo to Install Maven\\nTo install Maven, we first need to install the repo:\\n[pysparkbook@localhost binaries]$ sudo bash -c 'cat > /etc/yum.repos.d/\\nwandisco-svn.repo <<EOF\\n> [WANdiscoSVN]\\n> name=WANdisco SVN Repo 1.9\\n> enabled=1\\n> baseurl=http://opensource.wandisco.com/centos/7/svn-1.9/RPMS/$basearch/\\n> gpgcheck=1\\n> gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco\\n> EOF'\\nStep 2-8-4. Installing Dependencies of Maven\\nIt is time to install the dependencies required to install Maven:\\n[pysparkbook@localhost binaries]$ sudo yum install -y apache-maven python-\\ndevel java-1.8.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-\\nsasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel\\nStep 2-8-5. Downloading Apache Maven\\nNow we’re ready to download Maven:\\n[pysparkbook@localhost binaries]$ wget http://www-us.apache.org/dist/maven/\\nmaven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 59}, page_content='Chapter 2 ■ Installation\\n40\\nHere is the output:\\n100%[===================================================>] \\n8,491,533\\xa0\\xa0\\xa0\\xa0274KB/s\\xa0\\xa0\\xa0in 87s\\xa0\\xa0\\xa0\\xa0\\n2016-12-28 23:47:40 (95.5 KB/s) - \\'apache-maven-3.3.9-bin.tar.gz\\' saved \\n[8491533/8491533]\\nStep 2-8-6. Extracting the Maven Directory\\nAs with other software, we have to extract and move the Maven directory:\\n[pysparkbook@localhost binaries]$ tar -xvzf apache-maven-3.3.9-bin.tar.gz\\n[pysparkbook@localhost binaries]$ mv apache-maven-3.3.9 /allPySpark/maven\\nWe then have to link the mvn file:\\n[pysparkbook@localhost binaries]$ sudo ln -s /allPySpark/maven/bin/mvn /usr/\\nbin/mvn\\n[sudo] password for pysparkbook: \\nStep 2-8-7. Checking the Maven Installation\\nThe best way to check our installation is to run the version command:\\n[pysparkbook@localhost binaries]$ mvn -version\\nHere is the output:\\nApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-\\n10T22:11:47+05:30)\\nMaven home: /allPySpark/maven\\nJava version: 1.8.0_111, vendor: Oracle Corporation\\nJava home: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64/jre\\nDefault locale: en_US, platform encoding: UTF-8\\nOS name: \"linux\", version: \"3.10.0-514.2.2.el7.x86_64\", arch: \"amd64\", \\nfamily: \"unix\"'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 60}, page_content=\"Chapter 2 ■ Installation\\n41\\nStep 2-8-9. Configuring Mesos\\nWe have to move to the Mesos build directory. Then we have to run configure for the script.\\n[pysparkbook@localhost build]$ ../configure\\n----------------------------------------------------------------------------\\nFollowing lines will come as notes\\n----------------------------------------------------------------------------\\nYou may get errors as follows:\\nmake[2]: *** [../3rdparty/protobuf-2.6.1/python/dist/protobuf-2.6.1-\\npy2.7.egg] Error 1\\nmake[2]: Leaving directory '/allPySpark/mesos/build/src'\\nmake[1]: *** [all] Error 2\\nmake[1]: Leaving directory '/allPySpark/mesos/build/src'\\nmake: *** [all-recursive] Error 1\\nIf you do get these errors, you need to perform the following installations and \\nupgrade pytz:\\n[pysparkbook@localhost build]$sudo yum install python-setuptools python-\\nsetuptools-devel\\n[pysparkbook@localhost build]$sudo easy_install pip\\n[pysparkbook@localhost build]$sudo pip install --upgrade pytz\\n----------------------------------------------------------------------------\\nStep 2-8-10. Running Make\\n[pysparkbook@localhost build]$make\\nInstalling build/bdist.linux-x86_64/wheel/mesos.scheduler-1.1.0-py2.7-nspkg.\\npth\\nrunning install_scripts\\ncreating build/bdist.linux-x86_64/wheel/mesos.scheduler-1.1.0.dist-info/\\nWHEEL\\nmake[2]: Leaving directory '/allPySpark/mesos/build/src'\\nmake[1]: Leaving directory '/allPySpark/mesos/build/src'\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 61}, page_content='Chapter 2 ■ Installation\\n42\\nStep 2-8-11. Running make install\\nUse this command to install Mesos:\\n[pysparkbook@localhost build]$make install\\nStep 2-8-12. Starting Mesos Master\\nAfter successful installation of Mesos, we can start the master by using the following \\ncommand:\\n[pysparkbook@localhost build]$ mesos-master --work_dir=/allPySpark/mesos/workdir\\nStep 2-8-13. Starting Mesos Slaves\\nUse this command to start the slave on the same machine:\\n[root@localhost binaries]#mesos-slave --master=127.0.0.1:5050 --work_dir=/\\nallPySpark/mesos/workdir --systemd_runtime_directory=/allPySpark/mesos/systemd\\nIn an upcoming chapter, you will see how to start the PySpark shell on Mesos.\\nRecipe 2-9. Install HBase\\nProblem\\nYou want to install Apache HBase.\\nSolution\\nHBase is a NoSQL database, as we discussed in Chapter 1. We are going to install \\nHBase. Then we will read data from this HBase installation by using Spark. We will go \\nfor the simplest installation of HBase. We can download HBase from the HBase website \\n(https://hbase.apache.org/) and then configure it.\\nHow It Works\\nFollow the steps in this section to complete the installation.\\nStep 2-9-1. Obtaining HBase\\nUse the following command to download HBase in our binaries directory:\\n[pysparkbook@localhost binaries]$ wget http://www-eu.apache.org/dist/hbase/\\nstable/hbase-1.2.4-bin.tar.gz'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 62}, page_content='Chapter 2 ■ Installation\\n43\\nStep 2-9-2. Extracting HBase\\n[pysparkbook@localhost binaries]$ tar xzf\\xa0\\xa0hbase-1.2.4-bin.tar.gz\\n[pysparkbook@localhost binaries]$ sudo mv hbase-1.2.4 /usr/local/hbase\\nStep 2-9-3. Updating the HBase Environment File\\nHBase also looks for JAVA_HOME. So we’ll update the HBase environment file with JAVA_HOME:\\n[pysparkbook@localhost binaries]$ vim /allPySpark/hbase/conf/hbase-env.sh\\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64/\\nStep 2-9-4. Creating the HBase Directory in HDFS\\nEven HBase can put its data on a local machine, in the case of a single-machine \\ninstallation. But to make things clearer about the workings of HBase, we are going to \\ncreate a directory on HDFS for HBase. First, we have to start HDFS. Use the following \\ncommands to create a directory in HDFS:\\n[pysparkbook@localhost binaries]$/allPySpark/hadoop/sbin/start-dfs.sh\\n[pysparkbook@localhost binaries]$/allPySpark/hadoop/sbin/start-yarn.sh\\n[pysparkbook@localhost binaries]$hadoop fs -mkdir /hbase\\nStep 2-9-5. Updating the HBase Property File and .bashrc\\nLet’s start with updating the property file, and then we will update the .bashrc file. In the \\nHBase property file, we are going to update hbase:rootdir. The HBase property file stays \\nin the HBase configuration directory. For us, the HBase configuration directory is  \\n/allPySpark/hbase/conf.\\n[pysparkbook@localhost binaries]$ vim /allPySpark/hbase/conf/hbase-site.xml\\nNow add the following lines in the hbase-site.xml file:\\n<property>\\n<name>hbase:rootdir</name>\\n<value>hdfs://localhost:9746/hbase</value>\\n</property>\\nIt is time to update .bashrac, as shown here:\\n[pysparkbook@localhost binaries]$vim ~/.bashrc'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 63}, page_content='Chapter 2 ■ Installation\\n44\\nAdd the following lines in .bashrc:\\nexport HBASE_HOME=/allPySpark/hbase\\nexport PATH=$PATH:$HBASE_HOME/bin\\n[pysparkbook@localhost binaries]$ source\\xa0\\xa0~/.bashrc\\nStep 2-9-6. Starting HBase and the HBase Shell\\n[pysparkbook@localhost binaries]$ /allPySpark/hbase/bin/start-hbase.sh\\n[pysparkbook@localhost binaries]$/allPySpark/hbase/bin/hbase shell\\nHere is the output:\\nhbase(main):001:0>'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 64}, page_content='45\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_3 \\nCHAPTER 3\\nIntroduction to Python  \\nand NumPy\\nPython is a general-purpose, high-level programming language. It was developed by \\nGuido van Rossum, and since its inception, its popularity has increased exponentially. \\nA plethora of handy and high-performance packages for numerical and statistical \\ncalculations make Python popular among data scientists. Python is an indented \\nlanguage, which may bother programmers just learning Python. But indented code \\nimproves code readability, and the popularity of Python and its ease of use makes it good \\nfor programming Spark.\\nThis chapter introduces the basics of Python programming. We will also discuss \\nNumPy. In addition, I have included a recipe on IPython and on integrating IPython with \\nPySpark.\\nIf you know Python well, you can skip this chapter. But my suggestion is to go \\nthrough the content regardless., because this chapter might still provide good insight and \\nboost your knowledge further.\\nThis chapter covers the following recipes:\\nRecipe 3-1. Create data and verify the data type\\nRecipe 3-2. Create and index a Python string\\nRecipe 3-3. Typecast from one data type to another\\nRecipe 3-4. Work with a Python list\\nRecipe 3-5. Work with a Python tuple\\nRecipe 3-6. Work with a Python set\\nRecipe 3-7. Work with a Python dictionary\\nRecipe 3-8. Work with Define and Call functions\\nRecipe 3-9. Work with Create and Call lambda functions\\nRecipe 3-10. Work with Python conditionals\\nRecipe 3-11. Work with Python for and while loops'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 65}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n46\\nRecipe 3-12. Work with NumPy\\nRecipe 3-13. Integrate IPython and IPython Notebook with \\nPySpark\\nRecipe 3-1. Create Data and Verify the Data Type\\nProblem\\nYou want to create data and verify the data type.\\nSolution\\nPython is dynamically typed language. What does that mean? Dynamically typed means \\nthat at the time of variable definition, the programmer is not supposed to mention data \\ntypes, as we do in other programming languages such as C. To learn about Python data \\ntypes, you want to do the following:\\n• \\nCreate an integer and verify its data type\\n• \\nCreate a long integer and verify its data type\\n• \\nCreate a decimal and verify its data type\\n• \\nCreate a Boolean and verify its data type\\nThe Python interpreter interprets the data type whenever a literal is mentioned in the \\nconsole or in a Python script. The Python interpreter interprets the data type of a literal \\nwhen it is assigned to a variable. In order to verify the data type of a literal or variable, you \\ncan use the Python type() function.\\nHow It Works\\nLet’s follow the steps in this section to create data and verify data types.\\nStep 3-1-1. Creating an Integer and Verifying Its Data Type\\nLet’s create an integer in Python. In following line of code, we associate the Python \\nvariable pythonInt with the value 15:\\n>>> pythonInt = 15\\n>>> pythonInt\\nHere is the output:\\n15'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 66}, page_content=\"Chapter 3 ■ Introduction to Python and NumPy \\n47\\nWe can verify the type of a Python object by using the type() function:\\n>>> type(pythonInt)\\nHere is the output:\\n<type 'int'>\\nStep 3-1-2. Creating a Long Integer and Verifying Its Data Type\\nThe Long data type is used for large integers. At the time of creation, the number is \\nsuffixed by L. Creation of a Long data type is shown here:\\n>>> pythonLongInt = 15L\\n>>> pythonLongInt\\nHere is the output:\\n15L\\nUsing the type() function, we can see that data type of pythonLongInt is long.\\n>>> type(pythonLongInt)\\nHere is the output:\\n<type 'long'>\\nStep 3-1-3. Creating a Decimal and Verifying Its Data Type\\nDecimal numbers are used a lot in any numerical computation. Let’s create a  \\nfloating-point number:\\n>>> pythonFloat = 15.4\\n>>> pythonFloat\\nHere is the output:\\n15.4\\nAnd now let’s check its type:\\n>>> type(pythonFloat)\\nHere is the output:\\n<type 'float'>\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 67}, page_content=\"Chapter 3 ■ Introduction to Python and NumPy \\n48\\nStep 3-1-4. Creating a Boolean and Verifying Its Data Type\\nA programmer’s life is filled with concerns about various conditions, such as whether a \\ngiven number is greater than five. Here’s an example:\\n>>> pythonBool = True\\n>>> pythonBool\\nThe output is shown here:\\nTrue\\n>>> type(pythonBool)\\nHere is the output:\\n<type 'bool'>\\n>>> pythonBool = False\\n>>> pythonBool\\nHere is the output:\\nFalse\\nRecipe 3-2. Create and Index a Python String\\nProblem\\nYou want to create and index a Python string.\\nSolution\\nNatural language processing and other string-related problems. You want to do the \\nfollowing:\\n• \\nCreate a string and verify its data type\\n• \\nIndex a string\\n• \\nVerify whether a substring lies in a given string\\n• \\nCheck whether a string starts with a given substring\\n• \\nCheck whether a string ends with a given substring\\nYou can create a string by using either a set of double quotes (“ ”) or a set of single \\nquotes (‘ ’). Indexing can be done with a set of square brackets. You can use various ways \\nto verify whether a substring is in a given string. The find() function can be used to find \\nwhether a substring will stay inside a string.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 68}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n49\\nThe startswith() function indicates whether a given string starts with a given \\nsubstring. Similarly, the endswith() function can confirm whether a given string ends \\nwith given substring.\\nHow It Works\\nThe steps in this section will solve our problem.\\nStep 3-2-1. Creating a String and Verifying Its Data Type\\nTo create a string, you must put the given string literal between either double quotes or \\nsingle quotes. Always remember to not use mixed quotes, meaning having a single quote \\non one side and a double quote on the other side. The following lines of code create \\nstrings by using double and single quotes. Let’s start with double quotes:\\n>>> pythonString\\xa0\\xa0= \"PySpark Recipes\"\\n>>> pythonString\\nHere is the output:\\n\\'PySpark Recipes\\'\\nYou can use the following code to verifying the type:\\n>>> type(pythonString)\\nHere is the output:\\n<type \\'str\\'>\\nNow let’s create a string by using single quotes:\\n>>> pythonString\\xa0\\xa0= \\'PySpark Recipes\\'\\n>>> pythonString\\nHere is the output:\\n\\'PySpark Recipes\\'\\nStep 3-2-2. Indexing a String\\nString elements can be indexed. When you index a string, you start from zero:\\n>>> pythonStr = \"Learning PySpark is fun\"\\n>>> pythonStr\\nHere is the output:'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 69}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n50\\n\\'Learning PySpark is fun\\'\\nLet’s get the string at the tenth location. Let me mention again that string indexes \\nstart with zero:\\n>>> pythonStr[9]\\nHere is the output:\\n\\'P\\'\\nStep 3-2-3. Verifying That a Substring Lies in a Given String\\nIf a substring is found inside a string, the Python string find() function will return the \\nlowest index of the matching substring in a string. But if the given substring is not found \\nin a given string, the find() method returns –1. Let’s index the element at the ninth \\nposition:\\n>>> pythonStr[9]\\nHere is the output:\\n\\'P\\'\\nWe are going to search to see whether the substring Py is in our string pythonStr. \\nWe’ll use the find() method:\\n>>> pythonStr.find(\"Py\")\\nHere is the output:\\n9\\n>>> pythonStr.find(\"py\")\\nHere is the output:\\n-1\\nWe can see that the output is 9. This is the index where Py is started.\\nStep 3-2-4. Checking Whether a String Starts with a Given \\nSubstring\\nNow let’s focus on another important function, startswith(). This function ensures that \\na given string starts with a particular substring. If a given string starts with the mentioned \\nsubstring, then it returns True; otherwise, it returns False. Here is an example:'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 70}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n51\\n>>> pythonStr.startswith(\"Learning\")\\nHere is the output:\\nTrue\\nStep 3-2-5. Checking Whether a String Ends with a Given Substring\\nSimilarly, the endswith() function indicates whether a given string ends with given \\nsubstring.\\n>>> pythonStr.endswith(\"fun\")\\nHere is the output:\\nTrue\\nRecipe 3-3. Typecast from One Data Type to Another\\nProblem\\nYou want to typecast from one data type to another.\\nSolution\\nTypecasting from one data type to another is a general activity in data analysis. The \\nfollowing set of problems will help you to understand typecasting in Python. You want to \\ndo the following:\\n• \\nTypecast an integer number to a float\\n• \\nTypecast a string to an integer\\n• \\nTypecast a string to a float\\nTypecasting means changing one data type to another—for example, changing a \\nstring to an integer or float. In PySpark, we’ll often typecast one data type to another. \\nThere are four important functions in Python for typecasting one data type to another: \\nint(), float(), long(), and str().\\nHow It Works\\nWe’ll follow the steps in this section to solve the given problems.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 71}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n52\\nStep 3-3-1. Typecasting an Integer Number to a Float\\nLet’s start with creating an integer:\\n>>> pythonInt = 17\\nWe have created an integer variable, pythonInt. The type of a Python object can be \\nfound by using the type() function:\\n\\xa0>>> type(pythonInt)\\nHere is the output:\\n<type \\'int\\'>\\nWe can see clearly that the type() function has returned int.\\nTo typecast any data type to float, we use the float() function:\\n>>> pythonFloat = float(pythonInt)\\nThe pythonInt value has been changed to float. We can see the change by printing \\nthe variable:\\n>>> print pythonFloat\\nHere is the output:\\n17.0\\nPerforming the type() function on pythonFloat will ensure that the integer value \\nhas been typecasted to a floating number :\\n>>> type(pythonFloat)\\nHere is the output:\\n<type \\'float\\'>\\nStep 3-3-2. Typecasting a String to an Integer\\nThe function int() typecasts a String, Float, or Long type to an Integer. Typecasting a \\nstring to an integer will come up again and again in our problem-solving by PySpark. \\nWe’ll start by creating a string:\\n>>> pythonString = \"15\"\\n>>> type(pythonString)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 72}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n53\\nHere is the output:\\n<type \\'str\\'>\\nIn the preceding Python code snippet, we have created a string, and we also checked \\nthe type of our created variable pythonString.\\nThe Python built-in int() function can typecast a variable to an integer. The \\nfollowing code uses an int() function and changes a string to an integer:\\n>>> pythonInteger = int(pythonString)\\n>>> pythonInteger\\nHere is the output:\\n15\\n>>> type(pythonInteger)\\nHere is the output:\\n<type \\'int\\'>\\nStep 3-3-3. Typecasting a String to a Float\\nLet’s solve our last question of typecasting. Let’s create a string with the value 15.4, and \\ntypecast it to a float by using the float() function:\\n>>> pythonString = \"15.4\"\\n>>> type(pythonString)\\nHere is the output:\\n<type \\'str\\'>\\nNext, we’ll typecast our string to a floating-point number:\\n>>> pythonFloat = float(pythonString)\\n>>> pythonFloat\\nHere is the output:\\n15.4\\n>>> type(pythonFloat)\\nHere is the output:\\n<type \\'float\\'>'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 73}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n54\\nThere are four types of collections in Python. The four types are list, set, dictionary, \\nand tuple. In the following recipes, we are going to discuss these collections one by one.\\nRecipe 3-4. Work with a Python List\\nProblem\\nYou want to work with a Python list\\nSolution\\nA list is an ordered Python collection. A list is used in many problems. We are going to \\nconcentrate on the following problems:\\n• \\nCreating a list\\n• \\nExtending a list\\n• \\nAppending a list\\n• \\nCounting the number of elements in a list\\n• \\nSorting a list\\nA Python list is mutable. Generally, we create a list with objects of a similar type. But \\na list can be created of different object types. A list is created using square brackets, [ ].  \\nA List object has many built-in functions to work with. Extending a list can be done by \\nusing the extend() function. Appending a list can be done using the append() function. \\nYou might be wondering, what is the difference between appending a list and extending a \\nlist? Soon you are going to get the answer.\\nCounting the number of elements in a list can be done by using the len() function, \\nand sorting by using the sort() function.\\nHow It Works\\nUse the following steps to solve our problem.\\nStep 3-4-1. Creating a List\\nLet’s create a list:\\n>>> pythonList = [2.3,3.4,4.3,2.4,2.3,4.0]\\n>>> pythonList\\nHere is the output:\\n[2.3, 3.4, 4.3, 2.4, 2.3, 4.0]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 74}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n55\\nA list can be indexed by using square brackets, [ ]. The first element of a list is \\nindexed with 0:\\n>>> pythonList[0]\\nHere is the output:\\n2.3\\n>>> pythonList[1]\\nHere is the output:\\n3.4\\npythonList[0] indexes the first element of the list pythonList, and pythonList[1] \\nindexes the second element. Hence if a list has n elements, the last element can be \\nindexed as pythonList[n-1].\\nA list of elements can be of a different data type. The following example will  \\nmake it clear:\\n>>> pythonList1 = [\"a\",1]\\nThe preceding line creates a list with two elements. The first element of the list is a \\nstring, and the second element is an integer.\\n>>> pythonList1\\nHere is the output:\\n[\\'a\\', 1]\\n>>> type(pythonList1)\\nHere is the output:\\n<type \\'list\\'>\\nThe type() function outputs the type of pythonList1 as list:\\n>>> type (pythonList1[0])\\nHere is the output:\\n<type \\'str\\'>\\n>>> type (pythonList1[1])'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 75}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n56\\nHere is the output:\\n<type \\'int\\'>\\nThe type of the first element is a string, and the second element is an integer, which \\nis being shown by the type() function.\\nA list is a mutable collection in Python. A mutable collection means the elements of \\nthe collection can be changed. Let’s look at some examples:\\n>>>pythonList1 = [\"a\",1]\\n>>> pythonList1[0] = 5\\n>>> print\\xa0\\xa0pythonList1\\nHere is the output:\\n[5, 1]\\nIn this example, we used the same list, pythonList1, which we created. Then the first \\nelement is changed to 5. And we can see by printing pythonList1 that the first element of \\npythonList1 is 5 now.\\nStep 3-4-2. Extending a List\\nThe extend() function takes an object as an argument and extends the calling list object \\nwith object in argument element wise:\\n>>>pythonList1 = [5,1]\\n>>>print\\xa0\\xa0pythonList1\\nHere is the output:\\n[5, 1]\\n>>> pythonList1.extend([1,2,3])\\n>>> pythonList1\\n[5, 1, 1, 2, 3]\\nStep 3-4-3. Appending a List\\nApplying append() to a list will append the Python object that has been provided as an \\nargument to the function. In this example, append() has just appended another list to the \\nexisting list:\\n>>>pythonList1 = [5,1]\\n>>> pythonList1.append([1,2,3])\\n>>> print\\xa0\\xa0pythonList1'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 76}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n57\\nHere is the output:\\n[5, 1, [1, 2, 3]]\\nStep 3-4-4. Counting the Number of Elements in a List\\nThe length of a list is the number of elements in the list. The len() function will return the \\nlength of a list as follows:\\n>>>pythonList1 = [5,1]\\n>>> len(pythonList1)\\nOutput:\\n2\\nStep 3-4-5. Sorting a List\\nSorting a list can be done in an increasing or decreasing fashion. Our sort() function can \\nbe applied in the following ways to sort a given list in either ascending or descending order.\\nLet’s start with sorting our list pythonList in an ascending order in the following code:\\n>>> pythonList = [2.3,3.4,4.3,2.4,2.3,4.0]\\n>>> pythonList\\nHere is the output:\\n[2.3, 3.4, 4.3, 2.4, 2.3, 4.0]\\n>>> pythonList.sort()\\n>>> pythonList\\nHere is the output:\\n[2.3, 2.3, 2.4, 3.4, 4.0, 4.3]\\nIn order to sort data in descending order, we have to provide the reverse argument  \\nas True:\\n>>> pythonList.sort(reverse=True)\\n>>> pythonList\\nHere is the output:\\n[4.3, 4.0, 3.4, 2.4, 2.3, 2.3]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 77}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n58\\nRecipe 3-5. Work with a Python Tuple\\nProblem\\nYou want to work with a Python tuple.\\nSolution\\nA tuple is an immutable ordered collection. We are going to solve the following set  \\nof problems:\\n• \\nCreating a tuple\\n• \\nGetting the index of an element of a tuple\\n• \\nCounting the occurrence of a tuple element\\nA tuple, an immutable collection, is generally used to create record data. A tuple \\nis created by using a set of parentheses, ( ). A tuple is an ordered sequence. We can \\nput different data types together in a tuple. The index() function on a tuple object \\nwill provide us the index of the first occurrence of a given element. Another function, \\ncount(), defined on a tuple will return the frequency of a given element.\\nHow It Works\\nLet’s work out the solution step-by-step.\\nStep 3-5-1. Creating a Tuple\\nThe following code creates a tuple by using parentheses, ( ):\\n>>>pythonTuple = (2.0,9,\"a\",True,\"a\")\\nHere we have created a tuple, pythonTuple, which has five elements. The first \\nelement of our tuple is a decimal number, the second element is an integer, the third one \\nis a string, the fourth one is a Boolean, and last one is a string.\\nNow let’s check the type of the pythonTuple object:\\n>>> type(pythonTuple)\\nHere is the output:\\n<type \\'tuple\\'>'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 78}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n59\\nIndexing a tuple is done in a similar way as we indexed the list, but this time we use \\nsquare brackets, [ ]:\\n>>> pythonTuple[2]\\nHere is the output:\\n\\'a\\'\\nThis next line of code will show that the tuple is immutable. We are \\n>>> pythonTuple[1] = 5\\nHere is the output:\\nTraceback (most recent call last):\\n\\xa0\\xa0File \"<stdin>\", line 1, in <module>\\nTypeError: \\'tuple\\' object does not support item assignment\\nWe can see that we cannot modify the elements of a tuple.\\nStep 3-5-2. Getting the Index of a Tuple Element\\nThe index() function requires an element as an argument and returns the index of the \\nfirst occurrence of a value in a given tuple. In our tuple pythonTuple, \\'a\\' is at index 2. We \\ncan get this index as follows:\\n>>> pythonTuple.index(\\'a\\')\\nHere is the output:\\n2\\nIf a value is not found in a given tuple, the index() function throws an exception.\\nStep 3-5-3. Counting the Occurrences of a Tuple Element\\nProviding an element as input in a count() function returns the frequency of that element \\nin a given tuple. We can use the count() function in the following way:\\n>>> pythonTuple.count(\"a\")\\nHere is the output:\\n2'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 79}, page_content=\"Chapter 3 ■ Introduction to Python and NumPy \\n60\\nIn our tuple pythonTuple, the character a has occurred twice. Therefore, the count() \\nfunction has returned 2.\\nYou can try a simple exercise: apply count() on that tuple with 1 as an argument . \\nYou will get 1 as the answer.\\nRecipe 3-6. Work with a Python Set\\nProblem\\nYou want to work with a Python set.\\nSolution\\nDealing with a collection of distinct elements requires us to use a set. We are going to \\nwork on the following tasks:\\n• \\nCreating a set\\n• \\nAdding a new element to a set\\n• \\nPerforming a union on sets\\n• \\nPerforming an intersection operation on sets\\nA set cannot have duplicate elements. A Python set is created using set of curly \\nbrackets, { }. The most important point to note is that, at the time of creation, we can put \\nduplicate items in a set. But the set will then remove all the duplicate items from it. As \\nwith other collections, many functions have been defined on the set object too. If you \\nwant to add a new element to a set, you can use the add() function.\\nUnionizing two sets is a common activity that we’ll find in our day-to-day tasks. The \\nunion() function, which has been defined on our set object, will unionize two sets for us.\\nThe intersect() function is used to run an intersection on two given Python sets.\\nHow It Works\\nIn this section, we will solve our given problems step-by-step.\\nStep 3-6-1. Creating a Set\\nLet’s create a set of stationary items and then verify the existence of distinct elements. \\nThis code line creates a set of stationary items:\\n>>> pythonSet = {'Book','Pen','NoteBook','Pencil','Book'}\\n>>> pythonSet\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 80}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n61\\nHere is the output:\\nset([\\'Pencil\\', \\'Pen\\', \\'Book\\', \\'NoteBook\\'])\\nWe have created a set. We can observe that putting in a duplicate element doesn’t \\nthrow an error, but the set will not consider that duplicate element while creating the set.\\nStep 3-6-2. Adding a New Element to a Set\\nBy using the add() function, we can add a new element to the set. We have already \\ncreated the set pythonSet. Now we are going to add a new element, Eraser, to our set, as \\nshown here:\\n>>> pythonSet.add(\"Eraser\")\\n>>> pythonSet\\nHere is the output:\\nset([\\'Pencil\\', \\'Pen\\', \\'Book\\', \\'Eraser\\', \\'NoteBook\\'])\\nYou can see in this example that we have added the new element Eraser to our set.\\nStep 3-6-3. Performing a Union on Sets\\nA union operation on a Python set will behave in a mathematical way. Let’s create \\nanother set for this example:\\n>>> pythonSet1 = {\\'NoteBook\\',\\'Pencil\\',\\'Diary\\',\\'Marker\\'}\\n>>> pythonSet1\\nHere is the output:\\nset([\\'Marker\\', \\'Pencil\\', \\'NoteBook\\', \\'Diary\\'])\\nA union of two sets will return another set with all the elements either in any set or \\ncommon to both sets. In the following example, we can see that the union of pythonSet \\nand pythonSet1 has returned all the merged elements in pythonSet and pythonSet1:\\n>>> pythonSet.union(pythonSet1)\\nHere is the output:\\nset([\\'Pencil\\', \\'Pen\\', \\'NoteBook\\', \\'Book\\', \\'Eraser\\',\\xa0\\xa0\\'Diary\\', \\'Marker\\'])'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 81}, page_content=\"Chapter 3 ■ Introduction to Python and NumPy \\n62\\nStep 3-6-4. Performing an Intersection Operation on Sets\\nThe intersection() function will return a new set with common elements from two \\ngiven sets. We have already created two sets, pythonSet and pythonSet1. We can observe \\nthat Pencil and NoteBook are common elements in our sets. In following line, we use \\nintersection() on our sets:\\n>>> pythonSet.intersection(pythonSet1)\\nHere is the output:\\nset(['Pencil', 'NoteBook'])\\nAfter running the code, it is clear that the intersection will return the elements that \\nare common to both sets.\\nRecipe 3-7. Work with a Python Dictionary\\nProblem\\nYou want to work with a Python dictionary.\\nSolution\\nYou have seen that lists and tuples are indexed by their index numbers. This situation \\nbecomes clearer when the index uses words. A Python dictionary is a data structure that \\nstores key/value pairs. Each element of a Python dictionary is a key/value pair. In this \\nexercise, you want to do the following operations on a Python dictionary:\\n• \\nCreate a dictionary of stationary items, with the item ID as the key \\nand the item name as the value\\n• \\nIndex an element using a key\\n• \\nGet\\xa0all the keys\\n• \\nGet\\xa0all the values\\nThe creation of a dictionary can be achieved by using set of curly brackets, { }. You \\nmight be a little confused that we created a Python set using curly brackets and now we \\nare going to create a dictionary in the same way. But let me tell you that, in order to create \\na dictionary, we have to provide a key/value pair inside the curly brackets—for example, \\n{key:value}. We can observe that, in a key/value pair, the key is separated from the value \\nby a colon (:). Two different key/value pairs are separated by a comma (,).\\nYou can index a dictionary by using square brackets, [ ]. A Python dictionary object \\nhas a get() function, which returns the value for a given key.\\nYou can get\\xa0all keys by using the keys() function; the values() function returns all \\nthe values.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 82}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n63\\nHow It Works\\nWe’ll use the steps in this section to solve our problem.\\nStep 3-7-1. Creating a Dictionary of Stationary Items\\nWe’ll create a dictionary with the following line of Python code:\\n>>> pythonDict = {\\'item1\\':\\'Pencil\\',\\'item2\\':\\'Pen\\', \\'item3\\':\\'NoteBook\\'}\\n>>> pythonDict\\nHere is the output:\\n{\\'item2\\': \\'Pen\\', \\'item3\\': \\'NoteBook\\', \\'item1\\': \\'Pencil\\'}\\nStep 3-7-2. Indexing an Element by Using a Key\\nLet’s fetch the value of \\'item1:\\'\\n>>> pythonDict[\\'item1\\']\\nHere is the output:\\n\\'Pencil\\'\\nBut if the key is not found in the dictionary, a KeyError exception is thrown:\\n>>> pythonDict[\\'item4\\']\\nHere is the output:\\nTraceback (most recent call last):\\n\\xa0\\xa0File \"<stdin>\", line 1, in <module>\\nKeyError: \\'item4\\'\\nTo prevent a KeyError exception, we can use the get() function on a dictionary. \\nThen, if a value is not found for a given key, it returns nothing (and otherwise returns the \\nvalue associated with that key). Let’s explore get() in the following Python code snippet:\\n>>> pythonDict.get(\\'item1\\')\\nHere is the output:\\n\\'Pencil\\''),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 83}, page_content=\"Chapter 3 ■ Introduction to Python and NumPy \\n64\\nWe know that the key item4 is not in our dictionary. Therefore, the Python get() \\nfunction will return nothing in this case:\\n>>> pythonDict.get('item4')\\nStep 3-7-3. Getting All the Keys\\nGetting all the keys together is often required in problem-solving. Let’s get\\xa0all the keys by \\nusing our keys() function:\\n>>> pythonDict.keys()\\nHere is the output:\\n['item2', 'item3', 'item1']\\nFunction keys() returns all the keys in a list.\\nStep 3-7-4. Getting All the Values\\nApplying the value() function will return all the values in a dictionary:\\n>>> pythonDict.values()\\nHere is the output:\\n['Pen', 'NoteBook', 'Pencil']\\nSimilar to the keys() function, values() also returns all the values in a list as output.\\nRecipe 3-8. Work with Define and Call Functions\\nProblem\\nYou want to write a Python function that takes an integer as input and returns True if the \\ninput is an even number, and otherwise returns False.\\nSolution\\nFunctions improve code readability and reusability. In Python, a function definition \\nis started with the def keyword. The user-defined name of a function follows that def \\nkeyword. If a function takes an argument, it is written within parentheses; otherwise, we \\nwrite just the set of parentheses. The colon symbol (:) follows the parentheses. Then we \\nstart writing the statements as the body of the function. The body is indented with respect \\nto the line containing the def keyword. If our user-defined function returns a value, that is \\nachieved by using the return Python keyword.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 84}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n65\\nHow It Works\\nWe are going to create a function named isEvenInteger. The following lines define  \\nour function:\\n>>> def isEvenInteger(ourNum) :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return ourNum %2 == 0\\nIn this code example, we can see that our function name is isEvenInteger. Our \\nfunction name follows the def keyword. The argument to our function, which is ourNum \\nin this case, is within parentheses. After the colon, we define our function body. In this \\nfunction body, there is only one statement. The function body statement is returning \\nthe output of a logical expression. We return a value from a function by using the return \\nkeyword. If a function doesn’t return any value, the return keyword is omitted. The \\nlogical expression of the function body is checking whether modules of the input number \\nequal zero or not.\\nAfter defining our function isEvenInteger, we should call it so we can watch it work. \\nIn order to test our function working, we’ll call it with an even argument and then with \\nan odd integer argument. Let’s start with the even integer input. Here we provide input 4, \\nwhich is an even integer:\\n\\xa0>>> isEvenInteger(4)\\nHere is the output:\\nTrue\\nAs we can see, the output is True. This means that the input we have provided is an \\neven number.\\nNow let’s provide an odd number as input:\\n>>> isEvenInteger(5)\\nHere is the output:\\nFalse\\nWith an odd number as the input, our function isEvenInteger has returned False.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 85}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n66\\nRecipe 3-9. Work with Create and Call Lambda \\nFunctions\\nProblem\\nYou want to create a lambda function, which takes an integer as input and returns True if \\nthe input is an even number, and otherwise return False.\\nSolution\\nA lambda function is also known as an anonymous function. A lambda function is a \\nfunction without a name. A lambda function in Python is defined with the lambda keyword.\\nHow It Works\\nLet’s create a lambda function that will check whether a given number is an even number:\\n>>> isEvenInteger = lambda ourNum : ourNum%2 == 0\\nOur lambda function is lambda ourNum : ourNum%2 == 0. You can see that we have \\nused the keyword lambda to define it. After the keyword lambda, the arguments of the \\nfunction have been written. Then after the colon, we have a logical expression. Our logical \\nexpression will check whether the input number is divisible by 2. In Python, functions are \\nobjects, so our lambda function is also an object. Therefore, we can put our object in a \\nvariable. That is how we can put our lambda function in the variable isEvenInteger:\\n>>> isEvenInteger(4\\nHere is the output:\\nTrue\\n>>> isEvenInteger(5)\\nHere is the output:\\nFalse\\nThe drawback of lambda functions is that only one expression can be written. The \\nresult of that particular expression will be returned. The Python keyword return is not \\nrequired to return the value of an expression. A lambda function cannot be extended \\nbeyond more than one line'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 86}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n67\\nRecipe 3-10. Work with Python Conditionals\\nProblem\\nYou want to work with Python conditionals.\\nSolution\\nA car manufacturing company manufactures three types of cars. These three types of cars \\nare differentiated by their number of cylinders. It is obvious that the different number of \\ncylinders results in different gas mileage. Car A has four cylinders, car B has six cylinders, \\nand car C has eight cylinders. Table\\xa03-1 shows the number of cylinders and respective gas \\nmileage of the cars.\\nTable 3-1.\\u2002 Number of Cylinders and Respective Gas Mileage\\nNumber of Cylinders\\nGas Mileage (Miles per Gallon)\\n4\\n22\\n6\\n18\\n8\\n16\\nMr A, a sales executive, always forgets about this relationship between the number of \\ncylinders and the gas mileage. So you want to create a small Python application that will \\nreturn the mileage, given the number of cylinders. This will be a great help to Mr. A.\\nConditionals are imperative in solving any programming problem. Python is no \\nexception. Conditionals are implemented in Python by using if, elif, and else keywords.\\nHow It Works\\nTo create the required Python application, we are going to write a Python function that \\nwill take the number of cylinders as input and return the associated mileage.\\nHere’s the function we’ll use to create our application:\\n>>> def mpgFind(numOfCylinders) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if(numOfCylinders == 4 ):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0mpg = 22\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0elif(numOfCylinders == 6 ):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0mpg = 18\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0else :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0mpg = 16\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return mpg'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 87}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n68\\nHere we have defined a function named mpgFind. Our function will take the variable \\nnumOfCylinders. Entering into function our variable going to face a logical expression \\nnumOfCylinders == 4. If numOfCylinders has the value 4, the logical expression will \\nreturn True and the if block will be executed. Our if block has only one statement, \\nmpg=22. If required, more than one statement can be provided.\\nIf numOfCylinders == 4 results in False, the elif logical expression will be tested. \\nIf the value of numOfCylinders is 6, the logical expression of elif will return True, and 18 \\nwill be assigned to mpg.\\nWhat if the logical expression of if and elif both return False? In this condition, the \\nelse block will be executed. The variable mpg will be assigned 16.\\nLet’s call our function with the input 4 and see the result:\\n>>> mpgFind(4)\\nHere is the output:\\n22\\nYou can clearly see that our function can help our sales executive, Mr A.\\nRecipe 3-11. Work with Python “for” and  \\n“while” Loops\\nProblem\\nYou want to work with the Python for and while loops.\\nSolution\\nAfter learning that his application was written in Python, sales executive Mr A became \\ninterested in learning Python. He joined a Python class. His instructor, Mr X, gave him an \\nassignment to solve. Mr X asked his class to implement a Python function that will take a \\nlist of integers from 1 to 101 as input and then return the sum of the even numbers in the \\nlist. Mr A found that the required function can be implemented using the following:\\n• \\nfor loop\\n• \\nwhile loop\\nLoops are best for code reusability. We use a loop to run a segment of code many \\ntimes. A particular segment of code can be run using for and while loops. To get a \\nsummation of even numbers in a list, we require two steps: first we have to find the even \\nnumbers, and then we have to do the summations.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 88}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n69\\nHow It Works\\nStep 3-11-1. Implementing a for Loop\\nFirst, we are going to get the summation of even numbers in a list by using a for loop  \\nin Python:\\n\\xa0>>> def sumUsingForLoop(numbers) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sumOfEvenNumbers = 0\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for i in numbers :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if i % 2 ==0 :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sumOfEvenNumbers = sumOfEvenNumbers +i\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0else :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pass\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return\\xa0\\xa0\\xa0sumOfEvenNumbers\\nHere we have a written a Python function named sumUsingForLoop. Our function \\ntakes as input numbers. A for loop in Python iterates over a Python sequence. If we send \\na list of integers, the for loop starts iterating element by element. The if block will check \\nwhether the number being considered is an even number. If the number is even, the \\nvariable sumOfEvenNumbers will be increased by this number, using summation. After \\ncompletion of the iteration, the final sum will be returned.\\nLet’s check the working of our function:\\n>>> numbers = range(1,102)\\n>>> numbers\\nHere is the output:\\n[1, 2, 3, 4, 5, 6, 7,............,\\xa0\\xa098, 99, 100, 101]\\nWe have created a list of integers, from 1 to 101, using the range() function. Let’s call \\nour function sumUsingForLoop and see the result:\\n>>> sumUsingForLoop(numbers)\\nHere is the output:\\n2550\\nStep 3-11-2. Implementing a while Loop\\nIn this section, we are going to implement a solution to our problem that uses a while loop:\\n>>> def sumUsingWhileLoop(numbers) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0i = 1\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sumOfEvenNumbers = 0\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while i <= 101 :'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 89}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n70\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if i % 2 ==0 :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sumOfEvenNumbers = sumOfEvenNumbers +i\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0i = i + 1\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return\\xa0\\xa0sumOfEvenNumbers\\nHere we have defined a Python function named sumUsingWhileLoop. Our function \\ntakes as input numbers. A while loop in Python helps to iterate over a Python sequence. If \\nwe send a list of integers, we can iterate over our list, element by element, as shown in the \\ncode. The if block will check whether the number in consideration is an even number. \\nIf the number is even, the variable sumOfEvenNumbers will be increased by this number, \\nusing summation. After completion of the iteration, the final sum will be returned.\\nLet’s test our function:\\n>>> sumUsingWhileLoop(numbers)\\nHere is the output:\\n2550\\nRecipe 3-12. Work with NumPy\\nProblem\\nYou want to work with NumPy.\\nSolution\\nCompany XYZ wants to build its new factory at location A. The company needs a location \\nthat has a temperature meeting certain specific criteria. Environmental scientist Mr. Y \\ngathers the temperature reading at site A for five days at different times.\\nTable\\xa03-2 depicts the temperature readings.\\nTable 3-2.\\u2002 Temperatures in Celsius'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 90}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n71\\nYou want to do the following:\\n• \\nInstall pip\\n• \\nInstall NumPy\\n• \\nCreate a two-dimensional array using the NumPy array() \\nfunction\\n• \\nCreate a two-dimensional array using vertical and column \\nstacking of smaller arrays\\n• \\nKnow and change the data type of array elements\\n• \\nKnow shape of a given array\\n• \\nCalculate minimum and maximum temperature for each day\\n• \\nCalculate minimum and maximum temperature column-wise\\n• \\nCalculate the mean temperature for each day and column-wise\\n• \\nCalculate the standard deviation of temperature for each day and \\ncolumn-wise\\n• \\nCalculate the variance of temperature for each day and column-wise\\n• \\nCalculate the median temperature for each day and column-wise\\n• \\nCalculate the overall mean from all the gathered temperature data\\n• \\nCalculate the variance and standard deviation over all five days of \\ntemperature data\\nThese sorts of simple mathematical questions can be solved easily by using NumPy. \\nYou might be thinking that we can solve these problems by using nested lists, so why are \\nwe going to use NumPy? Looping becomes faster with the NumPy ndarray. NumPy is \\nopen source and easily available.\\nThe NumPy ndarray is a higher-level abstraction for multidimensional array data.  \\nIt also provides many functions for working on those multidimensional arrays. In order  \\nto work with NumPy, we first have to install it. After creating a two-dimensional array of \\ngiven temperature data, we can apply many NumPy functions to solve the given problems.\\nHow It Works\\nWe’ll use the following steps to solve our problem.\\nStep 3-12-1. Installing pip\\npip is a Python package management system that is written in Python itself. We can use \\npip to install other Python packages. Using the yum installer, we can install pip as follows:\\n[pysparkbook@localhost ~]$ sudo yum install python-pip'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 91}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n72\\nAfter installing pip, we have to install pyparsing. Run the following command to \\ninstall pyparsing:\\n[pysparkbook@localhost ~]$ sudo\\xa0\\xa0yum install ftp://mirror.switch.ch/pool/4/\\nmirror/centos/7.3.1611/cloud/x86_64/openstack-kilo/common/pyparsing-2.0.3-1.\\nel7.noarch.rpm\\nStep 3-12-2. Installing NumPy\\nAfter installing pip, it can be used to install NumPy. The following command installs \\nNumPy on our machine:\\n[pysparkbook@localhost ~]$ sudo pip install numpy\\nHere is the output:\\nCollecting numpy\\n\\xa0\\xa0Downloading numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl (16.5MB)\\n\\xa0\\xa0\\xa0\\xa0100% |████████████████████████████████| 16.5MB 64kB/s\\nInstalling collected packages: numpy\\nSuccessfully installed numpy-1.12.0\\nStep 3-12-3. Creating a Two-Dimensional Array by Using array(\\xa0)\\nA multidimensional array can be created in many ways. In this step, we are going to create \\na two-dimensional array by using a nested Python list. So let’s start creating a daily list of \\ntemperature data. In the following chunk of code, we are creating five lists for five days of \\ntemperatures:\\n>>> import numpy as NumPy\\n>>> temp1 = [15, 16, 17, 17, 18, 17, 16, 14]\\n>>> temp2 = [14, 15, 17, 17, 16, 17, 16, 15]\\n>>> temp3 = [16, 15, 17, 18, 17, 16, 15, 14]\\n>>> temp4 = [16, 17, 18, 19, 17, 15, 15, 14]\\n>>> temp5 = [16, 15, 17, 17, 17, 16, 15, 13]\\nThe variable temp1 has the temperature measurements of the first day. Similarly, \\ntemp2, temp3, temp4, and temp5 have measurements of the temperature on the second, \\nthird, fourth, and fifth day, respectively.\\nOur two-dimensional array of temperatures can be created by using the NumPy \\narray() function as follows:\\n>>> dayWiseTemp = NumPy.array([temp1,temp2,temp3,temp4,temp5])\\n>>> dayWiseTemp'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 92}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n73\\nHere is the output:\\narray([[15, 16, 17, 17, 18, 17, 16, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[14, 15, 17, 17, 16, 17, 16, 15],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 15, 17, 18, 17, 16, 15, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 17, 18, 19, 17, 15, 15, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 15, 17, 17, 17, 16, 15, 13]])\\nNow we have created a two-dimensional array.\\nStep 3-12-4. Creating a Two-Dimensional Array by Stacking\\nWe can create an array by using vertical stacking and column stacking of data. First, we \\nare going to create our same temperature array data by using vertical stacking. Vertical \\nstacking can be created by using the NumPy vstack() function:\\n>>> dayWiseTemp = NumPy.vstack((temp1,temp2,temp3,temp4,temp5))\\n>>> dayWiseTemp\\nHere is the output:\\narray([[15, 16, 17, 17, 18, 17, 16, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[14, 15, 17, 17, 16, 17, 16, 15],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 15, 17, 18, 17, 16, 15, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 17, 18, 19, 17, 15, 15, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 15, 17, 17, 17, 16, 15, 13]])\\nNow let’s see how to do horizontal stacking. Temperature data has been collected at \\ndifferent times and on different days. Let’s now create a temperature data list based on time:\\n>>> d6am = NumPy.array([15, 14, 16, 16, 16])\\n>>> d8am = NumPy.array([16, 15, 15, 17, 15])\\n>>> d10am = NumPy.array([17, 17, 17, 18, 17])\\n>>> d12am = NumPy.array([17, 17, 18, 19, 17])\\n>>> d2pm = NumPy.array([18, 16, 17, 17, 17])\\n>>> d4pm = NumPy.array([17, 17, 16, 15, 16])\\n>>> d6pm = NumPy.array([16, 16, 15, 15, 15])\\n>>> d8pm = NumPy.array([14, 15, 14, 14, 13])\\nColumn stacking can be done with the NumPy column_stack() function as follows:\\n>>> dayWiseTemp = NumPy.column_stack((d6am,d8am,d10am,d12am,d2pm,d4pm,d6pm,d8pm))\\n>>> dayWiseTemp'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 93}, page_content=\"Chapter 3 ■ Introduction to Python and NumPy \\n74\\nHere is the output:\\narray([[15, 16, 17, 17, 18, 17, 16, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[14, 15, 17, 17, 16, 17, 16, 15],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 15, 17, 18, 17, 16, 15, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 17, 18, 19, 17, 15, 15, 14],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[16, 15, 17, 17, 17, 16, 15, 13]])\\nStep 3-12-5. Knowing and Changing the Data Type of  \\nArray Elements\\nThe NumPy array dtype attribute will return the data type of a NumPy array:\\n>>> dayWiseTemp.dtype\\nHere is the output:\\ndtype('int64')\\nWe can observe that NumPy has inferred the data type as int64. From the previous \\nhistorical temperature data of the given location, we know that the temperature will \\ngenerally vary between 0 and 100. Therefore, using a 64-bit integer is not efficient. We can \\nuse 32-bit integers, which will take less memory than 64-bit integers.\\nIn order to create an array with the data type int32, we can provide the dtype \\nargument for the array() function:\\n>>> dayWiseTemp = NumPy.array([temp1,temp2,temp3,temp4,temp5],dtype='int32')\\n>>> dayWiseTemp.dtype\\nHere is the output:\\ndtype('int32')\\nBut if the array has been created using the default data type, no worries. The data \\ntype of the array elements can be changed by using the NumPy astype() function. We \\ncan change the data type of an existing array by using the astype function as follows:\\n>>> dayWiseTemp = dayWiseTemp.astype('int32')\\n>>> dayWiseTemp.dtype\\nHere is the output:\\ndtype('int32')\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 94}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n75\\nStep 3-12-6. Knowing the Dimensions of an Array\\nThe shape of an array can be calculated by using the shape attribute of the array:\\n>>> dayWiseTemp.shape\\nHere is the output:\\n(5, 8)\\nThe output clearly indicates that our array has five rows and eight columns.\\nStep 3-12-7. Calculating the Minimum and Maximum \\nTemperature Each Day\\nFor a NumPy array, we can use the min() function to calculate the minimum. We can \\ncalculate the minimum of an array’s data either by row or by column. To calculate the \\nminimum temperature value of a row, we have to set the value of the axis argument to 1.\\nIn our case, the data in a row indicates the temperature during one day. The \\nfollowing line of code will compute the minimum temperature value during a day:\\n>>> dayWiseTemp.min(axis=1)\\nHere is the output:\\narray([14, 14, 14, 14, 13], dtype=int32)\\nIn similar fashion, the daily maximum temperature can be calculated by using the \\nNumPy array max() function:\\n>>> dayWiseTemp.max(axis=1)\\nHere is the output:\\narray([18, 17, 18, 19, 17], dtype=int32)\\nStep 3-12-8. Calculating the Minimum and Maximum \\nTemperature by Column\\nWe can get the minimum and maximum temperature of a column by using the same \\nmin() and max() functions, respectively. But now we have to set the axis argument to 0, \\nas follows:\\n>>> dayWiseTemp.min(axis=0)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 95}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n76\\nHere is the output:\\narray([14, 15, 17, 17, 16, 15, 15, 13], dtype=int32)\\n>>> dayWiseTemp.max(axis=0)\\nHere is the output:\\narray([16, 17, 18, 19, 18, 17, 16, 15], dtype=int32)\\nStep 3-12-9. Calculating the Mean Temperature Each Day and \\nby Column\\nNow we are going to calculate the mean value of the daily temperatures. Let’s start by \\ncalculating the mean temperature of a day:\\n>>> dayWiseTemp.mean(axis=1)\\nHere is the output:\\narray([ 16.25,\\xa0\\xa015.875,\\xa0\\xa016.,\\xa0\\xa016.375,\\xa0\\xa015.75 ])\\nIn order to calculate the mean temperature by column, we have to set the axis \\nargument to 0:\\n>>> dayWiseTemp.mean(axis=0)\\nHere is the output:\\narray([ 15.4,\\xa0\\xa015.6,\\xa0\\xa017.2,\\xa0\\xa017.6,\\xa0\\xa017.,\\xa0\\xa016.2,\\xa0\\xa015.4,\\xa0\\xa014. ])\\nStep 3-12-10. Calculating the Standard Deviation of \\nTemperature Each Day\\nLet’s start with calculating the daily standard deviation of temperature. We are going to \\ncalculate the standard deviation by using the std() function of the ndarray class:\\n>>> dayWiseTemp.std(axis=1)\\nHere is the output:\\narray([ 1.19895788,\\xa0\\xa01.05326872,\\xa0\\xa01.22474487,\\xa0\\xa01.57619003,\\xa0\\xa01.29903811])'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 96}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n77\\nThe column’s standard deviation can be calculated by using the std() function with \\nthe value of axis set to 0:\\n>>> dayWiseTemp.std(axis=0)\\nHere is the output:\\narray([ 0.8,\\xa0\\xa00.8,\\xa0\\xa00.4,\\xa0\\xa00.8,\\xa0\\xa00.63245553,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.74833148,\\xa0\\xa00.48989795,\\xa0\\xa00.63245553])\\nStep 3-12-11. Calculating the Variance of Temperature Each Day\\nThe NumPy array var() function can be used to calculate variance per row. Let’s start \\nwith calculating the daily temperature variance:\\n>>> dayWiseTemp.var(axis=1)\\nHere is the output:\\narray([ 1.4375,\\xa0\\xa01.109375,\\xa0\\xa01.5,\\xa0\\xa02.484375,\\xa0\\xa01.6875\\xa0\\xa0])\\nThe temperature variance of columns can be calculated as follows:\\n>>> dayWiseTemp.var(axis=0)\\nHere is the output:\\narray([ 0.64,\\xa0\\xa00.64,\\xa0\\xa00.16,\\xa0\\xa00.64,\\xa0\\xa00.4 ,\\xa0\\xa00.56,\\xa0\\xa00.24,\\xa0\\xa00.4 ])\\nStep 3-12-12. Calculating Daily and Hourly Medians\\nThe median can be calculated by using the NumPy median() function as follows:\\n>>>NumPy.median(dayWiseTemp,axis=1)\\nHere is the output:\\narray([ 16.5,\\xa0\\xa016. ,\\xa0\\xa016. ,\\xa0\\xa016.5,\\xa0\\xa016. ])\\n>>> NumPy.median(dayWiseTemp,axis=0)\\nHere is the output:\\narray([ 16.,\\xa0\\xa015.,\\xa0\\xa017.,\\xa0\\xa017.,\\xa0\\xa017.,\\xa0\\xa016.,\\xa0\\xa015.,\\xa0\\xa014.])'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 97}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n78\\nStep 3-12-13. Calculating the Overall Mean of all the Gathered \\nTemperature Data\\nThe NumPy mean() function can be used to calculate the overall mean of all data:\\n>>> NumPy.mean(dayWiseTemp)\\nHere is the output:\\n16.050000000000001\\nStep 3-12-14. Calculating the Variance and Standard Deviation \\nover All Five Days of Temperature Data\\nThe NumPy var() function can be used to calculate the variance of all the gathered data:\\n>>> NumPy.var(dayWiseTemp)\\nHere is the output:\\n1.6974999999999993\\nAnd the std() function can be used to calculate the standard deviation:\\n>>> NumPy.std(dayWiseTemp)\\nHere is the output:\\n1.3028814220795379\\n■\\n■Note\\u2003  You can learn more about NumPy at www.numpy.org.\\nRecipe 3-13. Integrate IPython and IPython \\nNotebook with PySpark\\nProblem\\nYou want to integrate IPython and IPython Notebook with PySpark.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 98}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n79\\nSolution\\nIntegrating IPython with PySpark improves programmer efficiency. To accomplish this \\nintegration, we’ll do the following:\\n• \\nInstall IPython\\n• \\nIntegrate PySpark with IPython\\n• \\nInstall IPython Notebook\\n• \\nIntegrate PySpark with IPython Notebook\\n• \\nRun PySpark commands on IPython Notebook\\nYou have become familiar with the Python interactive shell. This interactive shell \\nenables us to learn Python faster, because we can see the result of each command, line \\nby line. The Python shell that comes with Python is very basic. It does not come with tab \\ncompletion and other features. A more advanced Python interactive shell is IPython. It \\nhas many advanced features that facilitate coding.\\nIPython Notebook start a web-browser-based facility to write Python code. \\nGenerally, readers may be confused that for a web-browser-based notebook, we need \\nan Internet connection. That’s not the case; we can run IPython Notebook without an \\nInternet connection.\\nWe can start PySpark with IPython and IPython Notebook.\\nIt is time to install IPython and IPython Notebook and integrate PySpark with those. \\nIPython and IPython Notebook can be installed using pip.\\nHow It Works\\nIn order to connect PySpark, we have to perform the following steps.\\nStep 3-13-1. Installing IPython\\nLet’s install IPython first. We have already installed pip. Pip can be used to install IPython \\nwith the following command:\\n[pysparkbook@localhost ~]$ sudo pip install ipython\\nHere is the output:\\nCollecting ipython\\nSuccessfully installed ipython-5.2.2 pathlib2-2.2.1 pexpect-4.2.1 \\npickleshare-0.7.4 prompt-toolkit-1.0.13 ptyprocess-0.5.1 scandir-1.4 \\nsimplegeneric-0.8.1 wcwidth-0.1.7'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 99}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n80\\nStep 3-13-2. Integrating PySpark with IPython\\nAfter installation, we are going to start PySpark with IPython. This is easy to do. First, we \\nset the environmental variable IPYTHON equal to 1, as follows:\\n[pysparkbook@localhost ~]$ export IPYTHON=1\\n[pysparkbook@localhost ~]$ pyspark\\nAfter starting PySpark, you’ll see the shell in Figure\\xa03-1.\\nFigure 3-1.\\u2002 Shell\\nIn Figure\\xa03-1, you can see that now In[1] has replaced the legacy >>> symbol of our \\nold Python console.\\nStep 3-13-3. Installing IPython Notebook\\nNow we are going to install IPython Notebook. Again, let’s use pip to install IPython:\\n[pysparkbook@localhost ~]$ sudo pip install ipython[notebook]\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 100}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n81\\nStep 3-13-4. Integrating PySpark with IPython Notebook\\nAfter installation, we have to set some environment variables:\\n[pysparkbook@localhost ~]$ export IPYTHON_OPTS=\"notebook\"\\n[pysparkbook@localhost ~]$ export XDG_RUNTIME_DIR=\"\"\\nNow it is time to start PySpark:\\n[pysparkbook@localhost ~]$ pyspark\\nAfter starting PySpark with IPython Notebook, we’ll see the web browser shown in \\nFigure\\xa03-2.\\nFigure 3-2.\\u2002 After starting PySpark with Notebook, you’ll see a web browser\\nYou might be amazed that the browser is showing Jupyter. But this isn’t amazing; \\nJupyter is the new name for IPython Notebook. You can see how easy it is to work with \\nNotebook; by clicking the Upload button, we can upload files to the machine.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 101}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n82\\nFigure 3-3.\\u2002 Creating a new notebook using Python 2\\nFigure 3-4.\\u2002 After creating the notebook, you’ll see this web page\\nNow we can run the Python commands to create a list. After writing our command, \\nwe have to run it. You can see in Figure\\xa03-4 that we can run our command by using the \\nRun button in the notebook.\\nStep 3-13-5. Running PySpark Commands on IPython Notebook\\nIn order to run the PySpark command, let’s create a new notebook by using Python 2,  \\nas shown in Figure\\xa03-3.\\nAfter creating the notebook, you will see the web page in Figure\\xa03-4.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 102}, page_content='Chapter 3 ■ Introduction to Python and NumPy \\n83\\nFigure 3-5.\\u2002 Printing the Python list\\nIn Figure\\xa03-5, we are printing pythonList.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 103}, page_content='85\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_4 \\nCHAPTER 4\\nSpark Architecture and the \\nResilient Distributed Dataset\\nYou learned Python in the preceding chapter. Now it is time to learn PySpark and utilize the \\npower of a distributed system to solve problems related to big data. We generally distribute \\nlarge amounts of data on a cluster and perform processing on that distributed data.\\nThis chapter covers the following recipes:\\nRecipe 4-1. Create an RDD\\nRecipe 4-2. Convert temperature data\\nRecipe 4-3. Perform basic data manipulation\\nRecipe 4-4. Run set operations\\nRecipe 4-5. Calculate summary statistics\\nRecipe 4-6. Start PySpark shell on Standalone cluster manager\\nRecipe 4-7. Start PySpark shell on Mesos\\nRecipe 4-8. Start PySpark shell on YARN\\nLearning about the architecture of Spark will be very helpful to your understanding \\nof the various components of Spark. Before delving into the recipes let’s explore this topic.\\nFigure\\xa04-1 describes the Spark architecture.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 104}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n86\\nThe main components of the Spark architecture are the driver and executors. For \\neach PySpark application, there will be one driver program and one or more executors \\nrunning on the cluster slave machine. You might be wondering, what is an application in \\nthe context of PySpark? An application is a whole bunch of code used to solve a problem.\\nThe driver is the process that coordinates with many executors running on various \\nslave machines. Spark follows a master/slave architecture. The SparkContext object is \\ncreated by the driver. SparkContext is the main entry point to a PySpark application. You \\nwill learn more about SparkContext in upcoming chapters. In this chapter, we will run \\nour PySpark commands in the PySpark shell. After starting the shell, we will find that \\nthe SparkContext object is automatically created. We will encounter the SparkContext \\nobject in the PySpark shell as the variable sc. The shell itself is working as our driver. \\nThe driver breaks our application into small tasks; a task is the smallest unit of your \\napplication. Tasks are run on different executors in parallel. The driver is also responsible \\nfor scheduling tasks to different executors.\\nExecutors are slave processes. An executor runs tasks. It also has the capability to \\ncache data in memory by using the BlockManager process. Each executor runs in its own \\nJava Virtual Machine (JVM).\\nThe cluster manager manages cluster resources. The driver talks to the cluster \\nmanager to negotiate resources. The cluster manager also schedules tasks on behalf of \\nthe driver on various slave executor processes. PySpark is dispatched with Standalone \\nCluster Manager. PySpark can also be configured on YARN and Apache Mesos. In our \\nrecipes, you are going to see how to configure PySpark on Standalone Cluster Manager \\nand Apache Mesos. On a single machine, PySpark can be started in local mode too.\\nThe main celebrated component of PySpark is the resilient distributed dataset (RDD). \\nThe RDD is a data abstraction over the distributed collection. Python collections such \\nas lists, tuples, and sets can be distributed very easily. An RDD is recomputed on node \\nfailures. Only part of the data is calculated or recalculated, as required. An RDD is created \\nusing various functions defined in the SparkContext class. One important method for \\nDriver\\nSparkContext\\nCluster\\nManager\\nExecutor\\nExecutor\\nTask\\nTask\\nTask\\nTask\\nFigure 4-1.\\u2002 Spark architecture'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 105}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n87\\ncreating an RDD is parallelize(), which you will encounter again and again in this \\nchapter. Figure\\xa04-2 illustrates the creation of an RDD.\\nData1\\nData2\\nData3\\nData4\\nData1\\nData2\\nData3\\nData4\\nData5\\nData6\\nData7\\nData5\\nData Distribution\\nData6\\nData7\\nFigure 4-2.\\u2002 Creating an RDD\\nLet’s say that we have a Python collection with the elements Data1, Data2, Data3, \\nData4, Data5, Data6, and Data7. This collection is distributed over the cluster to create \\nan RDD. For simplicity, we can assume that two executors are running. Our collection is \\ndivided into two parts. The first executor gets the first part of the collection, which has the \\nelements Data1, Data2, Data3, and Data4. The second part of the collection is sent to the \\nsecond executor. So, the second executor has the data elements Data5, Data6, and Data7.\\nWe can perform two types of operations on the RDD: transformation and action. \\nTransformation on an RDD returns another RDD. We know that RDDs are immutable; \\ntherefore, changing the RDD is impossible. Hence transformations always return \\nanother RDD. Transformations are lazy, whereas actions are eagerly evaluated. I say \\nthat the transformation is lazy because whenever a transformation is applied to an RDD, \\nthat operation is not applied to the data at the same time. Instead, PySpark notes the \\noperation request, but all the transformations are applied when the first action is called.\\nFigure\\xa04-3 illustrates a transformation operation. The transformation on RDD1 \\ncreates RDD2. RDD1 has two partitions. The first partition of RDD1 has four data \\nelements: Data1, Data2, Data3, and Data4. The second data partition of RDD1 has three \\nelements: Data5, Data6, and Data7. After transformation on RDD1, RDD2 is created. \\nRDD2 has six elements. So it is clear that the daughter RDD might have a different \\nnumber of data elements than the father RDD. RDD2 also has two partitions. The first \\npartition of RDD2 has three data points: Data8, Data9, and Data10. The second partition \\nof RDD2 also has three elements: Data11, Data12, and Data13. Don’t get confused about \\nthe daughter RDD having a different number of partitions than the father RDD.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 106}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n88\\nFigure\\xa04-4 illustrates an action performed on an RDD. In this example, we are \\napplying the summation action. Summed data is returned to the driver. In other cases, the \\nresult of an action can be saved to a file or to another destination.\\nData8\\nRDD1\\nRDD2\\nData9\\nData10\\nData11\\nData12\\nData13\\nData1\\nData2\\nData3\\nData4\\nData5\\nData6\\nData7\\nTransformation\\nTransformation\\nFigure 4-3.\\u2002 RDD transformations\\nData1\\nData2\\nData3\\nData4\\nData5\\nData6\\nData7\\nSummation\\nDriver\\nSummation\\nFigure 4-4.\\u2002 RDD action\\nYou might be wondering, if Spark has been written in Scala, then how is Python \\ncontacting with Scala? You might guess that a Python wrapper of PySpark has been \\nwritten using Jython, and that this Jython code is compiled to Java bytecode and run on \\nthe JVM. This guess isn’t correct.\\nA running Python program can access Java objects in a JVM by using Py4J. A running \\nJava program can also access Python objects by using Py4J. A gateway between Python \\nand Java enables Python to use Java objects.\\nDriver programs use Py4J to communicate between Python and the Java \\nSparkContext object. PySpark uses Py4J, so that PySpark Python code can \\nOn remote cluster machines, the PythonRDD object creates Python subprocesses \\nand communicates with them using pipes. The PythonRDD object runs in JVM and \\ncommunicates with Python processes by using pipes.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 107}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n89\\n■\\n■Note\\u2003  You can learn more about Py4J at the following locations:\\nwww.py4j.org\\nhttps://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals\\nRecipe 4-1. Create an RDD\\nProblem\\nYou want to create an RDD.\\nSolution\\nAs we know, an RDD is a distributed collection. You have a list with the following data:\\npythonList = [2.3,3.4,4.3,2.4,2.3,4.0]\\nYou want to do the following operations:\\n• \\nCreate an RDD of the list\\n• \\nGet the first element\\n• \\nGet the first two elements\\n• \\nGet the number of partitions in the RDD\\nIn PySpark, an RDD can be created in many ways. One way to create an RDD out of \\na given collection is to use the parallelize() function. The SparkContext object is used \\nto call the parallelize() function. You’ll read more about SparkContext in an upcoming \\nchapter.\\nIn the case of big data, even tabular data, a table might have more than 1,000 \\ncolumns. Sometimes analysts want to see what those columns of data look like. The \\nfirst() function is defined on an RDD and will return the first element of the RDD.\\nTo get more than one element from a list, we can use the take() function. The \\nnumber of partitions of a collection can be fetched by using getNumPartitions().\\nHow It Works\\nLet’s follow the steps in this section to solve the problem.\\nStep 4-1-1. Creating an RDD of the List\\nLet’s first create a Python list by using the following:\\n>>> pythonList = [2.3,3.4,4.3,2.4,2.3,4.0]\\n>>> pythonList'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 108}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n90\\nHere is the output:\\n[2.3, 3.4, 4.3, 2.4, 2.3, 4.0]\\nParallelization or distribution of data is done using the parallelize() function. This \\nfunction takes two arguments. The first argument is the collection to be parallelized, and \\nthe second argument indicates the number of distributed chunks of data you want:\\n>>> parPythonData = sc.parallelize(pythonList,2)\\nUsing the parallelize() function, we have distributed our data in two partitions. In \\norder to get\\xa0all the data on the driver, we can use the collect() function, as shown in the \\nfollowing code line. Using the collect() function is not recommended in production; \\nrather, it should be used only in code debugging.\\n>>> parPythonData.collect()\\nHere is the output:\\n[2.3, 3.4, 4.3, 2.4, 2.3, 4.0]\\nStep 4-1-2. Getting the First Element\\nThe first() function can be used to get the first data out of an RDD. You might have \\nfigured out that the collect() and first() functions perform actions:\\n>>> parPythonData.first()\\nHere is the output:\\n2.3\\nStep 4-1-3. Getting the First Two Elements\\nSometimes data analysts want to see more than one row of data. The take() function can \\nbe used to fetch more than one row from an RDD. The number of rows you want is given \\nas an argument to the take() function:\\n>>> parPythonData.take(2)\\nHere is the output:\\n[2.3, 3.4]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 109}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n91\\nStep 4-1-4. Getting the Number of Partitions in the RDD\\nIn order to optimize PySpark code, a proper distribution of data is required. The number \\nof partitions of an RDD can be found using the getNumPartitions() function:\\n>>> parPythonData.getNumPartitions()\\nHere is the output:\\n2\\nRecall that we were partitioning our data into two partitions while using the \\nparallelize() function.\\nRecipe 4-2. Convert Temperature Data\\nProblem\\nYou want to convert temperature data by writing a temperature unit conversion program \\non an RDD.\\nSolution\\nYou are given daily temperatures in Fahrenheit. You want to perform some analysis on \\nthat data. But your new software takes input in Celsius. Therefore, you want to convert \\nyour temperature data from Fahrenheit to Celsius. Table\\xa04-1 shows the data you have.\\nday1\\nday2\\nday3\\nday4\\nday5\\nday6\\nday7\\n59\\nTemp\\nIn °F\\n57.2\\n53.6\\n55.4\\n51.8\\n53.6\\n55.4\\nTable 4-1.\\u2002 Daily Temperature in Fahrenheit\\nYou want to do the following:\\n• \\nConvert temperature from Fahrenheit to Celsius\\n• \\nGet\\xa0all the temperature data points greater than 13o C\\nWe can convert temperature from Fahrenheit to Celsius by using the following \\nmathematical formula:\\noC = (oF – 32) × 5/9'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 110}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n92\\nWe can see that in PySpark, this is a transformation problem. We can achieve this \\ntask by using the map() function on the RDD.\\nGetting all the temperatures greater than 13o C is a filtering problem. Filtering of data \\ncan be done by using the filter() function on the RDD.\\nHow It Works\\nWe’ll follow the steps in this section to complete the conversion and filtering exercises.\\nStep 4-2-1. Parallelizing the Data\\nWe are going to parallelize data by using our parallelize() function. We are going to \\ndistribute our data in two partitions, as follows:\\n>>> tempData = [59,57.2,53.6,55.4,51.8,53.6,55.4]\\n>>> parTempData = sc.parallelize(tempData,2)\\n>>> parTempData.collect()\\nHere is the output:\\n[59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]\\nThe collection of data has returned our parallelized data.\\nStep 4-2-2. Converting Temperature from Fahrenheit to Celsius\\nNow we are going to convert our temperature in Fahrenheit to Celsius. We’ll write a \\nfahrenheitToCentigrade function, which will take the temperature in Fahrenheit and \\nreturn a temperature in Celsius for a given input:\\n>>> def fahrenheitToCentigrade(temperature) :\\n...\\xa0\\xa0centigrade = (temperature-32)*5/9\\n...\\xa0\\xa0return centigrade\\nLet’s test our fahrenheitToCentigrade function:\\n>>> fahrenheitToCentigrade(59)\\nHere is the output:\\n15'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 111}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n93\\nWe are providing 59 as the input in Fahrenheit. Our function returns a Celsius value \\nof our Fahrenheit input; 59o F is equal to 15o C.\\n>>> parCentigradeData = parTempData.map(fahrenheitToCentigrade)\\n>>> parCentigradeData.collect()\\nHere is the output:\\n[15, 14.000000000000002, 12.0, 13.0, 10.999999999999998, 12.0, 13.0]\\nWe have converted the given temperature to Celsius. Now let’s filter out all the \\ntemperatures greater than or equal to 13o C.\\nStep 4-2-3. Filtering Temperatures Greater than 13o C\\nTo filter data, we can use the filter() function on the RDD. We have to provide a \\npredicate as input to the filter() function. A predicate is a function that tests a condition \\nand returns True or False.\\nLet’s define the predicate tempMoreThanThirteen, which will take a temperature \\nvalue and return True if input is greater than or equal to 13:\\n>>> def tempMoreThanThirteen(temperature):\\n...\\xa0\\xa0return temperature >=13\\nWe are going to send our tempMoreThanThirteen function as input to the filter() \\nfunction. The filter() function will iterate over each value in the parCentigradeData \\nRDD. For each value, the tempMoreThanThirteen function will be applied. If the \\nvalue is greater than or equal to 13, True will be returned. The value for which \\ntempMoreThanThirteen returns True will come to filteredTemprature:\\n>>> filteredTemprature = parCentigradeData.filter(tempMoreThanThirteen)\\n>>> filteredTemprature.collect()\\nHere is the output:\\n[15, 14.000000000000002, 13.0, 13.0]\\nWe can replace our predicates by using the lambda function. (We discussed lambda \\nfunctions in Chapter 3.) Using a lambda function makes the code more readable. The \\nfollowing code line clearly depicts that the filter() function takes a predicate as input \\nand returns True for all the values greater than or equal to 13:\\n>>> filteredTemprature = parCentigradeData.filter(lambda x : x>=13)\\n>>> filteredTemprature.collect()'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 112}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n94\\nHere is the output:\\n[15, 14.000000000000002, 13.0, 13.0]\\nWe finally have four elements indicating a temperature that is either greater than or \\nequal to 13. So now you understand the way to do basic analysis on data with PySpark.\\nRecipe 4-3. Perform Basic Data Manipulation\\nProblem\\nYou want to do data manipulation and run aggregation operations.\\nSolution\\nIn this recipe, you are given data indicating student grades for a two-year (four-semester) \\ncourse. Seven students are enrolled in this course. Table\\xa04-2 depicts two years of grade \\ndata, divided into semesters, for seven enrolled students.\\nStudent\\nID\\nsi1\\nYear\\nyear1\\nyear2\\n62.08\\n75.94\\n62.4\\n76.75\\nyear1\\n68.26\\n72.95\\nyear2\\n85.49\\n75.8\\nyear1\\n75.08\\n79.84\\nyear2\\n54.98\\n87.72\\nyear1\\n50.03\\n66.85\\nyear2\\n71.26\\n69.77\\nyear1\\n52.74\\n76.27\\nyear2\\n50.39\\n68.58\\nyear1\\n74.86\\n60.8\\nyear1\\n63.95\\n74.51\\nyear2\\n58.29\\n62.38\\nyear2\\n66.69\\n56.92\\nSemester1 Marks\\nSemester2 Marks\\nsi1\\nsi2\\nsi2\\nsi3\\nsi3\\nsi4\\nsi4\\nsi5\\nsi5\\nsi6\\nsi6\\nsi7\\nsi7\\nTable 4-2.\\u2002 Student Grades'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 113}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n95\\nYou want to calculate the following:\\n• \\nAverage grades per semester, each year, for each student\\n• \\nTop three students who have the highest average grades in the \\nsecond year\\n• \\nBottom three students who have the lowest average grades in the \\nsecond year\\n• \\nAll students who have earned more than an 80% average in the \\nsecond semester of the second year\\nUsing the map() function is often helpful. In this example, the average grades per \\nsemester, for each year, can be calculated using map().\\nIt is a general data science problem to get the top k elements, such as the top k highly \\nperforming bonds. The PySpark takeOrdered() function is going to take the top k or top \\nbottom elements from our RDD.\\nStudents who have earned more than 80% averages in the second year can be filtered \\nusing the filter() function.\\nHow It Works\\nLet’s solve our problem in steps. We will start with creating an RDD of our data.\\nStep 4-3-1. Making a List from a Given Table\\nIn this step, we’ll create a nested list. This means that each element of the list is a record, \\nand each record is a list in itself:\\n>>> studentMarksData = [[\"si1\",\"year1\",62.08,62.4],\\n...\\xa0\\xa0[\"si1\",\"year2\",75.94,76.75],\\n...\\xa0\\xa0[\"si2\",\"year1\",68.26,72.95],\\n...\\xa0\\xa0[\"si2\",\"year2\",85.49,75.8],\\n...\\xa0\\xa0[\"si3\",\"year1\",75.08,79.84],\\n...\\xa0\\xa0[\"si3\",\"year2\",54.98,87.72],\\n...\\xa0\\xa0[\"si4\",\"year1\",50.03,66.85],\\n...\\xa0\\xa0[\"si4\",\"year2\",71.26,69.77],\\n...\\xa0\\xa0[\"si5\",\"year1\",52.74,76.27],\\n...\\xa0\\xa0[\"si5\",\"year2\",50.39,68.58],\\n...\\xa0\\xa0[\"si6\",\"year1\",74.86,60.8],\\n...\\xa0\\xa0[\"si6\",\"year2\",58.29,62.38],\\n...\\xa0\\xa0[\"si7\",\"year1\",63.95,74.51],\\n...\\xa0\\xa0[\"si7\",\"year2\",66.69,56.92]]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 114}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n96\\nStep 4-3-2. Parallelizing the Data\\nAfter parallelizing the data by using the parallelize() function, we will find that we have \\nan RDD in which each element is a list itself:\\n>>> studentMarksDataRDD = sc.parallelize(studentMarksData,4)\\nAs we know, the collect() function takes the whole RDD to the driver. If the RDD \\nsize is very large, the driver may face a memory issue. In order to fetch k first elements of \\nan RDD, we can use the take() function with n as input to take(). As an example, in the \\nfollowing line of code, we are fetching two elements of our RDD. Remember here that \\ntake() is an action:\\n>>> studentMarksDataRDD.take(2)\\nHere is the output:\\n[[\\'si1\\', \\'year1\\', 62.08, 62.4],\\n\\xa0[\\'si1\\', \\'year2\\', 75.94, 76.75]]\\nStep 4-3-3. Calculating Average Semester Grades\\nNow let me explain what I want to do in the following code. Just consider the first element \\nof the RDD. Our first element of the RDD is [\\'si1\\', \\'year1\\', 62.08, 62.4], which is a \\nlist of four elements. Our work is to calculate the mean of grades from two semesters. In \\nthe first element, the mean is 0.5(62.08 + 62.4). We are going to use the map() function to \\nget our solution.\\n>>> studentMarksMean = studentMarksDataRDD.map(lambda x : \\n[x[0],x[1],(x[2]+x[3])/2])\\nAgain, we use the take() function to visualize the map() function output:\\n>>> studentMarksMean.take(2)\\nHere is the output:\\n[[\\'si1\\', \\'year1\\', 62.239999999999995],\\n\\xa0[\\'si1\\', \\'year2\\', 76.345]]\\nStep 4-3-4. Filtering Student Average Grades in the Second Year\\nThe following line of code is going to filter out all the data of the second year. We have \\nimplemented our predicate by using a lambda function. Our predicate function checks \\nwhether year2 is in the list. If the predicate returns True, the list includes second-year grades.\\n>>> secondYearMarks = studentMarksMean.filter(lambda x : \"year2\" in x)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 115}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n97\\n>>> secondYearMarks.take(2)\\nHere is the output:\\n[['si1', 'year2', 76.345],\\n\\xa0['si2', 'year2', 80.645]]\\nWe can clearly see that the RDD output of secondYearMarks has only second-year grades.\\nStep 4-3-5. Finding the Top Three Students\\nWe can get the top three students in two ways. The first method is to sort the full data \\naccording to grades. Obviously, we are going to sort the data in decreasing order. Sorting \\nis done by the sortBy() function. Let’s see the implementation:\\n>>> sortedMarksData = secondYearMarks.sortBy(keyfunc = lambda x : -x[2])\\nIn our sortBy() function, we provide the keyfunc parameter. This parameter indicates \\nto sort the grades data in decreasing order. Now collect the output and see the result:\\n>>> sortedMarksData.collect()\\nHere is the output:\\n[['si2', 'year2', 80.645],\\n\\xa0['si1', 'year2', 76.345],\\n\\xa0['si3', 'year2', 71.35],\\n\\xa0['si4', 'year2', 70.515],\\n\\xa0['si7', 'year2', 61.805],\\n\\xa0['si6', 'year2', 60.335],\\n\\xa0['si5', 'year2', 59.485]]\\nAfter sorting data, we can take the first three elements by using our take() function:\\n>>>\\xa0\\xa0sortedMarksData.take(3)\\nHere is the output:\\n[['si2', 'year2', 80.645],\\n\\xa0['si1', 'year2', 76.345],\\n\\xa0['si3', 'year2', 71.35]]\\nWe have our answer. But can we optimize it further? In order to get top-three data, \\nwe are sorting the whole list. We can optimize this by using the takeOrdered() function.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 116}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n98\\nThis function takes two arguments: the number of elements we require, and key, which \\nuses a lambda function to determine how to take the data out.\\n>>> topThreeStudents = secondYearMarks.takeOrdered(num=3, key = lambda x :-x[2])\\nIn the preceding code, we set num to 3 for the three top elements, and lambda in key \\nso that it can provide three top in decreasing order.\\n>>> topThreeStudents\\nHere is the output:\\n[['si2', 'year2', 80.645],\\n\\xa0['si1', 'year2', 76.345],\\n\\xa0['si3', 'year2', 71.35]]\\nIn order to print the result, we are not using the collect() function to get the data. \\nRemember that transformation creates another RDD, so we require the collect() function \\nto collect data. But an action will directly fetch the data to the driver, and collect() is not \\nrequired. So you can conclude that the takeOrdered() function is an action.\\nStep 4-3-6. Finding the Bottom Three Students\\nWe have to find the bottom three students in terms of their average grades. One way is to \\nsort the data in increasing order and take the three on top. But that is not an efficient way, \\nso we will use the takeOrdered() function again, but with a different key parameter:\\n>>> bottomThreeStudents = secondYearMarks.takeOrdered(num=3, key = lambda x \\n:x[2]])\\n>>> bottomThreeStudents\\nHere is the output:\\n[['si5', 'year2', 59.485],\\n\\xa0['si6', 'year2', 60.335],\\n\\xa0['si7', 'year2', 61.805]]\\nStep 4-3-7. Getting All Students with 80% Averages\\nNow that you understand the filter() function, it is easy to guess that we can solve this \\nproblem by using filter(). We will have to provide a predicate, which will return True if \\ngrades are greater than 80; otherwise, it returns False.\\n>>> moreThan80Marks = secondYearMarks.filter(lambda x : x[2] > 80)\\n>>> moreThan80Marks.collect()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 117}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n99\\nHere is the output:\\n[['si2', 'year2', 80.645]]\\nIt can be observed that only one student (with the student ID si2) has secured more \\nthan an 80% average in the second year.\\nRecipe 4-4. Run Set Operations\\nProblem\\nYou want to run set operations on a research company’s data.\\nSolution\\nXYZ Research is a company that performs research on many diversified topics. Each \\nresearch project comes with a research ID. Research may come to a conclusion in one \\nyear or may take more than one year. The following data is provided, indicating the \\nnumber of research projects being conducted in three years:\\n2001: RIN1, RIN2, RIN3, RIN4, RIN5, RIN6, RIN7\\n2002: RIN3, RIN4, RIN7, RIN8, RIN9\\n2003: RIN4, RIN8, RIN10, RIN11, RIN12\\nNow we have to answer the following questions:\\n• \\nHow many research projects were initiated in the three years?\\n• \\nHow many projects were completed in the first year?\\n• \\nHow many projects were completed in the first two years?\\nA set is collection of distinct elements. PySpark performs pseudo set operations. \\nThey are called pseudo set operations because some functions do not remove duplicate \\nelements.\\nRemember, the first question is not asking about completed projects. The total \\nnumber of research projects initiated in three years is just the union of all three years of \\ndata. You can perform a union on two RDDs by using the union() function.\\nThe projects that have been started in the first year and not in the second year \\nare the projects that have been completed in the first year. Every project that is started \\nis completed. We can use the subtract() function to find all the projects that were \\ncompleted in the first year.\\nIf we make a union of first-year and second-year projects and subtract third-year \\nprojects, we are going to get\\xa0all the projects that have been completed in the first two years.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 118}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n100\\nHow It Works\\nLet’s solve this problem step-by-step.\\nStep 4-4-1. Creating a List of Research Data by Year\\nLet’s start with creating a list of all the projects that the company worked on each year:\\n>>> data2001 = ['RIN1', 'RIN2', 'RIN3', 'RIN4', 'RIN5', 'RIN6', 'RIN7']\\n>>> data2002 = ['RIN3', 'RIN4', 'RIN7', 'RIN8', 'RIN9']\\n>>> data2003 = ['RIN4', 'RIN8', 'RIN10', 'RIN11', 'RIN12']\\ndata2001 is list of all the projects started in 2001. Similarly, data2002 contains \\nall the research projects that either are continuing from 2001 or started in 2002. The \\ndata2003data list contains all the projects that the company worked on in 2003.\\nStep 4-4-2. Parallelizing the Data (Creating the RDD)\\nAfter creating lists, we have to parallelize our data:\\n>>> parData2001 = sc.parallelize(data2001,2)\\n>>> parData2002 = sc.parallelize(data2002,2)\\n>>> parData2003 = sc.parallelize(data2003,2)\\nAfter parallelizing, we get three RDDs. The first RDD is parData2001, the second \\nRDD is parData2002, and the last one is parData2003.\\nStep 4-4-3. Finding Projects Initiated in Three Years\\nThe total number of projects initiated in three years is determined just by getting the \\nunion of all the data for the given three years. RDD union() takes another RDD as input \\nand returns, merging these two RDDs. Let’s see how it works:\\n>>> unionOf20012002 = parData2001.union(parData2002)\\n>>> unionOf20012002.collect()\\nHere is the output:\\n['RIN1', 'RIN2', 'RIN3', 'RIN4',\\n\\xa0'RIN5', 'RIN6', 'RIN7', 'RIN3',\\n\\xa0'RIN4', 'RIN7', 'RIN8', 'RIN9']\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 119}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n101\\nWe have calculated the union of different research projects initiated in either the \\nfirst year or the second year. We can observe that the unionized data, unionOf20012002, \\nhas duplicate values. Having duplicates values in sets is not allowed. Therefore, a set \\noperation on an RDD is also known as a pseudo set operation. Don’t worry; we will \\nremove these duplicates.\\nIn order to get\\xa0all the research projects that have been initiated in three years, we \\nhave to get the union of parData2003 and unionOf20012002:\\n>>> allResearchs = unionOf20012002.union(parData2003)\\n>>> allResearchs.collect()\\nHere is the output:\\n['RIN1', 'RIN2', 'RIN3', 'RIN4',\\n\\xa0'RIN5', 'RIN6', 'RIN7', 'RIN3',\\n\\xa0'RIN4', 'RIN7', 'RIN8', 'RIN9',\\n\\xa0'RIN4', 'RIN8', 'RIN10', 'RIN11', 'RIN12']\\nWe have the union of all three years of data. Now we have to get rid of duplicates.\\nStep 4-4-4. Making Sets of Distinct Data\\nWe are going to apply the distinct() function to our RDD allResearchs:\\n>>> allUniqueResearchs = allResearchs.distinct()\\n>>> allUniqueResearchs.collect()\\nHere is the output:\\n['RIN1', 'RIN12', 'RIN5', 'RIN3',\\n\\xa0'RIN4', 'RIN2', 'RIN11', 'RIN7',\\n\\xa0'RIN9', 'RIN6', 'RIN8', 'RIN10']\\nWe can see that we have all the research projects that were initiated in the first  \\nthree years.\\nStep 4-4-5. Counting Distinct Elements\\nNow count all the distinct research projects by using the count() function on the RDD:\\n>>> allResearchs.distinct().count()\\nHere is the output:\\n12\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 120}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n102\\n■\\n■Note\\u2003  We can run telescopic commands in PySpark too.\\nThe following command is run in telescopic fashion:\\n>>> parData2001.union(parData2002).union(parData2003).distinct().count()\\nHere is the output:\\n12\\nStep 4-4-6. Finding Projects Completed the First Year\\nLet’s say we have two sets, A and B. Subtracting set B from set A will give us all the \\nelements that are members of set A but not set B. So now it is clear that, in order to know \\nall the projects that have been completed in the first year (2001), we have to subtract the \\nprojects in year 2002 from all the projects in year 2001.\\nSubtraction on a set can be done with the subtract() function:\\n>>> firstYearCompletion = parData2001.subtract(parData2002)\\n>>> firstYearCompletion.collect()\\nHere is the output:\\n['RIN5', 'RIN1', 'RIN6', 'RIN2']\\nWe have all the projects that were completed in 2001. Four projects were completed \\nin 2001.\\nStep 4-4-7. Finding Projects Completed in the First Two Years\\nA union of RDDs gives us all the projects started in the first two years. After getting all \\nthe projects started in the first two years, if we then subtract projects running and started \\nin the third year, we will return all the projects completed in the first two years. The \\nfollowing is the implementation:\\n>>> unionTwoYears = parData2001.union(parData2002)\\n>>> unionTwoYears.subtract(parData2003).collect()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 121}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n103\\nHere is the output:\\n['RIN1', 'RIN5', 'RIN3', 'RIN3',\\n\\xa0'RIN2', 'RIN7', 'RIN7', 'RIN9', 'RIN6']\\nNow subtract:\\n>>> unionTwoYears.subtract(parData2003).distinct().collect()\\nHere is the output:\\n['RIN1', 'RIN5', 'RIN3',\\n\\xa0'RIN2', 'RIN7', 'RIN9', 'RIN6']\\nStep 4-4-8. Finding Projects Started in 2001 and Continued \\nThrough 2003.\\nThis step requires using the intersection() method defined in PySpark on the RDD:\\n>>> projectsInTwoYear = parData2001.intersection(parData2002)\\n>>> projectsInTwoYear.collect()\\nHere is the output:\\n['RIN4', 'RIN7', 'RIN3']\\n>>> projectsInTwoYear.subtract(parData2003).distinct().collect()\\nHere is the output:\\n['RIN3', 'RIN7']\\nRecipe 4-5. Calculate Summary Statistics\\nProblem\\nYou want to calculate summary statistics on given data.\\nSolution\\nRenewable energy sources are gaining in popularity all over the world. The company \\nFindEnergy wants to install windmills at a given location. For efficient operation of \\nwindmills, the air requires certain characteristics.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 122}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n104\\nData is collected as shown in Table\\xa04-3.\\n8 am\\n10 am\\n12 am\\n2 pm\\n4 pm\\n6 pm\\n8 pm\\n12\\nAir\\nVelocity\\nin\\nKMPH\\n13\\n15\\n12\\n11\\n12\\n11\\nTable 4-3.\\u2002 Air Velocity Data\\nYou, as a data scientist, want to calculate the following quantities:\\n• \\nNumber of data points\\n• \\nSummation of air velocities over a day\\n• \\nMean air velocity in a day\\n• \\nVariance of air data\\n• \\nSample variance of air data\\n• \\nStandard deviation of air data\\n• \\nSample standard deviation of air data\\nPySpark provides many functions to summarize data on the RDD. The number of \\nelements in an RDD can be found by using the count() function on the RDD. There are \\ntwo ways to sum all the data in a given RDD. The first is to apply the sum() method to the \\nRDD. The second is to apply the reduce() function to the RDD.\\nThe mean represents the center point of the given data, and it can be calculated in \\ntwo ways too. We are going to use the mean() method and the fold() method to calculate \\nthe mean.\\nThe variance, which indicates the spread of data around the mean, can be calculated \\nusing the variance() function. Similarly, the sample variance can be calculated by using \\nthe sampleVariance() method on the RDD.\\nStandard deviation and sample standard deviation will be calculated using the \\nstdev() and sampleStdev() methods, respectively.\\nPySpark provides the stats() method, which can calculate all the previously \\nmentioned quantities in one go.\\nHow It Works\\nWe’ll follow the steps in this section to reach a solution.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 123}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n105\\nStep 4-5-1. Parallelizing the Data\\nLet’s parallelize the air velocity data from a list:\\n>>> airVelocityKMPH = [12,13,15,12,11,12,11]\\n>>> parVelocityKMPH = sc.parallelize(airVelocityKMPH,2)\\nThe parVelocityKMPH variable is an RDD.\\nStep 4-5-2. Getting the Number of Data Points\\nThe number of data points gives us an idea of the data size. We apply the count() \\nfunction to get the number of elements in the RDD:\\n>>>countValue =\\xa0\\xa0parVelocityKMPH.count()\\nHere is the output:\\n7\\nThe total number of data points is seven.\\nStep 4-5-3. Summing Air Velocities in a Day\\nLet’s get the summation by using the sum() method:\\n>>>sumValue =\\xa0\\xa0parVelocityKMPH.sum()\\nHere is the output:\\n86\\nStep 4-5-4. Finding the Mean Air Velocity\\nFigure\\xa04-5 shows the mathematical formula for finding a mean, where x1, x2, . . . xn are n \\ndata points.\\nFigure 4-5.\\u2002 Calculating the mean'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 124}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n106\\nWe calculate the mean by using the mean() function defined on the RDD:\\n>>>meanValue =\\xa0\\xa0parVelocityKMPH.mean()\\nHere is the output:\\n12.285714285714286\\nStep 4-5-5. Finding the Variance of Air Data\\nIf we have the data points x1, x2, . . . xn, then Figure\\xa04-6 shows the mathematical formula \\nfor calculating variance. We are going to calculate the variance of the given air data by \\nusing the variance() function defined on the RDD.\\n>>> varianceValue = parVelocityKMPH.variance()\\nFigure 4-6.\\u2002 Calculating the variance\\nHere is the output:\\n1.63265306122449\\nStep 4-5-6. Calculating Sample Variance\\nThe variance function calculates the population variance. In order to calculate the sample \\nvariance, we have to use sampleVariance() defined on the RDD.\\nFor data points x1, x2, . . . xn, the sample standard variance is defined in Figure\\xa04-7.\\nFigure 4-7.\\u2002 Calculating the sample variance'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 125}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n107\\nThe following line of code calculates the sample standard deviation:\\n>>> sampleVarianceValue =\\xa0\\xa0parVelocityKMPH.sampleVariance()\\nHere is the output:\\n1.904761904761905\\nStep 4-5-7. Calculating Standard Deviation\\nThe standard deviation is the square root of the variance value. Let’s calculate the \\nstandard deviation by using the stdev() function:\\n>>> stdevValue = parVelocityKMPH.stdev()\\nHere is the output:\\n1.2777531299998799\\nThe standard deviation of the given air velocity data is 1.2777531299998799.\\nStep 4-5-8. Calculating Sample Standard Deviation\\n>>> sampleStdevValue = parVelocityKMPH.sampleStdev()\\nHere is the output:\\n1.3801311186847085\\nStep 4-5-9. Calculating All Values in One Step\\nWe can calculate all the values of the summary statistics in one go by using the stats() \\nfunction. The StatCounter object is returned from the stats() function. Let’s use the \\nstats() function to calculate the summary statistics of the air velocity data:\\n>>> type(parVelocityKMPH.stats())\\nHere is the output:\\n<class 'pyspark.statcounter.StatCounter'>\\n>>> parVelocityKMPH.stats()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 126}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n108\\nHere is the output:\\n(count: 7, mean: 12.2857142857, stdev: 1.27775313, max: 15.0, min: 11.0)\\nWe can see that the stats() function is an action. It calculates the count, mean, \\nstandard deviation, maximum, and minimum of an RDD in one go. It returns the result as \\na tuple with elements that are key/value pairs. The result of the stats() function can be \\ntransformed into a dictionary by using the asDict() function:\\n>>> parVelocityKMPH.stats().asDict()\\nHere is the output:\\n{'count': 7, 'min': 11.0, 'max': 15.0, 'sum': 86.0, 'stdev': 1.3801311186847085, \\n'variance': 1.904761904761905, 'mean': 12.285714285714286}\\nWe also can get individual elements by using different functions defined on \\nStatCounter. Let’s start with fetching the mean. The mean value can be found by using \\nthe mean() function defined on the StatCounter object:\\n>>> parVelocityKMPH.stats().mean()\\nHere is the output:\\n12.285714285714286\\nSimilarly, we can get the number of elements in the RDD, the minimum value, the \\nmaximum value, and the standard deviation by using the functions count(), min(), \\nmax(), and stdev() functions, respectively. Let’s start with the standard deviation:\\n>>> parVelocityKMPH.stats().stdev()\\nHere is the output:\\n1.2777531299998799\\nThis command provides the number of elements:\\n>>> parVelocityKMPH.stats().count()\\nHere is the output:\\n7\\nNext, we find the minimum value:\\n>>> parVelocityKMPH.stats().min()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 127}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n109\\nHere is the output:\\n11.0\\nThen we find the maximum value:\\n>>> parVelocityKMPH.stats().max()\\nHere is the output:\\n15.0\\nRecipe 4-6. Start PySpark Shell on Standalone \\nCluster Manager\\nProblem\\nYou want to start PySpark shell using Standalone Cluster Manager.\\nSolution\\nStandalone Cluster Manager provides a master/slave structure. In order to start \\nStandalone Cluster Manager on a single machine, we can use the start-all.sh script. \\nWe can find this script inside $SPARK_HOME/sbin. In our case, $SPARK_HOME is  \\n/allPySpark/spark. In a cluster environment, we can run a master script to run the \\nmaster on one machine, and a slave script on a different slave machine to start the slave.\\nWe can start the PySpark shell by using the master URL of the Standalone master. \\nThe Standalone master URL looks like spark://<masterHostname>:<masterPort>. It can \\nbe easily found in the log file of the master.\\nHow It Works\\nWe have to start the Standalone master and slave processes. There are two ways to do this. \\nIn the spark/sbin directory, we will find the start-all.sh script. It will start the master \\nand slave together on the same machine.\\nAnother way to start the processes is as follows: on the master machine, start the \\nmaster by using the start-master.sh script; and start the slave on a different machine by \\nusing start-slave.sh.\\nStep 4-6-1. Starting Standalone Cluster Manager Using the \\nstart-all.sh Script\\nIn this step, we are going to run the start-all.sh script:\\n[pysparkbook@localhost ~]$ /allPySpark/spark/sbin/start-all.sh'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 128}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n110\\nHere is the output:\\nstarting org.apache.spark.deploy.master.Master, logging to /allPySpark/\\nlogSpark//spark-pysparkbook-org.apache.spark.deploy.master.Master-1-\\nlocalhost.localdomain.out\\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /\\nallPySpark/logSpark//spark-pysparkbook-org.apache.spark.deploy.worker.\\nWorker-1-localhost.localdomain.out\\nThe logs of starting the Standalone cluster by using the start-all.sh script are written \\nto two files. The file spark-pysparkbook-org.apache.spark.deploy.master.Master-1-\\nlocalhost.localdomain.out is the log of starting the master process. In order to connect \\nto the master, we need the master URL. We can find this URL in the log file.\\nLet’s open the master log file:\\n[pysparkbook@localhost ~]$ vim /allPySpark/logSpark//spark-pysparkbook-org.\\napache.spark.deploy.master.Master-1-localhost.localdomain.out\\nHere is the output:\\n17/03/02 03:33:59 INFO util.Utils: Successfully started service \\n'sparkMaster' on port 7077.\\n17/03/02 03:33:59 INFO master.Master: Starting Spark master at spark://\\nlocalhost.localdomain:7077\\n17/03/02 03:33:59 INFO master.Master: Running Spark version 1.6.2\\n17/03/02 03:34:10 INFO server.Server: jetty-8.y.z-SNAPSHOT\\n17/03/02 03:34:10 INFO server.AbstractConnector: Started \\nSelectChannelConnector@0.0.0.0:8080\\n17/03/02 03:34:10 INFO util.Utils: Successfully started service 'MasterUI' \\non port 8080.\\n17/03/02 03:34:10 INFO ui.MasterWebUI: Started MasterWebUI at \\nhttp://10.0.2.15:8080\\nThe logs in the master log file indicate that the master has been started on spark://\\nlocalhost.localdomain:7077 and that the master web UI is at http://10.0.2.15:8080.\\nLet’s open the master web UI, shown in Figure\\xa04-8. In this UI, we can see the URL of \\nthe Standalone master.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 129}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n111\\nThe following command will start the PySpark shell on the Standalone cluster:\\n[pysparkbook@localhost ~]$/allPySpark/spark/bin/pyspark --master spark://\\nlocalhost.localdomain:7077\\nAfter running jps, we can see the master and worker processes:\\n[pysparkbook@localhost ~]$ jps\\nHere is the output:\\n4246 Worker\\n3977 Master\\n4285 Jps\\nWe can stop Standalone Cluster Manager by using the stop-all.sh script:\\n[pysparkbook@localhost ~]$ /allPySpark/spark/sbin/stop-all.sh\\nHere is the output:\\nlocalhost: stopping org.apache.spark.deploy.worker.Worker\\nstopping org.apache.spark.deploy.master.Master\\nWe can see that the stop-all.sh script first stops the worker (slave) and then stops \\nthe master.\\nFigure 4-8.\\u2002 Spark master'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 130}, page_content='Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n112\\nStep 4-6-2. Starting Standalone Cluster Manager with an \\nIndividual Script\\nThe PySpark framework provides an individual script for starting the master and workers \\non different machines. Let’s first start the master by using the start-master.sh script:\\n[pysparkbook@localhost ~]$ /allPySpark/spark/sbin/start-master.sh\\nHere is the output:\\nstarting org.apache.spark.deploy.master.Master, logging to /allPySpark/\\nlogSpark//spark-pysparkbook-org.apache.spark.deploy.master.Master-1-\\nlocalhost.localdomain.out\\nAgain, as done previously, we have to get the master URL from the log file. Using that \\nmaster URL, we can start slaves on different machines by using the start-slave.sh script:\\n[pysparkbook@localhost ~]$ /allPySpark/spark/sbin/start-slave.sh spark://\\nlocalhost.localdomain:7077\\nHere is the output:\\nstarting org.apache.spark.deploy.worker.Worker, logging to /allPySpark/\\nlogSpark//spark-pysparkbook-org.apache.spark.deploy.worker.Worker-1-\\nlocalhost.localdomain.out\\nUsing the jps command, we can see the worker and master processes running on \\nour machine:\\n[pysparkbook@localhost ~]$ jps\\nHere is the output:\\n4246 Worker\\n3977 Master\\n4285 Jps\\nYou might remember that when we use stop-all.sh, the script first stops the worker \\nprocess and then the master process. We have to follow this order. We stop the worker \\nprocess first, followed by the master process.\\nTo stop the worker process, use the stop-slave.sh script:\\n[pysparkbook@localhost ~]$ /allPySpark/spark/sbin/stop-slave.sh\\nHere is the output:\\nstopping org.apache.spark.deploy.worker.Worker'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 131}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n113\\nRunning the jps command will show only the master process now:\\n[pysparkbook@localhost ~]$ jps\\nHere is the output:\\n4341 Jps\\n3977 Master\\nSimilarly, we can stop the master process by using the stop-master.sh script:\\n[pysparkbook@localhost ~]$ /allPySpark/spark/sbin/stop-master.sh\\nHere is the output:\\nstopping org.apache.spark.deploy.master.Master\\nRecipe 4-7. Start PySpark Shell on Mesos\\nProblem\\nYou want to start PySpark shell on a Mesos cluster manager.\\nSolution\\nMesos is another cluster manager. Mesos also follows a master/slave architecture, similar \\nto Standalone Cluster Manager. In order to start Mesos, we have to first start the Mesos \\nmaster and then the Mesos slave. Then the PySpark shell can be started by using the \\nmaster URL on Mesos.\\nHow It Works\\nWe installed Mesos in Chapter 3. Now we have to start the master and slave processes \\none by one. After starting the master and slaves, we have to start the PySpark shell. The \\nfollowing command starts the Mesos master process:\\n[pysparkbook@localhost ~]$\\xa0\\xa0mesos-master –work_dir=/allPySpark/mesos/workdir\\nHere is the output:\\nI0224 09:50:57.575908\\xa0\\xa09839 main.cpp:263] Build: 2016-12-29 00:42:08 by \\npysparkbook\\nI0224 09:50:57.576501\\xa0\\xa09839 main.cpp:264] Version: 1.1.0\\nI0224 09:50:57.582787\\xa0\\xa09839 main.cpp:370] Using 'HierarchicalDRF' allocator\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 132}, page_content=\"Chapter 4 ■ Spark Architecture and the Resilient Distributed Dataset\\n114\\nThe slave process will start subsequent to the start of the master process:\\n[root@localhost binaries]#mesos-slave --master=127.0.0.1:5050 --work_dir=/\\nallPySpark/mesos/workdir1 --systemd_runtime_directory=/allPySpark/mesos/systemd\\nHere is the output:\\nI0224 18:22:25.002970\\xa0\\xa03797 gc.cpp:55] Scheduling '/allPySpark/mesos/\\nworkdirSlave/slaves/dd2c5f22-57f9-416e-a71a-0cc83de8558d-S1/frameworks/\\ndd2c5f22-57f9-416e-a71a-0cc83de8558d-0000/executors/1/runs/a7c3d613-9696-\\n4d42-afe2-a27c5c825e72' for gc 6.99998839417778days in the future\\nI0224 18:22:25.003083\\xa0\\xa03797 gc.cpp:55] Scheduling\\nI0224 18:22:25.003123\\xa0\\xa03797 gc.cpp:55] Scheduling '/allPySpark/mesos/\\nworkdirSlave/slaves/dd2c5f22-57f9-416e-a71a-0cc83de8558d-S1/frameworks/\\ndd2c5f22-57f9-416e-a71a-0cc83de8558d-0000' for gc 6.99998839205926days in \\nthe future\\nWe have our master and slave processes started. Now we can start the PySpark  \\nshell by using the following command. We provide the master URL and one parameter, \\nspark.executor.uri. The spark.executor.uri parameter tells Mesos the location to get \\nthe PySpark assembly.\\n[pysparkbook@localhost binaries]$ /allPySpark/spark/bin/pyspark --master \\nmesos://127.0.0.1:5050 --conf spark.executor.uri=/home/pysparkbook/binaries/\\nspark-2.0.0-bin-hadoop2.6.tgz\\nWe can run the jps command to see the Java process after running PySpark on Mesos:\\n[pysparkbook@localhost binaries]$jps\\nHere is the output:\\n4174 SparkSubmit\\n4287 CoarseGrainedExecutorBackend\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 133}, page_content='115\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_5\\nCHAPTER 5\\nThe Power of Pairs:  \\nPaired RDDs\\nKey/value pairs are good for solving many problems efficiently in a parallel fashion. \\nApache Mahout, a machine-learning library that was initially developed on top of Apache \\nHadoop, implements many machine-learning algorithms in the areas of classification, \\nclustering, and collaborative filtering by using the MapReduce key/value-pair \\narchitecture. In this chapter, you’ll work through recipes that develop skills for solving \\ninteresting big data problems from many disciplines.\\nThis chapter covers the following recipes:\\nRecipe 5-1. Create a paired RDD\\nRecipe 5-2. Perform data aggregation\\nRecipe 5-3. Join data\\nRecipe 5-4. Calculate page rank\\nRecipe 5-1. Create a Paired RDD\\nProblem\\nYou want to create a paired RDD.\\nSolution\\nYou have an RDD, RDD1. The elements of RDD1 are b, d, m, t, e, and u. You want to create \\na paired RDD, in which the keys are elements of a single RDD, and the value of a key is 0 \\nif the element is a consonant, or1 if the element is a vowel. Figure\\xa05-1 clearly depicts the \\nrequirements.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 134}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n116\\nA paired RDD can be created in many ways. One way is to read data directly from \\nfiles. We’ll explore this method in an upcoming chapter. Another way to create a paired \\nRDD is by using the map() method, which you’ll learn about in this recipe.\\nHow It Works\\nIn this section, you’ll follow several steps to reach the solution.\\nStep 5-1-1. Creating an RDD with Single Elements\\nLet’s start by creating an RDD out of our given data:\\n>>> pythonList\\xa0\\xa0=\\xa0\\xa0['b' , 'd', 'm', 't', 'e', 'u']\\n>>> RDD1 = sc.parallelize(pythonList,2)\\n>>>RDD1.collect()\\nRDD1\\nRDD2\\nKey\\nValue\\nb\\nd\\nm\\nt\\ne\\nu\\nb\\nd\\nm\\nt\\ne\\nu\\n0\\n0\\n0\\n0\\n1\\n1\\nFigure 5-1.\\u2002 Creating a paired RDD\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 135}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n117\\nHere is the output:\\n['b', 'd', 'm', 't', 'e', 'u']\\nWe have created an RDD named RDD1. The elements of RDD1 are b, d, m, t, e, and u.  \\nThis is an RDD of letters. It can be observed that the elements b, d, m, and t are consonants.  \\nThe other elements of RDD1, e and u, are vowels.\\nStep 5-1-2. Writing a Python Method to Check for Consonants\\nWe are going to define a Python function named vowelCheckFunction(). This function \\nwill take a letter as input and return 1 if the input is a consonant, or 0 if it is not. Let’s \\nimplement the function:\\n>>> def vowelCheckFunction( data) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0if data in ['a','e','i','o','u']:\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return 1\\n...\\xa0\\xa0\\xa0\\xa0\\xa0else :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return 0\\nIt can be observed that our Python function vowelCheckFunction will take one input \\ndata. In this case, we are going to send a letter. Inside the function, we check whether our \\ndata is a vowel. If our if block logical expression results in True, our function will return \\n1; otherwise, it will return 0.\\nWithout testing vowelCheckFunction, we shouldn’t trust it. So let’s start with a vowel \\nas input:\\n>>> vowelCheckFunction('a')\\nHere is the output:\\n1\\nWe get 1 as the output for our vowel input. Our Python function vowelCheckFunction \\nmeets our expectation for vowels. That’s nice, but let’s test for consonants too. This time \\nwe are going to send b, a consonant, as the input:\\n>>> vowelCheckFunction('b')\\nHere is the output:\\n0\\nWe have tested our function. It is working for both consonants and vowels with the \\nanticipated output. We can bank on our developed function.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 136}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n118\\nStep 5-1-3. Creating a Paired RDD\\nWe can create our required RDD by using the map() function. We have to create a paired \\nRDD: the keys will be the elements of RDD1, and the value will be 0 for keys that are \\nconsonants, or 1 for keys that are vowels:\\n>>> RDD2 = RDD1.map( lambda data : (data, vowelCheckFunction(data)))\\n>>>RDD2.collect()\\nHere is the output:\\n[('b', 0),\\n\\xa0('d', 0),\\n\\xa0('m', 0),\\n\\xa0('t', 0),\\n\\xa0('e', 1),\\n\\xa0('u', 1)]\\nStep 5-1-4. Fetching Keys from a Paired RDD\\nThe keys() function can be used to fetch all the keys:\\n>>> RDD2Keys = RDD2.keys()\\nThe following code line gets all the keys:\\n>>> RDD2Keys.collect()\\nHere is the output:\\n['b', 'd', 'm', 't', 'e', 'u']\\nWe can see that the keys() function performs a transformation. Therefore, keys() \\nreturns an RDD that requires the collect() function to get the data to the driver.\\nStep 5-1-5. Fetching Values from a Paired RDD\\nSimilar to the keys() function, the values() function will fetch all the values from a \\npaired RDD. It also performs a transformation:\\n>>> RDD2Values = RDD2.values()\\n>>> RDD2Values.collect()\\nHere is the output:\\n[0, 0, 0, 0, 1, 1]\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 137}, page_content='Chapter 5 ■ The Power of Pairs: Paired RDDs \\n119\\nRecipe 5-2. Aggregate Data\\nProblem\\nYou want to aggregate data.\\nSolution\\nYou want to perform data aggregation on data from a lightbulb manufacturer, as shown in \\nTable\\xa05-1.\\nFilament Type\\nfilamentA\\nfilamentB\\nfilamentB\\nfilamentB\\nfilamentB\\nfilamentB\\nfilamentB\\nfilamentB\\nfilamentB\\nfilamentA\\nfilamentA\\nfilamentA\\nfilamentA\\nfilamentA\\nfilamentA\\nfilamentA\\nBulb Power\\n100W\\n100W\\n100W\\n200W\\n200W\\n200W\\n200W\\n200W\\n200W\\n200W\\n200W\\n100W\\n100W\\n100W\\n100W\\n100W\\n605\\n683\\n691\\n561\\n530\\n600\\n579\\n520\\n569\\n555\\n541\\n619\\n686\\n696\\n622\\n668\\nLife in Hours\\nTable 5-1.\\u2002 Filament Data \\nCompany YP manufactures two types of filaments: filamentA and filamentB. 100W \\nand 200W electric bulbs can be manufactured from both filaments. Table\\xa05-1 indicates \\nthe expected life of each bulb.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 138}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n120\\nYou want to calculate the following:\\n• \\nMean life in hours for bulbs of each filament type\\n• \\nMean life in hours for bulbs of each power level\\n• \\nMean life in hours based on both filament type and power\\nWe generally encounter aggregation of data in data-science problems. To get an \\naggregation of data, we can use many PySpark functions.\\nIn this recipe, we’ll use the reduceByKey() function to calculate the mean by using \\nkeys. Calculating the mean of complex keys requires creating those complex keys. \\nComplex keys can be created by using the map() function.\\nHow It Works\\nLet’s start with the creation of the RDD.\\nStep 5-2-1. Creating an RDD with Single Elements\\n>>> filDataSingle = [['filamentA','100W',605],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','100W',683],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','100W',691],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','200W',561],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','200W',530],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','100W',619],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','100W',686],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','200W',600],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','100W',696],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','200W',579],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','200W',520],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','100W',622],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','100W',668],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','200W',569],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentB','200W',555],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['filamentA','200W',541]]\\n>>> filDataSingleRDD = sc.parallelize(filDataSingle,2)\\n>>> filDataSingleRDD.take(3)\\nHere is the output:\\n[['filamentA', '100W', 605],\\n\\xa0['filamentB', '100W', 683],\\n\\xa0['filamentB', '100W', 691]]\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 139}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n121\\nHere we are first creating a nested Python list of filament data named \\nfilDataSingle. Then we create the RDD filDataSingleRDD. We divide our data into two \\nparts.  The output of the take() function on filDataSingleRDD clearly shows that the \\nelements of the RDD are a list.\\nStep 5-2-2. Creating a Paired RDD\\nFirst we have to calculate the mean lifetime of bulbs, based on their filament type. Better \\nthat we are creating a paired RDD with keys for the filament type and values for the life in \\nhours. So let’s create our required paired RDD and then investigate it:\\n>>> filDataPairedRDD1 = filDataSingleRDD.map(lambda x : (x[0], x[2]))\\n>>> filDataPairedRDD1.take(4)\\nHere is the output:\\n[('filamentA', 605),\\n\\xa0('filamentB', 683),\\n\\xa0('filamentB', 691),\\n\\xa0('filamentB', 561)]\\nWe have created a paired RDD, filDataPairedRDD1, by using the map() function \\ndefined on the RDD. The paired RDD filDataPairedRDD1 has the filament type as the \\nkey, and the life in hours as the value.\\nStep 5-2-3. Finding the Mean Lifetime Based on Filament Type\\nNow we have our required paired RDD. But is this all we need? No. To calculate the mean, \\nwe need a sum and a count. We have to add an extra 1 in our paired RDD so that we can \\nget a sum and a count. So let’s add an extra 1 now to each RDD element:\\n>>> filDataPairedRDD11 = filDataPairedRDD1.map(lambda x : (x[0], [x[1], 1]))\\n>>> filDataPairedRDD11.take(4)\\nHere is the output:\\n[('filamentA', [605, 1]),\\n\\xa0('filamentB', [683, 1]),\\n\\xa0('filamentB', [691, 1]),\\n\\xa0('filamentB', [561, 1])]\\nfilDataPairedRDD11 is a paired RDD. The values of filDataPairedRDD11 are \\npresented as a list; the first element is the lifetime of the bulb (in hours), and the second \\nelement is just 1.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 140}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n122\\nNow we have to calculate the sum of the values of the lifetimes for each filament type \\nas well as the count value, so that we can calculate the mean. Many PySpark functions \\ncould be used to do this job, but here we are going to use the reduceByKey() function for \\npaired RDDs.\\nThe reduceByKey() function applies aggregation operators key wise. It takes an \\naggregation function as input and applies that function on the values of each RDD key.\\nLet’s calculate the sum of the total life hours of bulbs based on the filament type, and \\nthe count of elements for each filament type:\\n>>> filDataSumandCount = filDataPairedRDD11.reduceByKey(lambda l1,l2 : \\n[l1[0] + l2[0] ,l1[1]+l2[1]])\\n>>> filDataSumandCount.collect()\\nHere is the output:\\n[('filamentA', [4684, 8]),\\n\\xa0('filamentB', [5041, 8])]\\nWe are applying the reduceBykey() function to our paired RDD, \\nfilDataPairedRDD11. To understand the workings of reduceByKey(), let’s see how our \\npaired RDD has been distributed. Let’s start with the number of elements in the RDD: the \\nfive elements of filDataPairedRDD11:\\n>>> filDataPairedRDD11.count()\\nHere is the output:\\n16\\nOur paired RDD has 16 elements. Now let’s see how many partitions our data has:\\n>>> filDataPairedRDD11.getNumPartitions()\\nHere is the output:\\n2\\nIt is clear that the data has been distributed in two parts. PySpark will try to distribute \\ndata evenly to executors, so it will distribute eight data points to each executor. Now let’s \\ntake five data points out of our RDD and see what that data looks like:\\n>>> filDataPairedRDD11.take(5)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 141}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n123\\nHere is the output:\\n[('filamentA', [605, 1]),\\n\\xa0('filamentB', [683, 1]),\\n\\xa0('filamentB', [691, 1]),\\n\\xa0('filamentB', [561, 1]),\\n\\xa0('filamentA', [530, 1])]\\nWe can see that first and fifth element of our filDataPairedRDD11 RDD has the key \\nfilamentA. Therefore, the first l1 and l2 of our reduceByKey() function for filament type \\nfilamentA will be [605, 1] and [530, 1], respectively.\\n>>> filDataSumandCount.collect()\\nHere is the output:\\n[('filamentA', [4684, 8]),\\n\\xa0('filamentB', [5041, 8])]\\nFinally, we have the summation of the life hours of bulbs and the count, based on \\nfilament type. The next step is to divide the sum by the count to get the mean value. Let’s \\ndo that:\\n>>> filDataMeanandCount = filDataSumandCount.map( lambda l : \\n[l[0],float(l[1][0])/l[1][1],l[1][1]])\\n>>> filDataMeanandCount.collect()\\nHere is the output:\\n[['filamentA', 585.5, 8],\\n\\xa0['filamentB', 630.125, 8]]\\nFinally, we have our required mean, based on filament type. The mean lifetime of \\nfilamentA is 585.5 hours, and the mean lifetime of filamentB is 630.125 hours. We can \\ninfer that filamentB has a longer life than filamentA.\\nStep 5-2-4. Finding the Mean Lifetime Based on Bulb Power\\nFirst, we will start with creating our paired RDD. The key will be the bulb power, and the \\nvalue will be the life in hours:\\n>>> filDataPairedRDD2 = filDataSingleRDD.map(lambda x : (x[1], x[2]))\\n>>> filDataPairedRDD2.take(4)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 142}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n124\\nHere is the output:\\n[('100W', 605),\\n\\xa0('100W', 683),\\n\\xa0('100W', 691),\\n\\xa0('200W', 561)]\\nWe have created a paired RDD, filDataPairedRDD2, and each element is a pair: of \\nbulb power and the corresponding life in hours.\\n>>> fillDataPairedRDD22 = filDataPairedRDD2.map( lambda x : (x[0],[x[1],1]))\\n>>> fillDataPairedRDD22.take(4)\\nHere is the output:\\n[('100W', [605, 1]),\\n\\xa0('100W', [683, 1]),\\n\\xa0('100W', [691, 1]),\\n\\xa0('200W', [561, 1])]\\nNow we have included 1 in the value part of the RDD. Therefore, each value is a list \\nthat consists of the life in hours and a 1.\\n>>> powerSumandCount = fillDataPairedRDD22.reduceByKey(lambda l1,l2 : \\n[l1[0]+l2[0], l1[1]+l2[1]])\\n>>> powerSumandCount.collect()\\nHere is the output:\\n[('100W', [5270, 8]),\\n\\xa0('200W', [4455, 8])]\\nWe have calculated the sum of the total life hours and the count, with bulb power as \\nthe key. \\n>>> meanandCountPowerWise =powerSumandCount.map(lambda val : \\n[val[0],[float(val[1][0])/val[1][1],val[1][1]]])\\n>>> meanandCountPowerWise.collect()\\nHere is the output:\\n[['100W', [658.75, 8]], ['200W', [556.875, 8]]]\\nIn this last step, we have computed the mean and the count. From the result, we can \\ninfer that the mean life of 100W bulbs is longer than that of 200W bulbs.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 143}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n125\\nStep 5-2-5. Finding the Mean Lifetime Based on Filament Type \\nand Power\\nTo solve this part of the exercise, we need a paired RDD with keys that are complex. You \\nmight be wondering what a complex key is. Complex keys have more than one type. In our \\ncase, our complex key will have both the filament type and bulb power type. Let’s start \\ncreating our paired RDD with a complex key type:\\n>>> filDataSingleRDD.take(4)\\nHere is the output:\\n[['filamentA', '100W', 605],\\n\\xa0['filamentB', '100W', 683],\\n\\xa0['filamentB', '100W', 691],\\n\\xa0['filamentB', '200W', 561]]\\n>>> filDataComplexKeyData = filDataSingleRDD.map( lambda val : [(val[0], \\nval[1]),val[2]])\\n>>> filDataComplexKeyData.take(4)\\nHere is the output:\\n[[('filamentA', '100W'), 605],\\n\\xa0[('filamentB', '100W'), 683],\\n\\xa0[('filamentB', '100W'), 691],\\n\\xa0[('filamentB', '200W'), 561]]\\nWe have created a paired RDD named filDataComplexKeyData. It can be easily \\nobserved that it has complex keys. The keys are a combination of filament type and bulb \\npower. The rest of the exercise will move as in the previous step. In the following code, we \\nare going to include an extra 1 in the values:\\n>>> filDataComplexKeyData1 = filDataComplexKeyData.map(lambda val : [val[0] \\n,[val[1],1]])\\n>>> filDataComplexKeyData1.take(4)\\nHere is the output:\\n[[('filamentA', '100W'), [605, 1]],\\n\\xa0[('filamentB', '100W'), [683, 1]],\\n\\xa0[('filamentB', '100W'), [691, 1]],\\n\\xa0[('filamentB', '200W'), [561, 1]]]\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 144}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n126\\nOur required paired RDD, filDataComplexKeyData1, has been created. Now we can \\napply the reduceByKey() function to get the sum and count, based on the complex keys:\\n>>> filDataComplexKeySumCount = filDataComplexKeyData1.reduceByKey(lambda \\nl1,l2 : [l1[0]+l2[0], l1[1]+l2[1]])\\n>>> filDataComplexKeySumCount.collect()\\nHere is the output:\\n[(('filamentA', '100W'), [2514, 4]),\\n\\xa0(('filamentB', '200W'), [2285, 4]),\\n\\xa0(('filamentB', '100W'), [2756, 4]),\\n\\xa0(('filamentA', '200W'), [2170, 4])]\\nAfter getting the sum, the mean can be calculated as follows:\\n>>> filDataComplexKeyMeanCount = filDataComplexKeySumCount.map(lambda val : \\n[val[0],[float(val[1][0])/val[1][1],val[1][1]]])\\n>>> filDataComplexKeyMeanCount.collect()\\n[[('filamentA', '100W'), [628.5, 4]],\\n\\xa0[('filamentB', '200W'), [571.25, 4]],\\n\\xa0[('filamentB', '100W'), [689.0, 4]],\\n\\xa0[('filamentA', '200W'), [542.5, 4]]]\\nFinally, we have calculated the mean and the count value.\\n■\\n■Note\\u2003  The following is a good tutorial about working with\\nreduceByKey(): http://stackoverflow.com/questions/30145329/reducebykey-how-\\ndoes-it-work-internally.\\nRecipe 5-3. Join Data\\nProblem\\nYou want to join data.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 145}, page_content='Chapter 5 ■ The Power of Pairs: Paired RDDs \\n127\\nSolution\\nWe have been given two tables: a Students table (Table\\xa05-2) and a Subjects table (Table\\xa05-3).\\nStudent ID\\nName\\nGender\\nsi1\\nRobin\\nM\\nsi2\\nMaria\\nF\\nsi3\\nJulie\\nF\\nsi4\\nBob\\nM\\nsi6\\nWilliam\\nM\\nTable 5-2.\\u2002 Students\\nYou want to perform the following on the Students and Subjects tables:\\n• \\nInner join\\n• \\nLeft outer join\\n• \\nRight outer join\\n• \\nFull outer join\\nJoining data tables is an integral part of data preprocessing. We are going to perform \\nfour types of data joins in this recipe.\\nAn inner join returns all the keys that are common to both tables. It discards the key \\nelements that are not common to both tables. In PySpark, an inner join is done by using \\nthe join() method defined on the RDD.\\nStudent ID\\nSubjects\\nsi1\\nPython\\nsi2\\nPython\\nsi4\\nPython\\nsi3\\nJava\\nsi3\\nRuby\\nsi4\\nC++\\nsi5\\nC\\nsi1\\nJava\\nsi2\\nJava\\nTable 5-3.\\u2002 Subjects'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 146}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n128\\nA left outer join includes all keys in the left table and excludes uncommon keys from \\nthe right table. A left outer join can be performed by using the leftOuterJoin() function \\ndefined on the RDD in PySpark.\\nAnother important type of join is a right outer join. In a right outer join, every key of \\nthe second table is included, but from the first table, only those keys that are common \\nto both tables are included. We can do a right outer join by using the rightOuterJoin() \\nfunction in PySpark.\\nIf you want to include all keys from both tables, go for a full outer join. It can be \\nperformed by using fullOuterJoin().\\nHow It Works\\nWe’ll follow the steps in this section to work with joins.\\nStep 5-3-1. Creating Nested Lists\\nLet’s start creating a nested list of our data from the Students table:\\n>>> studentData = [['si1','Robin','M'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si2','Maria','F'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si3','Julie','F'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si4','Bob',\\xa0\\xa0'M'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si6','William','M']]\\nAfter creating our Students table data, the next step is to create a nested list of \\nSubjects table data:\\n>>> subjectsData = [['si1','Python'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si3','Java'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si1','Java'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si2','Python'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si3','Ruby'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si4','C++'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si5','C'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si4','Python'],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si2','Java']]\\nWe have created nested lists from the Students table and Subjects table.\\nStep 5-3-2. Creating a Paired RDD of Students and Subjects\\nBefore creating a paired RDD, we first have to create a single RDD. Let’s create \\nstudentRDD:\\n>>> studentRDD = sc.parallelize(studentData, 2)\\n>>> studentRDD.take(4)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 147}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n129\\nHere is the output:\\n[['si1', 'Robin', 'M'],\\n\\xa0['si2', 'Maria', 'F'],\\n\\xa0['si3', 'Julie', 'F'],\\n\\xa0['si4', 'Bob', 'M']]\\nWe can see that, every element of the studentRDD RDD is a list, and each list has \\nthree elements. Now we have to transform it into a paired RDD:\\n>>> studentPairedRDD = studentRDD.map(lambda val : (val[0],[val[1],val[2]]))\\n>>> studentPairedRDD.take(4)\\nHere is the output:\\n[('si1', ['Robin', 'M']),\\n('si2', ['Maria', 'F']),\\n\\xa0('si3', ['Julie', 'F']),\\n\\xa0('si4', ['Bob', 'M'])]\\nThe paired RDD, studentPairedRDD, has the student ID as the key. Now we have to \\ncreate a paired RDD of the subject data:\\n>>> subjectsPairedRDD = sc.parallelize(subjectsData, 2)\\n>>> subjectsPairedRDD.take(4)\\nHere is the output:\\n[['si1', 'Python'],\\n\\xa0['si3', 'Java'],\\n\\xa0['si1', 'Java'],\\n\\xa0['si2', 'Python']]\\nWe do not need to do anything extra to create a paired RDD of subject data.\\nStep 5-3-3. Performing an Inner Join\\nAs we know, an inner join in PySpark is done by using the join() function. We \\nhave to apply this function on the paired RDD studentPairedRDD, and provide \\nsubjectsPairedRDD as an argument to the join() function:\\n>>> studenSubjectsInnerJoin = studentPairedRDD.join(subjectsPairedRDD)\\n>>> studenSubjectsInnerJoin.collect()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 148}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n130\\nHere is the output:\\n[('si3', (['Julie', 'F'], 'Java')),\\n\\xa0('si3', (['Julie', 'F'], 'Ruby')),\\n\\xa0('si2', (['Maria', 'F'], 'Python')),\\n\\xa0('si2', (['Maria', 'F'], 'Java')),\\n\\xa0('si1', (['Robin', 'M'], 'Python')),\\n\\xa0('si1', (['Robin', 'M'], 'Java')),\\n\\xa0('si4', (['Bob', 'M'], 'C++')),\\n\\xa0('si4', (['Bob', 'M'], 'Python'))]\\nAnalyzing the output of this inner join reveals that the key part contains only keys \\nthat are common to the Students and Subjects tables; these appear in the joined table. \\nThe keys that are not common to both tables are not the part of joined table.\\nStep 5-3-4. Performing a Left Outer Join\\nA left outer join can be performed by using the leftOuterJoin() function:\\n>>> studentSubjectsleftOuterJoin = studentPairedRDD.leftOuterJoin(subjects \\nPairedRDD)\\n>>> studentSubjectsleftOuterJoin.collect()\\nHere is the output:\\n[('si3', (['Julie', 'F'], 'Java')),\\n\\xa0('si3', (['Julie', 'F'], 'Ruby')),\\n\\xa0('si2', (['Maria', 'F'], 'Python')),\\n\\xa0('si2', (['Maria', 'F'], 'Java')),\\n\\xa0('si6', (['William', 'M'], None)),\\n\\xa0('si1', (['Robin', 'M'], 'Python')),\\n\\xa0('si1', (['Robin', 'M'], 'Java')),\\n\\xa0('si4', (['Bob', 'M'], 'C++')),\\n\\xa0('si4', (['Bob', 'M'], 'Python'))]\\nStudent ID si6 is in the Students table but not in the Subjects table. Hence, the left \\nouter join includes si6 in the joined table. Because si6 doesn’t have its counterpart in \\nthe Subjects table, it has None in place of the subject.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 149}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n131\\nStep 5-3-5. Performing a Right Outer Join\\nA right outer join on the Students and Subjects tables can be performed by using the \\nrightOuterJoin() function:\\n>>> studentSubjectsrightOuterJoin = studentPairedRDD.rightOuterJoin(subjects \\nPairedRDD)\\n>>> studentSubjectsrightOuterJoin.collect()\\nHere is the output:\\n[('si3', (['Julie', 'F'], 'Java')),\\n\\xa0('si3', (['Julie', 'F'], 'Ruby')),\\n\\xa0('si2', (['Maria', 'F'], 'Python')),\\n\\xa0('si2', (['Maria', 'F'], 'Java')),\\n\\xa0('si1', (['Robin', 'M'], 'Python')),\\n\\xa0('si1', (['Robin', 'M'], 'Java')),\\n\\xa0('si5', (None, 'C')),\\n\\xa0('si4', (['Bob', 'M'], 'C++')),\\n\\xa0('si4', (['Bob', 'M'], 'Python'))]\\nStudent ID si5 is in only the Subjects table; it is not part of the Students table. \\nTherefore, it appears in the joined table.\\nStep 5-3-6. Performing a Full Outer Join\\nNow let’s perform a full outer join. In a full outer join, keys from both tables will be included:\\n>>> studentSubjectsfullOuterJoin = studentPairedRDD.fullOuterJoin(subjects \\nPairedRDD)\\n>>> studentSubjectsfullOuterJoin.collect()\\nHere is the output:\\n[('si3', (['Julie', 'F'], 'Java')),\\n\\xa0('si3', (['Julie', 'F'], 'Ruby')),\\n\\xa0('si2', (['Maria', 'F'], 'Python')),\\n\\xa0('si2', (['Maria', 'F'], 'Java')),\\n\\xa0('si6', (['William', 'M'], None)),\\n\\xa0('si1', (['Robin', 'M'], 'Python')),\\n\\xa0('si1', (['Robin', 'M'], 'Java')),\\n\\xa0('si5', (None, 'C')),\\n\\xa0('si4', (['Bob', 'M'], 'C++')),\\n\\xa0('si4', (['Bob', 'M'], 'Python'))]\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 150}, page_content='Chapter 5 ■ The Power of Pairs: Paired RDDs \\n132\\nIn the joined table, keys from both tables have been included. Student ID si6 is part \\nof only the Students data, and it appears in the joined table. Similarly, student ID si5 is \\npart of only the Subjects table, but it appears in our joined table.\\nRecipe 5-4. Calculate Page Rank\\nProblem\\nYou want to calculate the page rank of the web-page system illustrated in Figure\\xa05-2.\\na\\nb\\nd\\nc\\nFigure 5-2.\\u2002 A web-page system\\nWe have four web pages (a, b, c, and d) in our system. Web page a has outbound links \\nto pages b, c, and d. Similarly, page b has an outbound link to pages d and c. Web page c \\nhas an outbound link to page b, and page d has an outbound link to pages a and c.\\nSolution\\nThe page-rank algorithm was developed by Sergey Brin and Larry Page. In the algorithm \\nname, page stands for Larry Page. The developers of the page-rank algorithm later \\nfounded Google. It is an iterative algorithm.\\nThe page rank of a particular web page indicates its relative importance within a \\ngroup of web pages. The higher the page rank, the higher up it will appear in a search \\nresult list.\\nThe importance of a page is defined by the importance of all the web pages that \\nprovide an outbound link to the web page in consideration. For example, say that web \\npage X has very high relative importance. Web page X is outbounding to web page Y; \\nhence, web page Y will also have high importance.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 151}, page_content='Chapter 5 ■ The Power of Pairs: Paired RDDs \\n133\\nLet’s summarize the page-rank algorithm:\\n\\t\\n1.\\t\\nInitialize each page with a page rank of 1 or some arbitrary \\nnumber.\\n\\t\\n2.\\t\\nFor i in someSequence, do the following:\\na.\\t\\nCalculate the contribution of each inbound page.\\nb.\\t\\nCalculate the sum of the contributions.\\nc.\\t\\nDetermine the updated page rank as follows:\\n\\xa0updated page rank = 1 – s + s × summation of contributions\\nLet me show you an example. Say PR(a), PR(b), PR(c), and PR(d) are the page ranks \\nof pages a, b, c, and d, respectively. Page d has two inbound links: the first from page a, \\nand the second from page b.\\nNow we have to know how the contribution to page rank by a web page is calculated. \\nThis contribution to page rank is given by the following formula:\\npage rank of contributing page\\nContribution to a page = ___________________________________________________________\\ntotal number of outbounds page from the contributing page\\nIn our example, web page a has three outbound links: the first is to page b, the \\nsecond is to page c, and the last is to page d. So the contribution to page rank of page \\nd by page a is PR(a) / 3. Now we have to calculate the contribution of page b to page d. \\nPage b has two outbound links: the first to page c, and the second to page d. Hence, the \\ncontribution by page b is PR(b) / 2.\\nSo the page rank of page d will be updated as follows, where s is known as the \\ndamping factor:\\nPR(d) = 1 – s + s × (PR(a)/3 + PR(b)/2)\\n■\\n■Note\\u2003  For more information on page rank, visit Wikipedia at https://en.wikipedia.org/ \\nwiki/PageRank.\\nHow It Works\\nFollow the steps in this section to work with page rank.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 152}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n134\\nStep 5-4-1. Creating Nested Lists of Web Pages with Outbound \\nLinks and Initializing Rank\\n>>> pageLinks =\\xa0\\xa0[['a' ,['b','c','d']],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['c', ['b']],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['b', ['d','c']],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['d', ['a','c']]]\\nWe have created a nested list of web pages and their outbound links. Now we have to \\ninitialize the page ranks of all the pages. We are initializing them by 1:\\n>>> pageRanks =\\xa0\\xa0[['a',1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['c',1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['b',1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['d',1]]\\nAfter creating ranks for the nested list, we have to define the number of iterations for \\nrunning the page rank.\\nStep 5-4-2. Writing a Function to Calculate Contributions\\nWe are going to write a function that will take two arguments. The first argument of our \\nfunction is a list of web page URIs, which provide the outbound links to other web pages. \\nThe second argument is the rank of the web page accessed through the outbound links \\nthat are the first argument. The function will return the contribution to all the web pages \\nin the first argument:\\n>>> def rankContribution(uris, rank):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0numberOfUris = len(uris)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0rankContribution = float(rank) / numberOfUris\\n...\\xa0\\xa0\\xa0\\xa0\\xa0newrank =[]\\n...\\xa0\\xa0\\xa0\\xa0\\xa0for uri in uris:\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0newrank.append((uri, rankContribution))\\n...\\xa0\\xa0\\xa0\\xa0\\xa0return newrank\\nThis code is very explicable. Our function, rankContribution, will return the \\ncontribution to the page rank for the list of URIs (first variable). The function will \\nfirst calculate the number of elements in our list URIs; then it will calculate the rank \\ncontributions to the given URIs. And finally, for each URI, the contributed rank will be \\nreturned.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 153}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n135\\nStep 5-4-3. Creating Paired RDDs\\nLet’s first create our paired RDDs of link data:\\n>>> pageLinksRDD\\xa0\\xa0= sc.parallelize(pageLinks, 2)\\n>>> pageLinksRDD.collect()\\nHere is the output:\\n[['a', ['b', 'c', 'd']],\\n\\xa0['c', ['b']],\\n\\xa0['b', ['d', 'c']],\\n\\xa0['d', ['a', 'c']]]\\nAnd then we’ll create the paired RDD of our rank data:\\n>>> pageRanksRDD\\xa0\\xa0= sc.parallelize(pageRanks, 2)\\n>>> pageRanksRDD.collect()\\nHere is the output:\\n[['a', 1],\\n\\xa0['c', 1],\\n\\xa0['b', 1],\\n\\xa0['d', 1]]\\nStep 5-4-4. Creating a Loop for Updating Page Rank\\nNow it is time to write our final loop to update the page rank of every page:\\n>>>numIter = 20\\n>>>s = 0.85\\nWe have defined the number of iterations and the damping factor, s.\\n>>> for i in range(numIter):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0linksRank = pageLinksRDD.join(pageRanksRDD)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0contributedRDD = linksRank.flatMap(lambda x : rankContribution(x[1]\\n[0],x[1][1]))\\n...\\xa0\\xa0\\xa0\\xa0\\xa0sumRanks = contributedRDD.reduceByKey(lambda v1,v2 : v1+v2)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0pageRanksRDD = sumRanks.map(lambda x : (x[0],(1-s)+s*x[1]))\\n...\\nLet’s investigate our for loop. First we join pageLinksRDD to pageRanksRDD via an \\ninner join. Then the second line of the for block calculates the contribution to the page \\nrank by using the rankContribution() function we defined previously.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 154}, page_content=\"Chapter 5 ■ The Power of Pairs: Paired RDDs \\n136\\nIn the next line, we aggregate all the contributions we have. In the last line, we update \\nthe rank of each web page by using the map() function. Now it is time to enjoy the result:\\n>>> pageRanksRDD.collect()\\nHere is the output:\\n[('b', 1.3572437951279825),\\n\\xa0('c', 1.2463781024360086),\\n\\xa0('d', 0.8746512999550939),\\n\\xa0('a', 0.521726802480915)]\\nFinally, we have the estimated page rank for every page.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 155}, page_content='137\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_6 \\nCHAPTER 6\\nI/O in PySpark\\nFile input/output (I/O) operations are an integral part of many software activities  \\nand for data\\nA data scientist deals with many types of files, including text files, comma-separated \\nvalues (CSV) files, JavaScript Object Notation (JSON) files, and many more. The Hadoop \\nDistributed File System (HDFS) is a very good distributed file system.\\nThis chapter covers the following recipes:\\nRecipe 6-1. Read a simple text file\\nRecipe 6-2. Write an RDD to a simple text file\\nRecipe 6-3. Read a directory\\nRecipe 6-4. Read data from HDFS\\nRecipe 6-5. Save an RDD to HDFS\\nRecipe 6-6. Read data from a sequential file\\nRecipe 6-7. Write data into a sequential file\\nRecipe 6-8. Read a CSV file\\nRecipe 6-9. Write an RDD to a CSV file\\nRecipe 6-10. Read a JSON file\\nRecipe 6-11. Write an RDD to a JSON file\\nRecipe 6-12. Read table data from HBase by using PySpark\\nRecipe 6-1. Read a Simple Text File\\nProblem\\nYou want to read a simple text file.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 156}, page_content=\"Chapter 6 ■ I/O in PySpark\\n138\\nSolution\\nYou have a simple text file named shakespearePlays.txt. The file content is as follows:\\nLove’s Labour’s Lost\\nA Midsummer Night’s Dream\\nMuch Ado About Nothing\\nAs You Like It\\nThe shakespearePlays.txt file has four lines. You want to read this file by using \\nPySpark. After reading the file, you want to calculate the following:\\n• \\nTotal number of lines in the file\\n• \\nTotal number of characters in the file\\nTo read a simple file, you can use two functions: textFile() and wholeTextFiles(). \\nThese two functions are defined on our SparkContext object.\\nThe textFile() method reads a text file and results in an RDD of lines. The \\ntextFile() method is a transformation, so textFile() does not read the data until the first \\naction is called. Because the file is not available at the time textFile() is run, it will not \\nthrow an error. It will throw an error when the first action is called. Why is this? Like other \\ntransformations, the textFile() function will be called when the first action is called.\\nAnother method, wholeTextFiles(), works in similar way as textFile() except it \\nreads the file as a key/value pair. The file name is read as the key, and the file data is read \\nas the value associated with that key.\\nHow It Works\\nLet’s see how these built-in methods work.\\nStep 6-1-1. Reading a Text File by Using the textFile() Function\\nThe textFile() function takes three inputs. The first input to textFile() is the path of \\nthe file that has to be read. The second argument is minPartitions, which defines the \\nminimum number of data partitions in the RDD. The third argument is use_unicode. If \\nuse_unicode is False, the file is read as a string. Here is the textFile() function:\\n>>> playData = sc.textFile('/home/pysparkbook/pysparkBookData/\\nshakespearePlays.txt',2)\\nHere is the output:\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 157}, page_content='Chapter 6 ■ I/O in PySpark\\n139\\n17/03/18 07:10:17 INFO MemoryStore: Block broadcast_8 stored as values in \\nmemory (estimated size 61.8 KB, free 208.0 KB)\\n17/03/18 07:10:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes \\nin memory (estimated size 19.3 KB, free 227.3 KB)\\nIn the preceding code, we have provided the file path and set minPartitions to 2.\\n>>> playDataList = playData.collect()\\n>>> type(playDataList)\\nHere is the output:\\n<type \\'list\\'>\\nWe can see that the textFile() function has read the file and created the RDD. The \\nelements of the RDD are lines:\\n>>> playDataList[0:4]\\nHere is the output:\\n\\xa0\\xa0[u\"Love\\'s Labour\\'s Lost\",\\n\\xa0\\xa0\\xa0u\"A Midsummer Night\\'s Dream\",\\n\\xa0\\xa0\\xa0u\\'Much Ado About Nothing\\',\\n\\xa0\\xa0\\xa0u\\'As You Like It\\']\\nStep 6-1-2. Reading a Text File by Using wholeTextFiles()\\nThe wholeTextFiles() function also takes three inputs. The first input to \\nwholeTextFiles() is the path of the file that has to be read. The second argument is \\nminPartitions, which defines the minimum number of data partitions in the RDD. The \\nthird argument is use_unicode. If use_unicode is False, the file is read as a string. Let’s \\nread the same text file, this time by using the wholeTextFiles() function:\\n>>> playData = sc.wholeTextFiles(\\'/home/pysparkbook/pysparkBookData/\\nshakespearePlays.txt\\',2)\\nHere is the output:\\n17/03/18 07:19:06 INFO MemoryStore: Block broadcast_11 stored as values in \\nmemory (estimated size 209.0 KB, free 446.0 KB)\\n17/03/18 07:19:06 INFO MemoryStore: Block broadcast_11_piece0 stored as \\nbytes in memory (estimated size 19.4 KB, free 465.4 KB)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 158}, page_content='Chapter 6 ■ I/O in PySpark\\n140\\nAs we know, the wholeTextFiles() function reads the file as a key/value pair, in \\nwhich the file name is the key, and the content is the value. Let’s get the file name:\\n>>> playData.keys().collect()\\nHere is the output:\\n[u\\'file:/home/pysparkbook/pysparkBookData/shakespearePlays.txt\\']\\nAnd now let’s fetch the content of the file:\\n>>> playData.values().collect()\\nHere is the output:\\n[u\"Love\\'s Labour\\'s Lost\",\\n\\xa0u\"A Midsummer Night\\'s Dream\",\\n\\xa0u\\'Much Ado About Nothing\\',\\n\\xa0u\\'As You Like It\\']\\nStep 6-1-3. Counting the Number of Lines in a File\\nWe can count the number of lines in our file by using the count() function:\\n>>> playData.count()\\nHere is the output:\\n4\\nStep 6-1-4. Counting the Number of Characters on Each Line\\nTo calculate the total number of characters in our file, we can calculate the number of \\ncharacters in each line and then sum them. To calculate the total number of characters in \\neach line, we can use the len() function. The len() function is defined in Python, and it \\ncalculates the number of elements in a sequence. For Python strings, the len() function \\nwill return the total number of characters in that string. Let’s observe the workings of the \\nlen() function in the following example:\\n>>> pythonString = \"My python\"\\n>>> len(pythonString)\\nHere is the output:\\n9'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 159}, page_content='Chapter 6 ■ I/O in PySpark\\n141\\nWe have created a string named pythonString. Then we’ve assigned a value,  \\nMy python, to pythonString. The string My python has nine characters, including the \\nspace. Therefore, the len() function on the pythonString variable has returned 9 as  \\nthe output. Now let’s use the len() function in the RDD map() function:\\n>>> playDataLineLength = playData.map(lambda x : len(x))\\n>>> playDataLineLength.collect()\\nHere is the output:\\n[21, 25, 22, 14]\\nFinally, we have calculated the number of characters in each line. If we apply sum() \\non the playDataLineLength RDD, we will get the total number of characters in the file:\\n>>> totalNumberOfCharacters = playDataLineLength.sum()\\n>>> totalNumberOfCharacters\\nHere is the output:\\n82\\nFinally, we have calculated that the total number of characters in our file is 82. \\nRemember, the count doesn’t include the newline character.\\nRecipe 6-2. Write an RDD to a Simple Text File\\nProblem\\nYou want to write an RDD to a simple text file.\\nSolution\\nIn Recipe 6-1, you calculated the number of characters in each line as the RDD \\nplayDataLineLength. Now you want to save it in a text file.\\nWe can save an RDD as a text file by using the saveAsTextFile() function. This \\nmethod is defined on the RDD—not on SparkContext, as we saw in the case of the \\ntextFile() and wholeTextFiles() functions. You have to provide the output directory. \\nThe file name is not required. The directory name you are providing must not already \\nexist; otherwise, the write operation will fail. The RDD exists in partitions. So PySpark will \\nstart many processes in parallel to write the file.\\nThe saveAsTextFile() function takes two inputs. The first input is path, which is \\nbasically the path of the directory where the RDD has to be saved. The second argument \\nis compressionCodecClass, an optional argument with a default value of None.  \\nWe can use compression codecs such as Gzip to compress files and thereby provide \\nmore-efficient computations.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 160}, page_content=\"Chapter 6 ■ I/O in PySpark\\n142\\nHow It Works\\nSo first let’s start with the code for counting the number of characters in each line. We \\nhave already done it, but for the sake of clarity, I have provided the code for calculating \\nthe number of characters in each line again.\\nStep 6-2-1. Counting the Number of Characters on Each Line\\nLet’s read the file and count the number of characters per line:\\n>>> playData = sc.textFile('/home/pysparkbook/pysparkBookData/\\nshakespearePlays.txt',4)\\n>>> playDataLineLength = playData.map(lambda x : len(x))\\n>>> playDataLineLength.collect()\\nHere is the output:\\n[21, 25, 22, 14]\\nOur playDataLineLength RDD has as its elements the number of characters in  \\neach line.\\nStep 6-2-2. Saving the RDD to a File\\nNow that we have the counted RDD, we want to save that RDD into a directory called \\nsavedData:\\n>>> playDataLineLength.saveAsTextFile('/home/pysparkbook/savedData')\\nLet’s investigate the savedData directory. We will find that, inside the  \\ndirectory, there are five files. Four files—part-00000, part-00000, part-00000, and \\npart-00000—will have the data, and the _SUCCESS file denotes that the data has been \\nwritten successfully. Have you wondered why the data is in four files? The answer is that \\nthe playDataLineLength RDD has been distributed in four parts. Four parallel processes \\nhave been used to write the file.\\nLet’s see what is inside those files. We will find that each data point is inside a \\nseparate file:\\nsavedData$ cat part-00000\\nHere is the output:\\n21\\nsavedData$ cat part-00001\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 161}, page_content=\"Chapter 6 ■ I/O in PySpark\\n143\\nHere is the output:\\n25\\nsavedData$ cat part-00002\\nHere is the output:\\n22\\nsavedData$ cat part-00003\\nHere is the output:\\n14\\nRecipe 6-3. Read a Directory\\nProblem\\nYou want to read a directory.\\nSolution\\nIn a directory, there are many files. You want to read the directory (all files at once).\\nReading many files together from a directory is a very common task nowadays. To \\nread a directory, we use the textFile() function or the wholetextFiles() function. The \\ntextFile() function reads small files in the directory and merges them. In contrast, the \\nwholeTextFiles() function reads files as a key/value pair, with the name of file as the \\nkey, and the content of the file as the value.\\nYou are provided with the directory name manyFiles. This directory consists of two \\nfiles named playData1.txt and playData2.txt. Let’s investigate the content of these files \\none by one:\\nmanyFiles$ cat playData1.txt\\nHere is the output:\\nLove's Labour's Lost\\nA Midsummer Night's Dream\\nmanyFiles$ cat playData2.txt\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 162}, page_content='Chapter 6 ■ I/O in PySpark\\n144\\nHere is the output:\\nMuch Ado About Nothing\\nAs You Like It\\nYou job is to read all these files from the directory in one go.\\nHow It Works\\nWe will use both functions, textFile() and wholeTextFiles(), one at a time, to read the \\ndirectory noted previously. Let’s start with textFile().\\nStep 6-3-1. Reading a Directory by Using textFile()\\nIn previous recipes, we provided the absolute file path as the input to the textFile() \\nfunction in order to read the file. The best part of the textFile() function is that just by \\nchanging the path input, we can change how this function reads data. In order to read \\nall files from a directory, we have to provide the absolute path of the directory as input to \\ntextFile(). The following line of code reads all the files in the manyFiles directory by \\nusing the textFile() function:\\n>>> manyFilePlayData = sc.textFile(\\'/home/pysparkbook/pysparkBookData/\\nmanyFiles\\',4)\\n>>> manyFilePlayData.collect()\\nHere is the output:\\n[u\\'Much Ado About Nothing\\',\\n\\xa0u\\'As You Like It\\',\\n\\xa0u\"Love\\'s Labour\\'s Lost\",\\n\\xa0u\"A Midsummer Night\\'s Dream\"]\\nThe output is very clear. The textFile() function has read all the files in the \\ndirectory and merged the content in the files. It has created an RDD of the merged data.\\nStep 6-3-2. Reading a Directory by Using wholeTextFiles()\\nLet’s now read the same set of files by using the wholeTextFiles() function. As we did \\nwith textFile(), here we also provide the path of the directory as one of the inputs to the \\nwholeTextFiles() function. The following code shows the use of the wholeTextFiles() \\nfunction to read a directory:\\n>>> manyFilePlayDataKeyValue = sc.wholeTextFiles(\\'/home/pysparkbook/\\npysparkBookData/manyFiles\\',4)\\n>>> manyFilePlayDataKeyValue.collect()'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 163}, page_content='Chapter 6 ■ I/O in PySpark\\n145\\nHere is the output:\\n[(u\\'file:/home/pysparkbook/pysparkBookData/manyFiles/playData2.txt\\', u\\'Much \\nAdo About Nothing\\\\nAs You Like It\\\\n\\'),\\n\\xa0(u\\'file:/home/pysparkbook/pysparkBookData/manyFiles/playData1.txt\\', \\nu\"Love\\'s Labour\\'s Lost\\\\nA Midsummer Night\\'s Dream\\\\n\")]\\nThe output has key/value pairs of file names and contents. Further, we have to \\nprocess as required by the problem requirements.\\nRecipe 6-4. Read Data from HDFS\\nProblem\\nYou want to read a file from HDFS by using PySpark.\\nSolution\\nHDFS is very good for storing high-volume files. You are given the file filamentData.csv  \\nin HDFS. This file is under the bookData directory. Our bookData is under the root \\ndirectory of HDFS. You want to read this file by using PySpark.\\nTo read a file from HDFS, we first need to know the fs.default.name property  \\nfrom the core-site.xml property file. We are going to get the core-site.xml file  \\ninside the Hadoop configuration directory. For us, the value of fs.default.name is  \\nhdfs://localhost:9746. The full path of our file in HDFS will be  \\nhdfs://localhost:9746/bookData/ filamentData.csv.\\nWe can use the textFile() function to read the file from HDFS by using the full path \\nof the file.\\nHow It Works\\nReading a file from HDFS is as easy as reading data from a local machine. In the following \\ncode line, we use the textFile() function to read the required file:\\n>>> filamentdata = sc.textFile(\\'hdfs://localhost:9746/bookData/filamentData.csv\\',4)\\n>>> filamentdata.take(4)\\nHere is the output:\\n[u\\'filamentA,100W,605\\',\\n\\xa0u\\'filamentB,100W,683\\',\\n\\xa0u\\'filamentB,100W,691\\',\\n\\xa0u\\'filamentB,200W,561\\']\\nThe result confirms that we are able to read our file from HDFS.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 164}, page_content=\"Chapter 6 ■ I/O in PySpark\\n146\\nRecipe 6-5. Save RDD Data to HDFS\\nProblem\\nYou want to save RDD data to HDFS.\\nSolution\\nRDD data can be saved to HDFS by using the saveAsTextFile() function. In Recipe 6-2, \\nwe saved the RDD in a file on the local file system. We are going to save the same RDD, \\nplayDataLineLength, to HDFS.\\nSimilar to the way we worked with the textFile() function, we have to provide the \\nfull path of the file, including the NameNode URI, to saveAsTextFile() to write an RDD \\nto HDFS.\\nHow It Works\\nBecause you are a keen reader and might not be looking for distractions, we’ll start with \\nwriting code that counts the total number of characters in each line.\\nStep 6-5-1. Counting the Number of Characters on Each Line\\n>>> playData = sc.textFile('/home/muser/bData/shakespearePlays.txt',4)\\n>>> playDataLineLength = playData.map(lambda x : len(x))\\n>>> playDataLineLength.collect()\\nHere is the output:\\n[21, 25, 22, 14]\\nStep 6-5-2. Saving an RDD to HDFS\\nThe playDataLineLength RDD is written using following code line:\\n>>> playDataLineLength.saveAsTextFile('hdfs://localhost:9746/savedData/')\\nWe have saved the RDD in the savedData directory, which is inside the root directory \\nof HDFS. Remember that the savedData directory didn’t exist before we saved the data; \\notherwise, the saveAsTextFile() function would throw an error. We have saved the RDD. \\nNow we are going to investigate the saved data. We will find five files in the savedData \\ndirectory: part-00000, part-00001, part-0002, part-00003, and our _SUCCESS file.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 165}, page_content='Chapter 6 ■ I/O in PySpark\\n147\\nLet’s see the data of each file, one by one, by using the HDFS cat command. This \\ncommand displays file data to the console:\\n$ hadoop fs -cat /savedData/part-00000\\nHere is the output:\\n21\\n$ hadoop fs -cat /savedData/part-00001\\nHere is the output:\\n25\\n$ hadoop fs -cat /savedData/part-00002\\nHere is the output:\\n22\\n$ hadoop fs -cat /savedData/part-00003\\nHere is the output:\\n14\\nEach file has a single data point because our RDD has four partitions.\\nRecipe 6-6. Read Data from a Sequential File\\nProblem\\nYou want to read data from a sequential file.\\nSolution\\nA sequential file uses the key/value file format. Here, the key values are in binary format. \\nThis is a commonly used file format for Hadoop. The keys and values are types of the \\nHadoop Writable class.\\nWe have data in a sequential file in HDFS, in the sequenceFileToRead directory \\ninside the root directory. In the file inside the directory, we have the data in Table\\xa06-1.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 166}, page_content=\"Chapter 6 ■ I/O in PySpark\\n148\\nWe can read the sequence file by using the sequenceFile() method defined in the \\nSparkContext class.\\nHow It Works\\nThe sequenceFile() function takes many arguments. Let me discuss some of them. \\nThe first argument is path, which is the path of the sequential file. The second argument \\nis keyClass, which indicates the key class of data in the sequence file. The argument \\nvalueClass represents the data type of the values. Remember that the key and value \\nclasses are children of the Hadoop Writable classes.\\n>>> simpleRDD = sc.sequenceFile('hdfs://localhost:9746/sequenceFileToRead')\\n>>> simpleRDD.collect()\\nHere is the output:\\n[(u'p', 20),\\n(u'q', 30),\\n(u'r', 20),\\n(u'm', 25)]\\nFinally, we have read our sequential file successfully.\\nRecipe 6-7. Write Data to a Sequential File\\nProblem\\nYou want to write data into a sequential file.\\nSolution\\nMany times we like to save the results from PySpark processing to a sequence file. We have \\nan RDD of subject data, as shown in Table\\xa06-2, and you want to write it to a sequence file.\\nTable 6-1.\\u2002 Sequential File Data\\nKey\\nValue\\np\\n20\\nq\\n30\\nr\\n20\\nm\\n25\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 167}, page_content=\"Chapter 6 ■ I/O in PySpark\\n149\\nHow It Works\\nIn this recipe, we are first going to create an RDD and then save it into a sequence file.\\nStep 6-7-1. Creating a Paired RDD\\nFirst, we’ll create a list of tuples:\\n>>> subjectsData = [('si1','Python'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si3','Java'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si1','Java'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si2','Python'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si3','Ruby'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si4','C++'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si5','C'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si4','Python'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('si2','Java')]\\nNext, we parallelize the data:\\n>>> subjectsPairedRDD = sc.parallelize(subjectsData, 4)\\n>>> subjectsPairedRDD.take(4)\\n[('si1', 'Python'),\\n\\xa0('si3', 'Java'),\\n\\xa0('si1', 'Java'),\\n\\xa0('si2', 'Python'),\\n]\\nTable 6-2.\\u2002 Student Subjects Data\\nStudent ID\\nSubjects\\nsi1\\nPython\\nsi4\\nPython\\nsi2\\nPython\\nsi3\\nJava\\nsi3\\nRuby\\nsi4\\nC++\\nsi5\\nC\\nsi1\\nJava\\nsi2\\nJava\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 168}, page_content=\"Chapter 6 ■ I/O in PySpark\\n150\\nStep 6-7-2. Saving the RDD as a Sequence File\\nFinally, we write our paired RDD into a sequence file:\\n>>>subjectsPairedRDD.saveAsSequenceFile('hdfs://localhost:9746/sequenceFiles')\\nOur sequence file is placed in HDFS, inside the sequenceFiles directory. Now let’s \\ninvestigate the saved files:\\n$ hadoop fs -ls /sequenceFiles\\nWe find five items:\\n-rw-r--r--\\xa0\\xa03 pysparkbook\\xa0\\xa0supergroup 0 2017-05-22 00:18 /sequenceFiles /_SUCCESS\\n-rw-r--r--\\xa0\\xa0\\xa03 pysparkbook supergroup\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0114 2017-05-22 00:18 /\\nsequenceFiles/part-00000\\n-rw-r--r--\\xa0\\xa0\\xa03 pysparkbook supergroup\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0114 2017-05-22 00:18 /\\nsequenceFiles/part-00001\\n-rw-r--r--\\xa0\\xa0\\xa03 pysparkbook supergroup\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0111 2017-05-22 00:18 /\\nsequenceFiles/part-00002\\n-rw-r--r--\\xa0\\xa0\\xa03 pysparkbook supergroup\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0128 2017-05-22 00:18 /\\nsequenceFiles/part-00003\\nHere we can see that the files have been saved in four parts.\\nRecipe 6-8. Read a CSV File\\nProblem\\nYou want to read a CSV file.\\nSolution\\nAs we have mentioned, CSV stands for comma-separated values. In a CSV file, each line \\ncontains fields, which are separated by a delimiter. Generally, a comma (,) is used as the \\nseparating delimiter. But a delimiter could be a different character too.\\nWe have seen that if PySpark reads a file, it creates an RDD of lines. So the best way to \\nread a CSV file is to read it by using the textFile() function and then to parse each line \\nby using a CSV parser in Python.\\nIn Python, we can use the csv module to parse CSV lines. This module provides all \\nthe functionality for handling CSV data. We also use the reader() function, which returns \\na reader object. The reader object iterates over lines.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 169}, page_content='Chapter 6 ■ I/O in PySpark\\n151\\nYou have been given a CSV file of filaments data, the same filament data we used in \\nChapter 5. The lines of the filament data file look like the following:\\nfilamentA,100W,605\\nfilamentB,100W,683\\nfilamentB,100W,691\\nfilamentB,200W,561\\nfilamentA,200W,530\\nWe can see that each line in the file is separated by a comma. And so parsing each \\nline by a CSV parser will complete our job.\\n■\\n■Note\\u2003  You can read more about reading a CSV file by using PySpark at  \\nhttps://stackoverflow.com/questions/28782940/load-csv-file-with-spark.\\nHow It Works\\nWe will start with writing a Python function that will parse each line.\\nStep 6-8-1. Writing a Python Function to Parse CSV Lines\\nWe are going to write a function named parseCSV(). This function will take each line and \\nparse it to return a list:\\n>>> import csv\\n>>> import\\xa0\\xa0StringIO\\n>>> def parseCSV(csvRow) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = StringIO.StringIO(csvRow)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dataReader =\\xa0\\xa0csv.reader(data)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return(dataReader.next())\\nOur function takes one line at a time. Each line is taken by the StringIO function \\ndefined in the StringIO module. Using the StringIO module, we can create a file-like \\nobject. We pass the data object to the reader() function of the csv module. It will return \\na reader object. The next() function will return a list of fields that are separated by \\ncommas. Let’s check whether our function is working correctly:\\n>>> csvRow = \"p,s,r,p\"\\n>>> parseCSV(csvRow)\\nHere is the output:\\n[\\'p\\', \\'s\\', \\'r\\', \\'p\\']'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 170}, page_content=\"Chapter 6 ■ I/O in PySpark\\n152\\nWe have created a string, csvRow. We can see that the fields in our string are \\nseparated by commas. Now we are parsing that line by using our function and getting a \\nlist as a result. So it is confirmed that our function meets our expectations.\\nStep 6-8-2. Creating a Paired RDD\\nWe will read the file and parse its lines. The following code will read the file, parse the \\nlines, and return the required data:\\n>>> filamentRDD =sc.textFile('/home/pysparkbook/pysparkBookData \\nfilamentData.csv',4)\\n>>> filamentRDDCSV = filamentRDD.map(parseCSV)\\n>>> filamentRDDCSV.take(4)\\nHere is the output:\\n[['filamentA', '100W', '605'],\\n\\xa0['filamentB', '100W', '683'],\\n\\xa0['filamentB', '100W', '691'],\\n\\xa0['filamentB', '200W', '561']]\\nThe final output returns a nested list. Each inside list consists of parsed lines.\\nRecipe 6-9. Write an RDD to a CSV File\\nProblem\\nYou want to write an RDD to a CSV file.\\nSolution\\nWriting data to a CSV file requires that we transform the list of fields into strings of  \\ncomma-separated fields. A list can be transformed to a string as elements are concatenated.\\nHow It Works\\nStep 6-9-1. Creating a Function to Convert a List into a String\\nLet’s create a function named createCSV, which will take a list and return a string by \\njoining the elements of the list that are separated by commas.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 171}, page_content=\"Chapter 6 ■ I/O in PySpark\\n153\\n>>> import csv\\n>>> import StringIO\\n>>> def createCSV(dataList):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = StringIO.StringIO()\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dataWriter = csv.writer(data,lineterminator='')\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dataWriter.writerow(dataList)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return (data.getvalue())\\nThe StringIO() function returns a file-like object. Then the writerow() function of \\nthe csv module transforms it into a string. Let’s observe the action of the createCSV() \\nfunction:\\n>>> listData = ['p','q','r','s']\\n>>> createCSV(listData)\\nHere is the output:\\n'p,q,r,s'\\nThe listData list has four elements. We provide listData as input to the \\ncreateCSV() function. And, finally, we get a string, which is created by concatenating \\nelements of the list separated by commas.\\nStep 6-9-2. Saving Data to a File\\nOur problem asks us to save the data to a file. But let’s create an RDD and then save the \\ndata. We are going to create an RDD from our simpleData nested list:\\n>>> simpleData = [['p',20],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['q',30],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['r',20],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['m',25]]\\n>>> simpleRDD = sc.parallelize(simpleData,4)\\n>>> simpleRDD.take(4)\\nHere is the output:\\n[['p', 20],\\n\\xa0['q', 30],\\n\\xa0['r', 20],\\n\\xa0['m', 25]]\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 172}, page_content='Chapter 6 ■ I/O in PySpark\\n154\\nWe have created our required RDD. Now, using the map() function, we’ll transform \\nour data into the required format for saving:\\nsimpleRDDLines = simpleRDD.map( createCSV)\\nsimpleRDDLines.take(4)\\nsimpleRDDLines.saveAsTextFile(\\'/home/pysparkbook/csvData/\\')\\nWe have saved our data as a CSV file in the csvData directory.\\nRecipe 6-10. Read a JSON File\\nProblem\\nYou want to read a JSON file by using PySpark.\\nSolution\\nAs noted previously in the chapter, JSON stands for JavaScript Object Notation. The \\npopularity of JSON data files has increased over the decades. It is a data interchange \\nformat. Nearly all programming languages support reading and writing JSON. JSON \\nformat has like fields name and values pair. Two fields name and value pair is separated \\nby comma (,), and each field name and associated value are separated by a colon (:). \\nData scientists often get data from clients in JSON format.\\nYou have been given a JSON file, tempData.json. JSON files have a .json extension. \\nThis file has data in the following format:\\n$ cat tempData.json\\nHere is the output:\\n{\"Time\":\"6AM\",\\xa0\\xa0\"Temperature\":15}\\n{\"Time\":\"8AM\",\\xa0\\xa0\"Temperature\":16}\\n{\"Time\":\"10AM\", \"Temperature\":17}\\n{\"Time\":\"12AM\", \"Temperature\":17}\\n{\"Time\":\"2PM\",\\xa0\\xa0\"Temperature\":18}\\n{\"Time\":\"4PM\",\\xa0\\xa0\"Temperature\":17}\\n{\"Time\":\"6PM\",\\xa0\\xa0\"Temperature\":16}\\n{\"Time\":\"8PM\",\\xa0\\xa0\"Temperature\":14}\\nOur JSON file consists of two keys, Time and Temperature, and their associated \\nvalues. We can see that for a particular field, the field name and value are associated by a \\ncolon. And the two fields’ name/value pairs are separated by a comma.\\nWhat is the big deal about reading a JSON file in PySpark? Now you understand that \\nwhenever we read a JSON file, it will create an RDD of lines and then parse the data line \\nby line. So we need a parser to parse the lines, and more precisely, we need a JSON data \\nparser.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 173}, page_content='Chapter 6 ■ I/O in PySpark\\n155\\nHow It Works\\nMany Python modules are available for JSON parsing, but we are going to use the json \\nPython module. This module provides many utilities for working with JSON format.\\nStep 6-10-1. Creating a Function to Parse JSON Data\\nLet’s create a function that will parse JSON data and return a Python list for each line:\\n>>> import json\\n>>> def jsonParse(dataLine):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0parsedDict = json.loads(dataLine)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0valueData = parsedDict.values()\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return(valueData)\\nThe jsonParse() function will take a string in JSON format and return a list. Input \\nto the jsonParse() function is passed to the json.loads() function. The json.loads() \\nfunction takes a JSON string as an argument and returns a dictionary. Then we fetch \\nvalues in the next line and finally return it. We must test our function:\\n>>> jsonData = \\'{\"Time\":\"6AM\",\\xa0\\xa0\"Temperature\":15}\\'\\nWe create a JSON string, jsonData:\\n>>> jsonParsedData = jsonParse(jsonData)\\n>>> print jsonParsedData\\nHere is the output:\\n\\xa0[15, \\'6AM\\']\\nTo parse our JSON string, we use the jsonParse() function and get the Python list \\njsonParsedData.\\nStep 6-10-2. Reading the File\\nWe are going to read the file by using textFile():\\n>>> tempData = sc.textFile(\"/home/pysparkbook//pysparkBookData/tempData.json\",4)\\n>>> tempData.take(4)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 174}, page_content='Chapter 6 ■ I/O in PySpark\\n156\\nHere is the output:\\n[u\\'{\"Time\":\"6AM\",\\xa0\\xa0\"Temperature\":15}\\',\\n\\xa0u\\'{\"Time\":\"8AM\",\\xa0\\xa0\"Temperature\":16}\\',\\n\\xa0u\\'{\"Time\":\"10AM\", \"Temperature\":17}\\',\\n\\xa0u\\'{\"Time\":\"12AM\", \"Temperature\":17}\\']\\nWe have read the file, and the output tells us that the RDD is a line of JSON strings.\\nStep 6-10-3. Creating a Paired RDD\\nNow we have to parse each line. This can be done by passing the jsonParse() function as \\nan argument to the map() function on our RDD tempData:\\n>>> tempDataParsed = tempData.map(jsonParse)\\n>>> tempDataParsed.take(4)\\nHere is the output:\\n[[15, u\\'6AM\\'],\\n\\xa0[16, u\\'8AM\\'],\\n\\xa0[17, u\\'10AM\\'],\\n\\xa0[17, u\\'12AM\\']]\\nFinally, we have the required result.\\nRecipe 6-11. Write an RDD to a JSON File\\nProblem\\nYou want to write an RDD to a JSON file.\\nSolution\\nJSON string objects can be created from a Python dictionary by using the json module.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 175}, page_content='Chapter 6 ■ I/O in PySpark\\n157\\nHow It Works\\nStep 6-11-1. Creating a Function That Takes a List and Returns \\na JSON String\\nWe are going to create a function, createJSON(), that will take a Python list and return a \\nJSON string:\\n>>> def createJSON(data):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0dataDict = {}\\n...\\xa0\\xa0\\xa0\\xa0\\xa0dataDict[\\'Name\\'] = data[0]\\n...\\xa0\\xa0\\xa0\\xa0\\xa0dataDict[\\'Age\\'] = data[1]\\n...\\xa0\\xa0\\xa0\\xa0\\xa0return(json.dumps(dataDict))\\nInside the createJSON() function, we first create a dataDict dictionary. Then we \\npass that dictionary to the dumps() function of the json module. This function returns a \\nJSON string object. The following line of code will test our function:\\n>>> nameAgeList = [\\'Arun\\',22]\\n>>> createJSON(nameAgeList)\\nHere is the output:\\n\\'{\"Age\": 22, \"Name\": \"Arun\"}\\'\\nOur function returns a JSON string object.\\nStep 6-11-2. Saving Data in JSON Format\\nWe are going to follow the same procedure as before. First, we will create an RDD. Then \\nwe will write that RDD to a file in JSON format.\\n>>> nameAgeData = [[\\'Arun\\',22],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[\\'Bony\\',35],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[\\'Juna\\',29]]\\n>>> nameAgeRDD = sc.parallelize(nameAgeData,3)\\n>>> nameAgeRDD.collect()\\nHere is the output:\\n[[\\'Arun\\', 22],\\n\\xa0[\\'Bony\\', 35],\\n\\xa0[\\'Juna\\', 29]]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 176}, page_content='Chapter 6 ■ I/O in PySpark\\n158\\nWe have created an RDD; now we have to transform this data into JSON strings. The \\ncreateJSON() function is being passed to the map() function as an argument. The map() \\nfunction works on our nameAgeRDD RDD.\\n>>> nameAgeJSON = nameAgeRDD.map(createJSON)\\n>>> nameAgeJSON.collect()\\nHere is the output:\\n[\\'{\"Age\": 22, \"Name\": \"Arun\"}\\',\\n\\xa0\\'{\"Age\": 35, \"Name\": \"Bony\"}\\',\\n\\xa0\\'{\"Age\": 29, \"Name\": \"Juna\"}\\']\\nThe nameAgeJSON RDD elements are JSON strings. Now, using the saveAsTextFile() \\nfunction, we save the nameAgeJSON RDD to the jsonDir directory.\\n>>> nameAgeJSON.saveAsTextFile(\\'/home/pysparkbook/jsonDir/\\')\\nWe are going to investigate the jsonDir directory. We use the ls command to find \\nthat there are four files:\\njsonDir$ ls\\nHere is the output:\\npart-00000\\npart-00001\\npart-00002\\n_SUCCESS\\njsonDir$ cat part-00000\\nHere is the output:\\n{\"Age\": 22, \"Name\": \"Arun\"}\\nThe part-00000 file contains the first element of the RDD.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 177}, page_content='Chapter 6 ■ I/O in PySpark\\n159\\nRecipe 6-12. Read Table Data from HBase by \\nUsing PySpark\\nProblem\\nYou want to read table data from HBase.\\nSolution\\nWe have been given a data table named pysparkTable in HBase. You want to read that \\ntable by using PySpark. The data in pysparkTable is shown in Table\\xa06-3.\\nTable 6-3.\\u2002 pysparkTable Data\\nRowID\\nbtcf1\\nbtc1\\nc11\\nc12\\nc13\\nc14\\n00001\\n00002\\n00003\\n00004\\nbtcf2\\nbtc2\\nc21\\nc22\\nc23\\nc24\\nLet’s explain this table data. The pysparkTable table consists of four rows and two \\ncolumn families, btcf1 and btcf2. Column btc1 is under column family btcf1, and \\ncolumn btc2 is under column family btcf2. Remember that the code presented later in \\nthis section will work only with spark-1.6 and the older PySpark versions. Try tweaking \\nthe code to run on PySpark version 2.x.\\nWe are going to use the newAPIHadoopRDD() function, which is defined on \\nSparkContext sc. This function returns a paired RDD. Table\\xa06-4 lists the arguments of \\nthe newAPIHadoopRDD() function.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 178}, page_content=\"Chapter 6 ■ I/O in PySpark\\n160\\nHow It Works\\nLet’s first define all the arguments that have to be passed into our  \\nnewAPIHadoopRDD() function:\\n>>> hostName = 'localhost'\\n>>> tableName = 'pysparkBookTable'\\n>>>\\xa0\\xa0\\xa0ourInputFormatClass='org.apache.hadoop.hbase.mapreduce.TableInputFormat'\\n>>> ourKeyClass='org.apache.hadoop.hbase.io.ImmutableBytesWritable'\\n>>> ourValueClass='org.apache.hadoop.hbase.client.Result'\\n>>> \\x07ourKeyConverter='org.apache.spark.examples.pythonconverters.\\nImmutableBytesWritableToStringConverter'\\n>>> \\x07ourValueConverter='org.apache.spark.examples.pythonconverters.\\nHBaseResultToStringConverter'\\n>>> configuration = {}\\n>>> configuration['hbase.mapreduce.inputtable'] = tableName\\n>>> configuration['hbase.zookeeper.quorum'] = hostName\\nNow it is time to call the newAPIHadoopRDD() function with its arguments:\\n>>> tableRDDfromHBase = sc.newAPIHadoopRDD(\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputFormatClass = ourInputFormatClass,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0keyClass = ourKeyClass,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0valueClass = ourValueClass,\\nTable 6-4.\\u2002 Arguments of the newAPIHadoopRDD() Function\\nArgument\\nDescription\\nFully qualified classname of Hadoop\\nInputFormat (e.g.,\\n“org.apache.hadoop.mapreduce.lib.input.TextI\\nnputFormat”)\\nFully qualified classname of value Writable\\nclass (e.g.,\\n“org.apache.hadoop.io.LongWritable”)\\nThe number of Python objects represented as\\na single Java object. (default 0, choose\\nbatchSize automatically)\\nkeyClass\\ninputFormatClass\\nkeyConverter\\nKey converter\\nvalueConverter\\nValue converter\\nHadoop configuration, passed in as a dict\\nconf\\nbatchSize\\nvalueClass\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 179}, page_content='Chapter 6 ■ I/O in PySpark\\n161\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0keyConverter = ourKeyConverter,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0valueConverter = ourValueConverter,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0conf = configuration\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\nLet’s see how our paired RDD, tableRDDfromHBase, looks:\\n>>> tableRDDfromHBase.take(2)\\nHere is the output:\\n[(u\\'00001\\', u\\'{\"qualifier\" : \"btc1\", \"timestamp\" : \"1496715394968\", \\n\"columnFamily\" : \"btcf1\", \"row\" : \"00001\", \"type\" : \"Put\", \"value\" : \"c11\"}\\\\\\nn{\"qualifier\" : \"btc2\", \"timestamp\" : \"1496715408865\", \"columnFamily\" : \\n\"btcf2\", \"row\" : \"00001\", \"type\" : \"Put\", \"value\" : \"c21\"}\\'), (u\\'00002\\', \\nu\\'{\"qualifier\" : \"btc1\", \"timestamp\" : \"1496715423206\", \"columnFamily\" : \\n\"btcf1\", \"row\" : \"00002\", \"type\" : \"Put\", \"value\" : \"c12\"}\\\\n{\"qualifier\" : \\n\"btc2\", \"timestamp\" : \"1496715436087\", \"columnFamily\" : \"btcf2\", \"row\" : \\n\"00002\", \"type\" : \"Put\", \"value\" : \"c22\"}\\')]\\nThe paired RDD tableRDDfromHBase has RowID as a key. The columns and column \\nclassifiers are JSON strings, which is the value part. In a previous recipe, we solved the \\nproblem of reading JSON files.\\n■\\n■Note\\u2003  Remember, Recipe 6-12 code will work with only spark-1.6 and before. \\nYou can get the code on GitHub at https://github.com/apache/spark/blob/\\ned9d80385486cd39a84a689ef467795262af919a/examples/src/main/python/hbase_\\ninputformat.py.\\nThere is another twist. We are using many classes, so we have to add some JARs \\nwhile starting the PySpark shell. The following are the JAR files:\\n• \\nspark-examples-1.6.0-hadoop2.6.0.jar\\n• \\nhbase-client-1.2.4.jar\\n• \\nhbase-common-1.2.4.jar\\nThe following is the command to start the PySpark shell:\\npyspark --jars \\'spark-examples-1.6.0-hadoop2.6.0.jar\\',\\'/hbase-client-\\n1.2.4.jar\\',\\'hbase-common-1.2.4.jar\\''),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 180}, page_content='163\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_7 \\nCHAPTER 7\\nOptimizing PySpark and \\nPySpark Streaming\\nSpark is a distributed framework for facilitating parallel processing. The parallel \\nalgorithms require computation and communication between machines. While \\ncommunicating, machines send or exchange data. This is also known as shuffling.\\nWriting code is easy. But writing a program that is efficient and easy to understand \\nby others requires more effort. This chapter presents some techniques for making the \\nPySpark program clearer and more efficient.\\nMaking decisions is a day-to-day activity. Our data-conscious population wants to \\ninclude data analysis and result inference at the time of decision-making. We can gather \\ndata and do analysis, and we have done all of that in previous chapters. But people are \\nbecoming more interested in analyzing data as it is coming in. This means people are \\nbecoming more interested in analyzing streaming data.\\nHandling streaming data requires more robust systems and proper algorithms.  \\nThe fault-tolerance of batch-processing systems is sometimes less complex than the  \\nfault-tolerance of a streaming-execution system. This is because in stream data \\nprocessing, we are reading data from an outer source, running execution, and saving the \\nresults, all at the same time. More activities translate into a greater chance of failure.\\nIn PySpark, streaming data is handled by its library, PySpark Streaming. PySpark \\nStreaming is a set of APIs that provide a wrapper over PySpark Core. These APIs are \\nefficient and deal with many aspects of fault-tolerance too. We are going to read \\nstreaming data from the console by using PySpark and then analyze it. We are also going \\nto read data from Apache Kafka by using PySpark Streaming and then analyze the data.\\nThis chapter covers the following recipes:\\nRecipe 7-1. Optimize the page-rank algorithm using PySpark code\\nRecipe 7-2. Implement the k-nearest neighbors algorithm \\nusing PySpark\\nRecipe 7-3. Read streaming data from the console using \\nPySpark Streaming\\nRecipe 7-4. Integrate Apache Kafka with PySpark Streaming, \\nand read and analyze the data'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 181}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n164\\nRecipe 7-5. Execute a PySpark script in local mode\\nRecipe 7-6. Execute a PySpark script using Standalone Cluster \\nManager and the Mesos cluster manager\\n\\x07Recipe 7-1. Optimize the Page-Rank Algorithm by \\nUsing PySpark Code\\n\\x07Problem\\nYou want to optimize the page-rank algorithm by using the PySpark code you wrote in \\nChapter 5.\\n\\x07Solution\\nWe already implemented the page-rank algorithm in Chapter 5. Can we optimize the \\ncode? Whenever we start looking at every line of code, we can try to optimize it. In this \\nrecipe, we are going to optimize RDD joining. But you might want to look at other parts of \\nthe code and try to optimize different lines of the program.\\nPaired RDD joining is one of the costliest activities in PySpark or any distributed \\nframework. Why? Because for any key in the first RDD, the system looks for all the keys in \\ndifferent data partitions of other RDDs. And then after lookup, the data is shuffled. If we \\nrepartition the data in such a way that the similar keys come to the same machine, then \\nthe data shuffle will be reduced. PySpark provides partitioners for the same purpose. We \\ncan use the partitionBy() function with the partitioner of our choice, so that similar \\nkeys occur on the same machine and less data shuffles. This results in improved speed of \\ncode execution.\\n\\x07How It Works\\nWe already discussed the page-rank algorithm in detail in Chapter 5; if you need a refresher, \\nplease review that detailed discussion. In this recipe, I discuss only the code lines that have \\nbeen modified. The modified code lines are in bold font. Let’s look at those lines:\\n>>> pageLinks =\\xa0\\xa0[['a' ,['b','c','d']],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['b', ['d','c']],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['c', ['b']],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['d', ['a','c']]]\\n>>> pageRanks =\\xa0\\xa0[['a',1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['b',1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['c',1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['d',1]]\\n>>> numIter = 20\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 182}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n165\\n>>> pageRanksRDD\\xa0\\xa0= sc.parallelize(pageRanks, 2).partitionBy(2,hash).persist()\\n>>> pageLinksRDD\\xa0\\xa0= sc.parallelize(pageLinks, 2).partitionBy(2,hash).persist()\\nIn the bold code lines, I have added the partitionBy() method. The first argument \\nof partitionBy() is 2, which tells us that the data is split into two partitions. There \\nis another argument value that we have provided, hash. This argument value is the \\npartitioner name. We repartition the data into two partitions, using the hash partitioner. \\nThe hash partitioner uses a hashing technique to repartition the data. We are using the \\nsame technique on both pageRanksRDD and pageLinksRDD, so the same key will go to the \\nsame machine. Therefore, we are tackling the shuffling problem. The other parts of the \\ncode are the same as in Chapter 5.\\n>>> s = 0.85\\n>>> def rankContribution(uris, rank):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0numberOfUris = len(uris)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0rankContribution = float(rank) / numberOfUris\\n...\\xa0\\xa0\\xa0\\xa0\\xa0newrank =[]\\n...\\xa0\\xa0\\xa0\\xa0\\xa0for uri in uris:\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0newrank.append((uri, rankContribution))\\n...\\xa0\\xa0\\xa0\\xa0\\xa0return newrank\\n>>> for i in range(numIter):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0linksRank = pageLinksRDD.join(pageRanksRDD)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0contributedRDD = linksRank.flatMap(lambda x : rankContribution(x[1]\\n[0],x[1][1]))\\n...\\xa0\\xa0\\xa0\\xa0\\xa0sumRanks = contributedRDD.reduceByKey(lambda v1,v2 : v1+v2)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0pageRanksRDD = sumRanks.map(lambda x : (x[0],(1-s)+s*x[1]))\\n...\\n>>> pageRanksRDD.collect()\\nHere is the output:\\n[('b', 1.357243795127982),\\n\\xa0('d', 0.8746512999550939),\\n\\xa0('a', 0.5217268024809147),\\n\\xa0('c', 1.2463781024360086)]\\nWe have the final answer for the page-rank implementation.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 183}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n166\\n\\x07Recipe 7-2. Implement the k-Nearest Neighbors \\nAlgorithm by Using PySpark\\n\\x07Problem\\nYou want to implement the k-nearest neighbors (KNN) algorithm by using PySpark.\\n\\x07Solution\\nThe k-nearest neighbors algorithm is one of the simplest data-classification algorithms. \\nThe similarity between two data points is measured on the basis of the distance between \\ntwo points.\\nWe have been given a dataset of nine records. This dataset is shown in Table\\xa07-1. In \\nthis table, you can see a column named RN. That column indicates the record number. \\nThis is not part of the data; the record number is given to help you understand the KNN \\nalgorithm.\\nRN\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nivs1\\n3.09\\n2.96\\n2.87\\n3.02\\n1.80\\n1.36\\n1.71\\n1.03\\n2.30\\nivs2\\n1.97\\n2.15\\n1.93\\n1.55\\n3.65\\n4.43\\n4.35\\n3.75\\n3.59\\nivs3\\n3.73\\n4.16\\n4.39\\n4.43\\n2.08\\n1.95\\n1.94\\n2.12\\n1.99\\nGroup\\ngroup1\\ngroup1\\ngroup1\\ngroup1\\ngroup2\\ngroup2\\ngroup2\\ngroup2\\ngroup2\\nTable 7-1.\\u2002 Data for Classification by KNN\\nLet’s say that we have a record (ivs1 = 2.5, ivs2 = 1.7, ivs3 = 4.2). We will call this \\nrecord new record. We have to classify this record; it will be in either group1 or group2.\\nTo classify the record, we’ll use the KNN algorithm. Here are the steps:\\n\\t\\n1.\\t\\nDecide the k.\\nk is the number of nearest neighbors we are going to choose \\nfor deciding the class of the new record. Let’s say k is 5.\\n\\t\\n2.\\t\\nFind the distance of the new record from each record in the data.\\nThe distance calculation is done using the Euclidean distance \\nmethod, as shown in Table\\xa07-2.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 184}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n167\\nIn this table, we have calculated the distance between the new \\nrecord and other records. The third column is the distance. \\nThe distance value of the first row of this table is the distance \\nbetween the new record and record 1.\\n\\t\\n3.\\t\\nSort the distances.\\nSorting is required now. And we have to sort the distances \\nin increasing order. Simultaneously, we have to maintain \\nthe association between the RN column and the Distance \\ncolumn. In Table\\xa07-3, we have sorted the Distance column, \\nwhich is still associated with the RN and Group columns.\\nRN\\nDistance Calculation with New Record\\nsqrt((1.80-2.5)^2 + (3.65 -1.7)^2 + (2.08 -4.2) ^2)\\nsqrt((1.03-2.5)^2 + (3.75 -1.7)^2 + (2.12 -4.2) ^2)\\nsqrt((2.30-2.5)^2 + (3.59 -1.7)^2 + (1.99 -4.2) ^2)\\nsqrt((1.71-2.5)^2 + (4.35 -1.7)^2 + (1.94 -4.2) ^2)\\nsqrt((1.36-2.5)^2 + (4.43 -1.7)^2 + (1.95 -4.2) ^2)\\nsqrt((3.02-2.5)^2 + (1.55 -1.7)^2 + (4.43 -4.2) ^2)\\nsqrt((2.87-2.5)^2 + (1.93 -1.7)^2 + (4.39 -4.2) ^2)\\nsqrt((2.96-2.5)^2 + (2.15 -1.7)^2 + (4.16 -4.2) ^2)\\nsqrt((3.09-2.5)^2 + (1.97 -1.7)^2 + (3.73 -4.2) ^2)\\nDistance\\n0.80\\n0.65\\n0.47\\n0.58\\n2.96\\n3.71\\n3.57\\n3.26\\n2.91\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nTable 7-2.\\u2002 Distance Calculation\\nRN\\nDistance\\n3\\n0.47\\n4\\n0.58\\n2\\n0.65\\n1\\n0.8\\n9\\n2.91\\n5\\n2.96\\n8\\n3.26\\n7\\n3.57\\n6\\n3.71\\nRN\\nDistance\\nGroup\\ngroup1\\ngroup1\\ngroup1\\ngroup1\\ngroup2\\ngroup2\\ngroup2\\ngroup2\\ngroup2\\n3\\n0.47\\n4\\n0.58\\n2\\n0.65\\n1\\n0.8\\n9\\n2.91\\n5\\n2.96\\n8\\n3.26\\n7\\n3.57\\n6\\n3.71\\nTable 7-3.\\u2002 Distance Calculation'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 185}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n168\\n\\t\\n4.\\t\\nFind the k-nearest neighbors.\\nNow that we have sorted the Distance column, we have to \\nidentify the neighbors of the new record. What do I mean by \\nneighbors here? Neighbors are those records in the table that \\nare near the new record. Near means having less distance \\nbetween two nodes. Now look for the five nearest neighbors \\nin Table\\xa07-3. For the new record, records 3, 4, 2, 1, and 9 are \\nneighbors. The group for records 3, 4, 2, and 1 is group1. The \\ngroup for record 9 is group2. The majority of neighbors are \\nfrom group1. Therefore, we can classify the new record in \\ngroup1.\\nWe have discussed the KNN algorithm in detail. Let’s see how to implement it by \\nusing PySpark. We are going to implement KNN in a naive way and then we will optimize \\nit in the “How It Works” section.\\nFirst, we are going to calculate the distance between two tuples. We’ll write a Python \\nfunction, distanceBetweenTuples(). This function will take two tuples, calculate the \\ndistance between them, and return that distance:\\n>>> def distanceBetweenTuples(data1 , data2) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0squaredSum = 0.0\\n...\\xa0\\xa0\\xa0\\xa0\\xa0for i in range(len(data1)):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0squaredSum = squaredSum + (data1[i] - data2[i])**2\\n...\\xa0\\xa0\\xa0\\xa0\\xa0return(squaredSum**0.5)\\nNow that we’ve written the function to calculate the distance, let’s test it:\\n>>> pythonTuple1 = (1.2, 3.4, 3.2)\\n>>> pythonTuple2 = (2.4, 2.2, 4.2)\\n>>> distanceBetweenTuples(pythonTuple1, pythonTuple2)\\nHere is the output:\\n1.9697715603592207\\nOur method has been tested. It is a general function. We can run it for tuples of \\nlength 4 or 5 also. In the following lines of code, we’ll create a list. The elements of this \\nlist are tuples. Each tuple has two elements. The first element is itself a tuple of data. The \\nsecond element of each tuple is the group associated with each tuple.\\n>>> knnDataList = [((3.09,1.97,3.73),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((2.96,2.15,4.16),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((2.87,1.93,4.39),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((3.02,1.55,4.43),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.80,3.65,2.08),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.36,4.43,1.95),'group2'),\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 186}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n169\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.71,4.35,1.94),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.03,3.75,2.12),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((2.30,3.59,1.99),'group2')]\\n>>> knnDataRDD = sc.parallelize(knnDataList, 4)\\nThe data has been parallelized. We define newRecord as [(2.5, 1.7, 4.2)]:\\n>>> newRecord = [(2.5, 1.7, 4.2)]\\n>>> newRecordRDD = sc.parallelize(newRecord, 1)\\n>>> cartesianDataRDD = knnDataRDD.cartesian(newRecordRDD)\\n>>> cartesianDataRDD.take(5)\\nHere is the output:\\n[(((3.09, 1.97, 3.73), 'group1'), (2.5, 1.7, 4.2)),\\n(((2.96, 2.15, 4.16), 'group1'), (2.5, 1.7, 4.2)),\\n(((2.87, 1.93, 4.39), 'group1'), (2.5, 1.7, 4.2)),\\n(((3.02, 1.55, 4.43), 'group1'), (2.5, 1.7, 4.2)),\\n(((1.8, 3.65, 2.08), 'group2'), (2.5, 1.7, 4.2))]\\nWe have created a Cartesian by using the older record data and the new record data. \\nYou might be wondering why I have created this Cartesian. at the time of defining the \\nlist knnDataList. In a real case, you would have a large file. That file might be distributed \\nalso. So for that condition, we’d have to read the file first and then create the Cartesian. \\nAfter creating the Cartesian, we have the older data and the new record data in the same \\nrow, so we can easily calculate the distance with the map() method:\\n>>> K = 5\\n>>> groupAndDistanceRDD = cartesianDataRDD.map(lambda data : (data[0][1] \\n,distanceBetweenTuples(data[0][0], data[1])))\\n>>> groupAndDistanceRDD.take(5)\\nHere is the output:\\n[('group1', 0.8011866199581719),\\n\\xa0('group1', 0.6447480127925947),\\n\\xa0('group1', 0.47528938553264566),\\n\\xa0('group1', 0.5880476171195661),\\n\\xa0('group2', 2.9642705679475347)]\\nWe have calculated the RDD groupAndDistanceRDD; its first element is the group,  \\nand the second element is the distance between the new record and older records. \\nWe have to sort it now in increasing order of distance. You might remember the\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 187}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n170\\ntakeOrdered() function described in Chapter 4. So let’s get five groups in increasing \\norder of distance:\\n>>> ourClasses = groupAndDistanceRDD.takeOrdered(K, key = lambda data : data[1])\\n>>> ourClasses\\nHere is the output:\\n[('group1', 0.47528938553264566),\\n\\xa0('group1', 0.5880476171195661),\\n\\xa0('group1', 0.6447480127925947),\\n\\xa0('group1', 0.8011866199581719),\\n\\xa0('group2', 2.9148241799463652)]\\nUsing the takeOrdered() method, we have fetched five elements of the RDD, with \\nthe distance in increasing order. We have to find the group that is in the majority. So we \\nhave to first fetch only the group part and then we have to find the most frequent group:\\n>>> ourClassesGroup = [data[0] for data in ourClasses]\\n>>> ourClassesGroup\\nHere is the output:\\n['group1', 'group1', 'group1', 'group1', 'group2']\\nThe group part has been fetched. The most frequent group can be found using the \\nmax() Python function as follows:\\n>>> max(ourClassesGroup,key=ourClassesGroup.count)\\nHere is the output:\\n'group1'\\nWe finally have the group of the new record, and that is group1.\\nYou might be thinking that now that we have implemented KNN, what’s next? Next, \\nwe should optimize the code. Let me say that again. We can optimize different aspects \\nof this code. For this example, we’ll use the broadcasting technique using the broadcast \\nvariable. This is a very good technique for optimizing code.\\nThe Cartesian has been applied to join the older records with the new record. \\nPySpark provides another way to achieve a similar result. We can send the new record to \\nevery executor before. This new record data will be available to each executor, and they \\ncan use it for distance calculations. We can send the new record tuple to all the executors \\nas a broadcast variable.\\nBroadcast variables are shared and read-only variables. Read-only means executors \\ncannot change the value of a broadcast variable; they can only read the value of it. \\nIn PySpark, we create a broadcast variable by using the broadcast() function. This \\nbroadcast() function is defined on SparkContext. We know that in the PySpark console,\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 188}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n171\\nwe have SparkContext as sc. We are going to reimplement the KNN by using the \\nbroadcast technique.\\n\\x07How It Works\\nWe have already discussed most of the code. Therefore, I will keep the discussion short in \\nthe coming steps.\\n\\x07Step 7-2-1. Creating a Function to Calculate the Distance \\nBetween Two Tuples\\n>>> def distanceBetweenTuples(data1 , data2) :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0squaredSum = 0.0\\n...\\xa0\\xa0\\xa0\\xa0\\xa0for i in range(len(data1)):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0squaredSum = squaredSum + (data1[i] - data2[i])**2\\n...\\xa0\\xa0\\xa0\\xa0\\xa0return(squaredSum**0.5)\\nWe have already created and tested this method.\\n\\x07Step 7-2-2. Creating a List of Given Records and Transforming \\nIt to an RDD\\n>>> knnDataList = [((3.09,1.97,3.73),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((2.96,2.15,4.16),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((2.87,1.93,4.39),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((3.02,1.55,4.43),'group1'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.80,3.65,2.08),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.36,4.43,1.95),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.71,4.35,1.94),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((1.03,3.75,2.12),'group2'),\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0((2.30,3.59,1.99),'group2')]\\n>>> K = 5\\nWe again want to go for five neighbors in order to determine the group of the new \\nrecord. We also parallelize the data and transform it to an RDD of four partitions:\\n>>> knnDataRDD = sc.parallelize(knnDataList, 4)\\n>>> knnDataRDD.take(5)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 189}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n172\\nHere is the output:\\n[((3.09, 1.97, 3.73), 'group1'),\\n\\xa0((2.96, 2.15, 4.16), 'group1'),\\n\\xa0((2.87, 1.93, 4.39), 'group1'),\\n\\xa0((3.02, 1.55, 4.43), 'group1'),\\n\\xa0((1.8, 3.65, 2.08), 'group2')]\\n\\x07Step 7-2-3. Broadcasting the Record Value\\nNow we have to create the required new record:\\n>>> newRecord = [(2.5, 1.7, 4.2)]\\nBroadcasting the new record will be done by the broadcast() method, which is \\ndefined on the SparkContext object. We can read the value of the broadcasted variable by \\nusing the value attribute:\\n>>> broadCastedValue = sc.broadcast(newRecord)\\n>>> broadCastedValue.value\\nHere is the output:\\n[(2.5, 1.7, 4.2)]\\nWe can see that it returns the data as we have broadcasted it. But we need the tuple \\nthat we can get by fetching the first element of the list:\\n>>> broadCastedValue.value[0]\\nHere is the output:\\n(2.5, 1.7, 4.2)\\n\\x07Step 7-2-4. Broadcasting the Record Value\\nAfter broadcast, we have to create an RDD that will be an RDD of tuples. Each tuple’s first \\nelement is the group, and the second element is the distance:\\n>>> groupAndDistanceRDD = knnDataRDD.map(lambda data : (data[1] \\n,distanceBetweenTuples(data[0], tuple(broadCastedValue.value[0]))))\\nTo calculate the distance, we use the distanceBetweenTuples() method:\\n>>> groupAndDistanceRDD.take(5)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 190}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n173\\nHere is the output:\\n[('group1', 0.8011866199581719),\\n\\xa0('group1', 0.6447480127925947),\\n\\xa0('group1', 0.47528938553264566),\\n\\xa0('group1', 0.5880476171195661),\\n\\xa0('group2', 2.9642705679475347)]\\nThe requirement is achieved.\\n\\x07Step 7-2-5. Finding the Class of a New Record\\nWe’ll find the class of the new record in the same way as we did in its naive part:\\n>>> ourClasses = groupAndDistanceRDD.takeOrdered(K, key = lambda data : \\ndata[1])\\n>>> ourClasses\\nHere is the output:\\n[('group1', 0.47528938553264566),\\n\\xa0('group1', 0.5880476171195661),\\n\\xa0('group1', 0.6447480127925947),\\n\\xa0('group1', 0.8011866199581719),\\n\\xa0('group2', 2.9148241799463652)]\\n>>> ourClassesGroup = [data[0] for data in ourClasses]\\n>>> ourClassesGroup\\nHere is the output:\\n['group1', 'group1', 'group1', 'group1', 'group2']\\n>>> max(ourClassesGroup,key=ourClassesGroup.count)\\nHere is the output:\\n'group1'\\nWe can see that the class of the new record is group1.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 191}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n174\\n■\\n■Note\\u2003  You can read more about the PySpark broadcast variable in the following blogs:\\nhttps://stackoverflow.com/questions/34499650/spark-broadcast-vs-join\\nhttps://stackoverflow.com/questions/34864751/apache-spark-broadcast-variables- \\nare-type-broadcast-not-a-rdd\\nhttps://stackoverflow.com/questions/40685469/in-spark-how-does-broadcast-work/ \\n40694867\\nYou can find more on KNN on Wikipedia, at https://en.wikipedia.org/wiki/K-nearest_\\nneighbors_algorithm.\\n\\x07Recipe 7-3. Read Streaming Data from the \\nConsole Using PySpark Streaming\\n\\x07Problem\\nYou want to read streaming data from the console by using PySpark Streaming.\\n\\x07Solution\\nNetcat is network utility software. It can read and write data using TCP or UDP. It can be \\nused as a client or server or both. Many options can be provided. We can provide listen \\nmode by using the -l option. And the s option can be used to keep inbound sockets open \\nfor multiple connections. We are going to use a Netcat server to create a console source \\nfor data.\\nOpen a terminal and start a Netcat server by using the command nc -lk 55342. \\nFigure\\xa07-1 depicts the starting of a Netcat server.\\nFigure 7-1.\\u2002 Netcat server in a console'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 192}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n175\\nPySpark Streaming can read data from many sources. In order to read data from a \\nconsole source, we are going to use the socketTextStream() function. This function is \\ndefined on the StreamingContext object. Just as SparkContext is a way to enter PySpark, \\nStreamingContext is a way to enter the PySpark Streaming library and use the APIs \\navailable in the package.\\nJust consider that a source produces numeric data. A source puts data on the \\nconsole. A line of numeric data is created with time. We have to read the console data \\nby using PySpark Streaming, do a summation of each line, and print the result on the \\nconsole. The data is shown here:\\n22 34 21 11\\n22 32 44 11\\n32 43 34 54\\n21 23 32 21\\nThe result looks like this:\\n88.0\\n109.0\\n163.0\\n97.0\\n\\x07How It Works\\n\\x07Step 7-3-1. Starting a Netcat Server\\nA Netcat server can be started by using the nc command. After starting the server, we can \\ntype some data on it. Or we can type some data after connecting PySpark Streaming to \\nthis server. Here is the data:\\n$ nc -lk\\xa0\\xa055342\\n22 34 21 11\\n22 32 44 11\\n32 43 34 54\\n21 23 32 21\\n22 32 21 32\\n21 23 32 21\\n22 32 21 32\\n22 32 21 32\\n32 44 54 32'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 193}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n176\\n\\x07Step 7-3-2. Defining a Function to Sum Row Data\\nIn this step, we’ll define a function named stringToNumberSum(). PySpark will read the \\ndata from the Netcat server as a string. Sometimes a string might have leading and trailing \\nspaces. So first we have to remove the spaces, which can be done by using the strip() \\nfunction defined on the String data type. Sometimes we might get a blank string; we can \\ndeal with that by using an if block. If we get a blank string, we return the None data type. \\nBut if a string is not empty, we split the data points by using the split() function defined \\non the Python string. But remember, even after splitting the data points, they are of the \\nString type. We can change it to a float type by using the float() function or integers by \\nusing the int() function. A string is transformed to a float by using list comprehension. \\nAnd finally, the sum() function is used to calculate the sum of a list of elements, and the \\nfinal result is returned.\\n>>> def stringToNumberSum(data):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0removedSpaceData = data.strip()\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0\\xa0\\xa0removedSpaceData == '' :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return(None)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0splittedData =\\xa0\\xa0removedSpaceData.split(' ')\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0numData =\\xa0\\xa0[float(x) for x in splittedData]\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sumOfData = sum(numData)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return (sumOfData)\\n\\x07Step 7-3-3. Reading Data from the Netcat Server and \\nCalculating the Sum\\nWe should start reading data from the Netcat server by using the PySpark Streaming \\nlibrary. In order to use the APIs defined in the Streaming library, we have to first import \\nStreamingContext and create an object out of it:\\n>>> from pyspark.streaming import StreamingContext\\n>>> pysparkBookStreamingContext = StreamingContext(sc, 10)\\nLet me explain the preceding code. First, we import StreamingContext. Then we \\ncreate an object of StreamingContext. In StreamingContex(), the first argument is the \\nSparkContext object sc, and the second argument is the time in seconds. Using this \\nStreamingContext object pysparkBookStreamingContext, we are going to read data from \\nthe Netcat server by using the socketTextStream() function:\\n>>> consoleStreamingData = pysparkBookStreamingContext.socketTextStream(\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0hostname = 'localhost',\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0port = 55342\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 194}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n177\\nThe first argument of the socketTextStream() method is the hostname where the \\nNetcat server is running. The second argument is the port of the server.\\n>>> sumedData = consoleStreamingData.map(stringToNumberSum)\\n>>> sumedData.pprint()\\nWe do the summation using map(), and then we print it to the console by using the \\npprint() function:\\n>>> pysparkBookStreamingContext.start() ;pysparkBookStreamingContext.\\nawaitTerminationOrTimeout(30)\\nA streaming program will start working only when you apply the start() method. \\nAnd then you have to also provide a function to terminate the execution. We terminate \\nthe execution by using awaitTerminationOrTimeout() after 30 seconds. Both methods, \\nstart() and awaitTerminationOrTimeout(), are defined on StreamingContext. The \\noutput of our code is as follows:\\n-------------------------------------------\\nTime: 2017-08-27 12:58:20\\n-------------------------------------------\\n88.0\\n109.0\\n163.0\\n97.0\\n107.0\\n-------------------------------------------\\nTime: 2017-08-27 12:58:30\\n-------------------------------------------\\n-------------------------------------------\\nTime: 2017-08-27 12:58:40\\n-------------------------------------------\\n97.0\\n107.0\\n\\xa0-------------------------------------------\\nTime: 2017-08-27 12:58:50\\n-------------------------------------------\\n107.0\\n162.0\\n-------------------------------------------\\nTime: 2017-08-27 12:59:00\\n-------------------------------------------\\nFor the time interval of 10 seconds, whatever row it is getting, it returns the sum of \\nnumbers in each row.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 195}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n178\\n\\x07Recipe 7-4. Integrate PySpark Streaming with \\nApache Kafka, and Read and Analyze the Data\\n\\x07Problem\\nYou want to integrate PySpark Streaming with Apache Kafka, and read and analyze the data.\\n\\x07Solution\\nWe discussed Apache Kafka in Chapter 1, and you already know the meaning of topic, \\nproducer, consumer, and broker. You know that Kafka uses ZooKeeper. Let’s see the \\nworking of Apache Kafka. We will start a ZooKeeper server. Then we will create a Kafka \\ntopic. Let’s start a ZooKeeper server first:\\nStarting zookeeper \\nkafka$ bin/zookeeper-server-start.sh config/zookeeper.properties\\nThe ZooKeeper server is started by using the zookeeper-server-start.sh script. \\nThis script is in the bin directory of the Kafka installation. After starting ZooKeeper,  \\nwe are going to create a Kafka topic. This can be created by using the kafka-topics.sh \\nscript, which resides in the bin directory:\\nStarting a Kafka Topic \\nkafka$ bin/kafka-topics.sh --create --zookeeper localhost:2185 \\n--replication-factor 1 --partitions 1 --topic\\xa0\\xa0pysparkBookTopic\\nHere is the output:\\nCreated topic \"pysparkBookTopic\".\\nWe have provided many options to Kafka. The ZooKeeper option provides data about \\nZooKeeper; we provide a replication factor of 1, as we are not replicating the data. On a \\nreal-time server, you are supposed to replicate the data for fault-tolerance.\\nWe have created a topic named pysparkBookTopic.\\nLet’s start the broker:\\nStarting Kafka Server\\nkafka$ bin/kafka-server-start.sh config/server.properties\\nWe are going to start a console producer. The Apache Kafka console producer will \\nread data from the console and produce it to the broker:'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 196}, page_content='Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n179\\nkafka$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic \\npysparkBookTopic\\nWe can start the Kafka console producer by using the kafka-console-producer.sh  \\nscript. You will find this script under the bin directory of Kafka home. The console \\nproducer produces message from the console.\\nLet’s put some data in the console where the console producer has been started:\\n20 25 25 23\\n21 24 21 20\\n20 25 25 23\\n21 23 21 23\\nNow it is time to start a console consumer. The console consumer will finally print \\nthe data on the console:\\nkafka$ bin/kafka-console-consumer.sh --from-beginning --zookeeper \\nlocalhost:2185 --topic pysparkBookTopic\\n20 25 25 23\\n21 24 21 20\\n20 25 25 23\\n21 23 21 23\\nJust after starting the console consumer, you will see the data produced by the \\nconsole producer on the console of the console consumer.\\nNow that you understand the workings of Kafka, we have to rework Recipe 7-3 with \\nsome changes. This time, we’ll read data from the console by using Apache Kafka, and \\nthen from Kafka we will read the data by using PySpark Streaming. After reading the data, \\nwe have to analyze it. We’ll read rows of numbers and calculate the sum of the numbers of \\neach row.\\n\\x07How It Works\\n\\x07Step 7-4-1. Starting ZooKeeper, Creating the Topic, Starting the \\nApache Kafka Broker and the Console Producer\\nkafka$ bin/zookeeper-server-start.sh config/zookeeper.properties\\nkafka$ bin/kafka-server-start.sh config/server.properties\\nkafka$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic \\npysparkBookTopic'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 197}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n180\\nkafka$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic \\npysparkBookTopic\\n32 43 45\\n43 54 57\\n32 21 32\\n34 54 65\\n\\x07Step 7-4-2. Starting PySpark with the spark-streaming-kafka \\nPackage\\n$ pyspark --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0\\n\\x07Step 7-4-3. Creating a Sum of Each Row of Numbers\\nWe already created this function in the previous recipe and explained it too:\\n>>> def stringToNumberSum(data):\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0removedSpaceData = data.strip()\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if\\xa0\\xa0\\xa0removedSpaceData == '' :\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return(None)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0splittedData =\\xa0\\xa0removedSpaceData.split(' ')\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0numData =\\xa0\\xa0[float(x) for x in splittedData]\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sumOfData = sum(numData)\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return (sumOfData)\\n>>> dataInString = '10 10 20 '\\n>>> stringToNumberSum(dataInString)\\n40.0\\nWe have tested this function too.\\n\\x07Step 7-4-4. Reading Data from Kafka and Getting the Sum of \\nEach Row\\nThe function for dealing with Kafka has been defined in the KafKaUtils class. Therefore, \\nwe first have to import it. Then we have to create the StreamingContext object:\\n>>> from pyspark.streaming.kafka import KafkaUtils\\n>>> from pyspark.streaming import StreamingContext\\n>>> bookStreamContext = StreamingContext(sc, 10)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 198}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n181\\nNext we are going to read data from Kafka by using the createStream() method \\ndefined in KafkaUtils:\\n>>> bookKafkaStream = KafkaUtils.createStream(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ssc = bookStreamContext,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0zkQuorum = 'localhost:2185',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0groupId = 'pysparkBookGroup',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0topics = {'pysparkBookTopic':1}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\nThe first argument of the createStream() function is the StreamingContext \\nobject. The second argument is zkQuorum, where we provide the host machine and port \\nof ZooKeeper. The topic is pysparkBookTopic, which we already created, and 1 in the \\ndictionary is the replication factor of the data.\\n>>> sumedData = bookKafkaStream.map( lambda data : \\nstringToNumberSum(data[1]))\\nAfter getting the data, we run a summation on it:\\n>>> sumedData.pprint()\\n>>> bookStreamContext.start() ;bookStreamContext.\\nawaitTerminationOrTimeout(30)\\nHere is the output:\\n-------------------------------------------\\nTime: 2017-08-26 20:21:40\\n-------------------------------------------\\n120.0\\n154.0\\n17/08/26 20:21:44 WARN BlockManager: Block input-0-1503759104200 replicated \\nto only 0 peer(s) instead of 1 peers\\n----------------------------------- --------\\nTime: 2017-08-26 20:21:50\\n------------------------------- ------------\\n85.0\\n17/08/26 20:21:51 WARN BlockManager: Block input-0-1503759110800 replicated \\nto only 0 peer(s) instead of 1 peers\\n---------------------------------- ---------\\nTime: 2017-08-26 20:22:00\\n------------------------------------ -------\\n153.0\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 199}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n182\\n-------------------------------- -----------\\nTime: 2017-08-26 20:22:10\\n------------------------------------ -------\\n-------------------------------- -----------\\nTime: 2017-08-26 20:22:20\\n------------------------------------ -------\\n■\\n■Note\\u2003  You can read more about Apache Kafka in the following blogs:\\nhttps://stackoverflow.com/questions/17205561/data-modeling-with-kafka-\\ntopics-and-partitions\\nhttps://stackoverflow.com/questions/23751708/kafka-is-zookeeper-a-must\\nhttps://stackoverflow.com/documentation/apache-kafka/1986/getting-started-\\nwith-apache-kafka#t=201709171739464528411\\nhttp://kafka.apache.org/quickstart\\n\\x07Recipe 7-5. Execute a PySpark Script in Local \\nMode\\n\\x07Problem\\nYou want to execute a PySpark script in local mode.\\n\\x07Solution\\nWe have written the PySpark script innerJoinInPySpark.py. The content of this code file \\nis as follows:\\nfrom pyspark import SparkContext\\nstudentData = [['si1','Robin','M'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si2','Maria','F'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si3','Julie','F'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si4','Bob',\\xa0\\xa0'M'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si6','William','M']]\\nsubjectsData = [['si1','Python'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si3','Java'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si1','Java'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si2','Python'],\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 200}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n183\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si3','Ruby'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si4','C++'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si5','C'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si4','Python'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0['si2','Java']]\\nourSparkContext = SparkContext(appName = 'innerDataJoining')\\nourSparkContext.setLogLevel('ERROR')\\nstudentRDD = ourSparkContext.parallelize(studentData, 2)\\nstudentPairedRDD = studentRDD.map(lambda val : (val[0],[val[1],val[2]]))\\nsubjectsPairedRDD = ourSparkContext.parallelize(subjectsData, 2)\\nstudenSubjectsInnerJoin = studentPairedRDD.join(subjectsPairedRDD)\\ninnerJoinedData = studenSubjectsInnerJoin.collect()\\nprint innerJoinedData\\nThis is the inner join program we used in Chapter 5 for inner joins. But two extra \\nlines have been added. The extra lines are as follows:\\nfrom pyspark import SparkContext\\nand\\nourSparkContext = SparkContext(appName = 'innerDataJoining')\\nWe found that in the PySpark console, PySpark itself creates the SparkContext \\nobject as sc and enables us to use it. But in PySpark scripts, we have to create our own \\nSparkContext. SparkContext is a way to use the APIs provided by PySpark. Therefore, \\nthere are two extra lines. The first line imports SparkContext, and the second line creates \\nthe SparkContext object with the application name (appName) innerDataJoining. Let’s \\nrun this PySpark script in PySpark local mode.\\n\\x07How It Works\\nTo run the PySpark script, we use the spark-submit command. In the following \\ncommand, local[2] means we are using two threads for execution. The spark-submit \\noption master defines which master we are going to use. We are using the local master.\\n[pysparkbook@localhost bookCode]$ spark-submit - -master local[2] \\ninnerJoinInPySpark.py\\nHere is the output:\\n17/08/29 12:52:09 INFO executor.Executor: Starting executor ID driver on \\nhost localhost\\n17/08/29 12:52:09 INFO util.Utils: Successfully started service 'org.apache.\\nspark.network.netty.NettyBlockTransferService' on port 42886.\\n17/08/29 12:52:09 INFO netty.NettyBlockTransferService: Server created on 42886\\n17/08/29 12:52:09 INFO storage.BlockManagerMaster: Trying to register \\nBlockManager\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 201}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n184\\n17/08/29 12:52:09 INFO storage.BlockManagerMasterEndpoint: Registering block \\nmanager localhost:42886 with 517.4 MB RAM, BlockManagerId(driver, localhost, \\n42886)\\n17/08/29 12:52:09 INFO storage.BlockManagerMaster: Registered BlockManager\\n[('si3', (['Julie', 'F'], 'Java')), ('si3', (['Julie', 'F'], 'Ruby')), \\n('si2', (['Maria', 'F'], 'Python')), ('si2', (['Maria', 'F'], 'Java')), \\n('si1', (['Robin', 'M'], 'Python')), ('si1', (['Robin', 'M'], 'Java')), \\n('si4', (['Bob', 'M'], 'C++')), ('si4', (['Bob', 'M'], 'Python'))]\\nWe have the inner join data as output. In the next recipe, we are going to run the \\nsame script using Standalone and Mesos cluster managers.\\n\\x07Recipe 7-6. Execute a PySpark Script Using \\nStandalone Cluster Manager and Mesos Cluster \\nManager\\n\\x07Problem\\nYou want to execute a PySpark script by using Standalone Cluster Manager and the Mesos \\ncluster manager.\\n\\x07Solution\\nWe can execute our script by using spark-submit. But first we have to start Standalone \\nCluster Manager. It can be started with the start-all.sh script in sbin of SparkHome:\\n[pysparkbook@localhost sbin]$ /allPySpark/spark/sbin/start-all.sh\\nHere is the output:\\nstarting org.apache.spark.deploy.master.Master, logging to /allPySpark/\\nlogSpark//spark-pysparkbook-org.apache.spark.deploy.master.Master-1-\\nlocalhost.localdomain.out\\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /\\nallPySpark/logSpark//spark-pysparkbook-org.apache.spark.deploy.worker.\\nWorker-1-localhost.localdomain.out\\nSimilarly, to run on Mesos, we have to start the Mesos master and slaves:\\n[pysparkbook@localhost bookCode]$ mesos-master --work_dir=/allPySpark/mesos/\\nworkdir &\\n[root@localhost bookCode]# mesos-slave --master=127.0.0.1:5050 --work_dir=/\\nallPySpark/mesos/workdir --systemd_runtime_directory=/allPySpark/mesos/systemd &\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 202}, page_content=\"Chapter 7 ■ Optimizing PySpark and PySpark Streaming\\n185\\n\\x07How It Works\\nLet’s first run our script by using Standalone Cluster Manager:\\n[pysparkbook@localhost bookCode]$ spark-submit --master spark://localhost.\\nlocaldomain:7077 --num-executors 2 innerJoinInPySpark.py\\nHere is the output:\\n[('si3', (['Julie', 'F'], 'Java')), ('si3', (['Julie', 'F'], 'Ruby')), \\n('si2', (['Maria', 'F'], 'Python')), ('si2', (['Maria', 'F'], 'Java')), \\n('si1', (['Robin', 'M'], 'Python')), ('si1', (['Robin', 'M'], 'Java')), \\n('si4', (['Bob', 'M'], 'C++')), ('si4', (['Bob', 'M'], 'Python'))]\\nIn the command, the value of the master option is the Standalone master URL. \\nSimilarly, we can execute on Mesos by using the Mesos master URL:\\nspark-submit --master mesos://127.0.0.1:5050 --conf spark.executor.uri=/home/\\npysparkbook/binaries/spark-1.6.2-bin-hadoop2.6.tgz innerJoinInPySpark.py\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 203}, page_content='187\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_8\\nCHAPTER 8\\nPySparkSQL\\nMost data that a data scientist deals with is either structured or semistructured. \\nNowadays, the amount of unstructured data is increasing rapidly. The PySparkSQL \\nmodule is a higher-level abstraction over PySpark Core for processing structured and \\nsemistructured datasets. By using PySparkSQL, we can use SQL and HiveQL code too, \\nwhich makes this module popular among database programmers and Apache Hive users. \\nThe APIs provided by PySparkSQL are optimized. PySparkSQL can read data from many \\nfile types such as CSV files, JSON files, and files from other databases.\\nThe DataFrame abstraction is similar to a table in a relational database management \\nsystem. The DataFrame consists of named columns and is a collection of Row objects. \\nRow objects are defined in PySparkSQL. Users are familiar with the schema of tabular \\nforms, so it becomes easy to operate on DataFrames.\\nIn PySparkSQL 1.6, a new Dataset interface was included. This interface is a hybrid of \\nthe DataFrame and RDD, so it provides the benefits of both. The Dataset interface has not \\nbeen implemented in Spark with Python.\\nThe GraphFrames library is used to process graphs. It is similar to the GraphX \\nlibrary, which does not work for Python. For PySpark users, the GraphFrames library \\nis most suitable for graph processing. It has been developed on top of the SparkSQL \\nDataFrame. We can run our DataFrame queries by using GraphFrames, which makes it \\nunique from GraphX.\\nYou’ll find this chapter full of exciting topics, from PySparkSQL and DataFrames to \\nand the graph analysis recipes.\\nThis chapter covers the following recipes:\\nRecipe 8-1. Create a DataFrame\\nRecipe 8-2. Perform exploratory data analysis on DataFrames\\nRecipe 8-3. Perform aggregation operations on DataFrames\\nRecipe 8-4. Execute SQL and HiveQL queries on DataFrames\\nRecipe 8-5. Perform data joining on DataFrames\\nRecipe 8-6. Calculate breadth-first searches using \\nGraphFrames\\nRecipe 8-7. Calculate page rank using GraphFrames\\nRecipe 8-8. Read data from Apache Hive'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 204}, page_content=\"Chapter 8 ■ PySparkSQL\\n188\\nRecipe 8-1. Create a DataFrame\\nProblem\\nYou want to create a DataFrame.\\nSolution\\nAs you know, a DataFrame is collection of named columns. You might remember the \\nfilament data from Chapter 5. You want to do the following on the filament data:\\n• \\nCreate a DataFrame\\n• \\nKnow the schema of a DataFrame\\n• \\nPrint the content of a DataFrame\\n• \\nFilter out the data for 100W bulbs\\n• \\nSelect data from a DataFrame for bulbs of 100W with a life  \\ngreater than 650\\nHow It Works\\nStep 8-1-1. Creating a Nested List of Filament Data\\nFirst we have to create a nested list of filament data. You’re already familiar with this data \\nfrom Chapter 5:\\n>>> filamentData = [['filamentA','100W',605],\\n...\\xa0\\xa0['filamentB','100W',683],\\n...\\xa0\\xa0['filamentB','100W',691],\\n...\\xa0\\xa0['filamentB','200W',561],\\n...\\xa0\\xa0['filamentA','200W',530],\\n...\\xa0\\xa0['filamentA','100W',619],\\n...\\xa0\\xa0['filamentB','100W',686],\\n...\\xa0\\xa0['filamentB','200W',600],\\n...\\xa0\\xa0['filamentB','100W',696],\\n...\\xa0\\xa0['filamentA','200W',579],\\n...\\xa0\\xa0['filamentA','200W',520],\\n...\\xa0\\xa0['filamentA','100W',622],\\n...\\xa0\\xa0['filamentA','100W',668],\\n...\\xa0\\xa0['filamentB','200W',569],\\n...\\xa0\\xa0['filamentB','200W',555],\\n...\\xa0\\xa0['filamentA','200W',541]]\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 205}, page_content='Chapter 8 ■ PySparkSQL\\n189\\nAfter creating this filamentData nested list, we are going to create an RDD of it. To \\ncreate the RDD, we’ll use our parallelize() function:\\n>>> filamentDataRDD = sc.parallelize(filamentData, 4)\\n>>> filamentDataRDD.take(4)\\nHere is the output:\\n[[\\'filamentA\\', \\'100W\\', 605],\\n\\xa0[\\'filamentB\\', \\'100W\\', 683],\\n\\xa0[\\'filamentB\\', \\'100W\\', 691],\\n\\xa0[\\'filamentB\\', \\'200W\\', 561]]\\nThe filamentDataRDD RDD has four partitions. We have created our RDD \\nsuccessfully. The next step is to create a schema for our DataFrame.\\nStep 8-1-2. Creating a Schema of a DataFrame\\nIn our DataFrame, we have three columns. First, we are going to define the columns.  \\nWe define the columns by using the StructField() function. PySparkSQL has its own \\ndata types, and all of these are defined in the submodule pyspark.sql.types. We have to \\nimport everything from pyspark.sql.types:\\n>>>from pyspark.sql.types import *\\nAfter importing the required submodule, we define our first column of the \\nDataFrame:\\n>>> FilamentTypeColumn = StructField(\"FilamentType\",StringType(),True)\\nLet’s look at the arguments of StructField(). The first argument is the column \\nname. In this example, the column name is FilamentType. The second argument is the \\ndata type of the elements in the column. In this example, the data type of the first column \\nis StringType(). We know that some elements of a column might be null. So the last \\nargument, which has the value True, indicates that this column might have null values or \\nmissing data.\\n>>> BulbPowerColumn = StructField(\"BulbPower\",StringType(),True)\\n>>> LifeInHoursColumn = StructField(\"LifeInHours\",StringType(),True)\\nWe have created a StructField of each column. Now we have to create a schema of \\nfull DataFrames by using the StructType object as follows:\\n>>> FilamentDataFrameSchema = StructType([FilamentTypeColumn, \\nBulbPowerColumn, LifeInHoursColumn])'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 206}, page_content='Chapter 8 ■ PySparkSQL\\n190\\nFilamentDataFrameSchema is the full schema of our DataFrame.\\n>>> FilamentDataFrameSchema\\nHere is the output:\\nStructType(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0List(StructField(FilamentType,StringType,true),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0StructField(BulbPower,StringType,true),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0StructField(LifeInHours,StringType,true))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\nThe schema of the three columns of our DataFrame can be observed via the \\nFilamentDataFrameSchema variable, as shown in the preceding code.\\nWe know that a DataFrame is an RDD of Row objects. Therefore, we have to transform \\nour filamentDataRDD RDD to the RDD of Row objects. In the RDD of Row objects, every \\nrow is a Row object. In the next recipe step, we are going to transform our RDD to an RDD \\nof Row objects.\\nStep 8-1-3. Creating an RDD of Row Objects\\nThe RDD map() function is best for transforming any RDD from one structure to another. \\nIn order to transform our filamentRDD2 RDD to an RDD of Row objects. A DataFrame is \\nnothing but an RDD of Row objects. Let’s create an RDD of rows. But in order to work with \\nRow, we have to first import it. Row is in pyspark.sql. We can import Row as shown here:\\n>>> from pyspark.sql import Row\\n>>> filamentRDDofROWs = filamentDataRDD.map(lambda x  \\n:Row(str(x[0]), str(x[1]), str(x[2])))\\n>>> filamentRDDofROWs.take(4)\\nHere is the output:\\n[<Row(filamentA, 100W, 605)>,\\n\\xa0<Row(filamentB, 100W, 683)>,\\n\\xa0<Row(filamentB, 100W, 691)>,\\n\\xa0<Row(filamentB, 200W, 561)>]\\nYou can see that we have created an RDD of rows, filamentRDDofROWs. We apply the \\ntake() function on our RDD and print four rows of elements out of that.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 207}, page_content='Chapter 8 ■ PySparkSQL\\n191\\nStep 8-1-4. Creating a DataFrame\\nWe have created the schema and RDD of rows. Therefore, we can create our DataFrame. \\nIn order to create a DataFrame, we need the SQLContext object. Let’s create the \\nSQLContext object in the following line of code:\\n>>> from pyspark.sql import SQLContext\\n>>> sqlContext = SQLContext(sc)\\nWe have created our sqlContext object. As SparkContext, in our case means  \\nin PySpark console sc is entering point to PySpark I have mentioned that sc is \\nan object of SparkContext. In a similar way, SQLContext is the entering point to \\nPySparkSQL.\\nUsing the createDataFrame() function, which has been defined on SQLContext, \\nwe’ll create our DataFrame, filamentDataFrameRaw. We provide two arguments to the \\ncreateDataFrame() function: the first one is an RDD of Row objects filamentRDDofROWs; \\nand the second argument is the schema for our DataFrame, FilamentDataFrameSchema.\\n>>> filamentDataFrameRaw = sqlContext.createDataFrame(filamentRDDofROWs, \\nFilamentDataFrameSchema)\\nWe have created our DataFrame from our filament data. We have given our \\nDataFrame the reference filamentDataFrameRaw. Let’s print the records of our \\nDataFrame on the console. Previously, we used the take() function to fetch data from the \\nDataFrame. But now we are going to change the way we fetch data; we are going to use \\nthe show() function. The show() function prints data in a beautiful way. We can provide \\nthe number of rows as input to the show() function. In the following line, four records are \\nbeing fetched:\\n>>> filamentDataFrameRaw.show(4)\\nHere is the output, showing only the top four rows:\\n+------------+---------+-----------+\\n|FilamentType|BulbPower|LifeInHours|\\n+------------+---------+-----------+\\n|\\xa0\\xa0\\xa0filamentA|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0605|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0683|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0691|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0200W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0561|\\n+------------+---------+-----------+'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 208}, page_content=\"Chapter 8 ■ PySparkSQL\\n192\\nStep 8-1-5. Printing a Schema of a DataFrame\\nWe have created our DataFrame. Let’s check its schema. We can fetch the schema of a \\nDataFrame by using the printSchema() function defined on the DataFrame:\\n>>> filamentDataFrameRaw.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- FilamentType: string (nullable = true)\\n\\xa0|-- BulbPower: string (nullable = true)\\n\\xa0|-- LifeInHours: string (nullable = true)\\nThe printSchema() function’s output shows that the DataFrame has three columns. \\nThese columns indicate the column name, the data type of the columns, and whether \\na column is nullable. You are an observant reader if you have noticed that the data type \\nof the LifeInHours column is string. It’s better to represent time in hours as either an \\ninteger data type or a floating-point type. Therefore, we have to change the data type of \\nthe third column.\\nWe can typecast a column value from one data type to another by using the cast() \\nfunction.\\nStep 8-1-6. Changing the Data Type of a Column\\nThe withColumn() function returns a DataFrame by adding a new column to it. But if that \\ncolumn is already in the DataFrame, the withColumn() function will replace the existing \\ncolumn: \\n>>> filamentDataFrame = filamentDataFrameRaw.withColumn('LifeInHours',filame\\nntDataFrameRaw.LifeInHours.cast(FloatType()))\\nInvestigating schema will now return float as the data type for the LifeInHours \\ncolumn:\\n>>> filamentDataFrame.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- FilamentType: string (nullable = true)\\n\\xa0|-- BulbPower: string (nullable = true)\\n\\xa0|-- LifeInHours: float (nullable = true)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 209}, page_content=\"Chapter 8 ■ PySparkSQL\\n193\\nThe data type of the LifeInHours column has been transformed from a string type \\nto a float type. Let’s investigate the DataFrame by fetching some rows from it. We can \\ndisplay rows by using the following function:\\n>>> filamentDataFrame.show(5)\\nHere is the output, showing only the top five rows:\\n+--------------+----------+------------+\\n|FilamentType\\xa0\\xa0|BulbPower\\xa0|LifeInHours |\\n+--------------+----------+------------+\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0605.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0683.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0691.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0200W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0561.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0200W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0530.0\\xa0\\xa0\\xa0\\xa0|\\n+--------------+----------+------------+\\nWe can observe that the data type of the LifeInHours column has been changed to \\nfloating-point.\\nThe column names can be fetched by using the columns attribute of the DataFrame \\nobject:\\n>>> filamentDataFrame.columns\\nHere is the output:\\n['FilamentType', 'BulbPower', 'LifeInHours']\\nStep 8-1-7. Filtering Out Data Where BulbPower Is 100W\\nFiltering rows, based on particular logic, can be done by using the filter() function. \\nThis function takes a logical expression and returns a DataFrame of filtered data:\\n>>> filamentDataFrame100Watt = filamentDataFrame.filter(filamentDataFrame.\\nBulbPower == '100W')\\nWe need all the rows where BulbPower is equal to 100W. Therefore, we provide \\nfilamentDataFrame.BulbPower == '100W' as an argument to the filter() function. \\nLet’s see what is inside the filamentDataFrame100Watt DataFrame:\\n>>> filamentDataFrame100Watt.show()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 210}, page_content=\"Chapter 8 ■ PySparkSQL\\n194\\nHere is the output:\\n+--------------+------------+---------------+\\n|FilamentType\\xa0\\xa0|BulbPower\\xa0|LifeInHours |\\n+----------------+-------------+---------------+\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0605.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0683.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0691.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0619.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0686.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0696.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0622.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0668.0\\xa0\\xa0\\xa0\\xa0|\\n+----------------+-------------+---------------+\\nThe filter() function has done its job accurately.\\nStep 8-1-8. Selecting Data from a DataFrame\\nA compound logical expression can also be used in the filter() function. In this step, we \\nare going to use a compound logical expression with the & operator:\\n>>> filamentData100WGreater650 =filamentDataFrame.filter((filamentDataFrame.\\nBPower == '100W')\\xa0\\xa0& (filamentDataFrame.LifeInHours > 650.0))\\n>>> filamentData100WGreater650.show()\\nHere is the output:\\n+--------------+-----------+------------+\\n|FilamentType\\xa0\\xa0|BulbPower\\xa0\\xa0|LifeInHours |\\n+--------------+-----------+------------+\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0683.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0691.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0686.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentB\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0696.0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0filamentA\\xa0\\xa0|\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0668.0\\xa0\\xa0\\xa0\\xa0|\\n+--------------+-----------+------------+\\nFinally, we have met our requirement. In the next recipe, we are going to do \\nexploratory analysis on a DataFrame.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 211}, page_content='Chapter 8 ■ PySparkSQL\\n195\\nRecipe 8-2. Perform Exploratory Data Analysis  \\non a DataFrame\\nProblem\\nYou want to perform exploratory data analysis on a DataFrame.\\nSolution\\nIn exploratory data analysis, we explore the given data. Exploring the data means \\ncounting the number of records and then looking for meaningful patterns. For data of \\nnumeric columns, we calculate the measures of central tendency and the spread in the \\ndata. The spread in the data is nothing but the variability in the data. You might know that \\nmeasures of central tendency are the mean, median, and mode. But how is variability, or \\ndata spread, measured? We can measure it by using either variance or standard deviation.\\nA given dataset might have categorical columns. For categorical columns, we count \\nthe frequency for each value of that variable. A count of records gives us an idea of the \\nnumber of data points we have. We calculate the minimum and maximum data points \\nfrom a given numerical column. Knowing the minimum and maximum shows us the \\nrange of data.\\nPySparkSQL has a summary() function defined on the DataFrame. This function will \\nreturn the number of records (count), mean, standard deviation (stdev), minimum (min), \\nand maximum (max) from a column of numerical values in the DataFrame.\\nYou have a file filamentData.csv. This time we have to read data from the CSV file \\nand create a DataFrame. After creating the DataFrame, we have to do a summary analysis \\non the DataFrame’s numerical columns. Apart from summary statistics on the numerical \\ncolumns, we have to know the frequency of distinct values in each categorical field.\\nYou want to perform the following on the DataFrame of filament data:\\n• \\nRead data from the CSV file filamentData.csv\\n• \\nCreate a DataFrame\\n• \\nCalculate summary statistics on a numerical column\\n• \\nCount the frequency of distinct values in the FilamentType \\ncategorical column\\n• \\nCount the frequency of distinct values in the BulbPower \\ncategorical column\\nHow It Works\\nFirst we have to read the given file and transform the data into a DataFrame. In the \\npreceding recipe, we started from a nested list and performed several steps to create a \\nDataFrame. And in Chapter 6 we found that it took numerous steps to get a nested list \\nfrom a CSV file. It will be good if there is some.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 212}, page_content='Chapter 8 ■ PySparkSQL\\n196\\nThe PySpark package can read the CSV file and transform data directly to a \\nDataFrame. And we should be happy that we have a PySpark package to help us.  \\nThe package name is com.databricks.spark.csv. This package was developed by \\nDatabricks. Before PySpark.2.x.x, the user had to use this package separately. But in \\nPySpark version 2.x.x.x, the package is merged in PySpark, so you don’t need to include \\nthe JARs separately. Thanks to Databricks for this beautiful and very useful package.\\nStep 8-2-1. Defining the DataFrame Schema\\nHere we are going to define the schema of our DataFrame. In our schema, there are  \\nthree columns. The first column is FilamentType, which has the data type of string.  \\nThe second column is BulbPower, which also has the data type of string. The last column \\nis LifeInHours, which has the data type of double.\\nWe need different data types defined in PySparkSQL. We can find all the data types in \\nthe pyspark.sql.types submodule:\\n>>> from pyspark.sql.types import *\\n>>> FilamentTypeColumn = StructField(\"FilamentType\",StringType(),True)\\n>>> BulbPowerColumn = StructField(\"BulbPower\",StringType(),True)\\n>>> LifeInHoursColumn = StructField(\"LifeInHours\",DoubleType(),True)\\nWe have created three StructFields:\\n>>> FilamentDataFrameSchema = StructType([FilamentTypeColumn, \\nBulbPowerColumn, LifeInHoursColumn])\\nUsing these StructFields, we have created a schema for our DataFrame. The name \\nof our DataFrame schema is FilamentDataFrameSchema.\\nStep 8-2-2. Reading a CSV File and Creating a DataFrame\\nLet’s create a DataFrame. We are going use a spark.read.csv function to read and \\nconvert the file data to a DataFrame: \\n>>> filamentDataFrame = spark.read.csv(\\'file:///home/pysparkbook/\\npysparkBookData/filamentData.csv\\',header=True, schema = \\nFilamentDataFrameSchema, mode=\"DROPMALFORMED\")\\nLet’s discuss the arguments of the spark.read.csv function. The first argument is \\nthe file path. The second argument indicates that our file has a header line. The third \\nargument provides the schema of our DataFrame. What is the mode argument? This fourth \\nargument, mode, provides a way to deal with corrupt records during parsing. The value of \\nthe mode argument is DROPMALFORMED. This value is saying to drop all the corrupt data.\\nWe have read our filamentData.csv data file. And the spark.read.csv() function \\nhas already transformed our CSV data into a DataFrame. Let’s check whether we have \\nour DataFrame. Do you remember the functions on a DataFrame that can help you'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 213}, page_content='Chapter 8 ■ PySparkSQL\\n197\\nvisualize it? The show() function will work for our purpose. Let’s fetch five rows from our \\nDataFrame. The show() function prints records of the DataFrame on the console. It prints \\nrecords in a tabular format, which is easier to read and understand. Let’s apply the show() \\nfunction to print five records on the console:\\n>>> filamentDataFrame.show(5)\\nHere is the output, showing only the top five rows:\\n+------------+---------+-----------+\\n|FilamentType|BulbPower|LifeInHours|\\n+------------+---------+-----------+\\n|\\xa0\\xa0\\xa0filamentA|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0605.0|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0683.0|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0691.0|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0200W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0561.0|\\n|\\xa0\\xa0\\xa0filamentA|\\xa0\\xa0\\xa0\\xa0\\xa0200W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0530.0|\\n+------------+---------+-----------+\\nWe have our DataFrame. Let’s check its schema:\\n>>> filamentDataFrame.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- FilamentType: string (nullable = true)\\n\\xa0|-- BulbPower: string (nullable = true)\\n\\xa0|-- LifeInHours: double (nullable = true)\\nWe have a proper schema too.\\n■\\n■Note\\u2003  To learn more about getting a CSV file into a Spark DataFrame, read this Stack \\nOverflow discussion: https://stackoverflow.com/questions/29936156/get-csv-to-\\nspark-dataframe.\\nStep 8-2-3. Calculating Summary Statistics\\nThe describe() function, which is defined on a DataFrame, will give us the following:\\n>>> dataSummary = filamentDataFrame.describe()\\n>>> dataSummary.show()'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 214}, page_content=\"Chapter 8 ■ PySparkSQL\\n198\\nHere is the output:\\n+---------+------------------------------+\\n|summary\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0LifeInHours\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n+---------+------------------------------+\\n|\\xa0\\xa0count\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa016\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0mean\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0607.8125\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n| stddev\\xa0\\xa0| 61.11652122517009 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0min\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0520.0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0max\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0696.0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n+---------+------------------------------+\\nWe have our results: count, mean, stddev, min, and max. Next, we have to find the \\nfrequency of the values of our two categorical columns, FilamentType and BulbPower, \\nand a combination of them.\\nStep 8-2-4. Counting the Frequency of Distinct Values in the \\nFilamentType Categorical Column\\nA very naive method for finding the frequency of values is to filter the records by using the \\nfilter() function and then count them. Let’s perform these tasks one by one: \\n>>> filamentDataFrame.filter(filamentDataFrame.FilamentType == 'filamentA').\\ncount()\\nHere is the output:\\n8\\nIn the preceding code, we filter out all the records where FilamentType is equal to \\nfilamentA. Eight rows have filamentA. Now let’s see how many rows have filamentB in \\nthe first column:\\n>>> filamentDataFrame.filter(filamentDataFrame.FilamentType == 'filamentB').\\ncount()\\nHere is the output:\\n8\\nWe have filtered out all the records where FilamentType is equal to filamentB. \\nUsing the count() function on the filtered data returns the total number of rows that have \\nfilamentB in the first column. Eight rows have filamentB in the first column.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 215}, page_content=\"Chapter 8 ■ PySparkSQL\\n199\\nStep 8-2-5. Counting the Frequency of Distinct Values  \\nin the BulbPower Categorical Column\\nNow let’s filter data where BulbPower is equal to 100W. After filtering out the required \\nrows, we have to count them:\\n>>> filamentDataFrame.filter(filamentDataFrame.BulbPower\\xa0\\xa0== '100W').count()\\nHere is the output:\\n8\\nEight rows have BulbPower values of 100W. Similarly, we can count other values and \\ntheir combinations. Let’s compute the frequency of 200W bulbs:\\n>>> filamentDataFrame.filter(filamentDataFrame.BulbPower\\xa0\\xa0== '200W').count()\\nHere is the output:\\n8\\nOur BulbPower columns have eight 100W bulbs and eight 200W bulbs.\\nStep 8-2-6. Counting the Frequency of Distinct Values in a \\nCombination of FilamentType and BulbPower Columns\\nIn the following code, we are going to count rows on the basis of a compound logical \\nexpression:\\n>>> filamentDataFrame.filter((filamentDataFrame.FilamentType == 'filamentB') \\n& (filamentDataFrame.BulbPower\\xa0\\xa0== '100W')).count()\\nHere is the output:\\n4\\n>>> filamentDataFrame.filter((filamentDataFrame.FilamentType == 'filamentB') \\n& (filamentDataFrame.BulbPower\\xa0\\xa0== '200W')).count()\\nHere is the output:\\n4\\n>>> filamentDataFrame.filter((filamentDataFrame.FilamentType == 'filamentA') \\n& (filamentDataFrame.BulbPower\\xa0\\xa0== '200W')).count()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 216}, page_content=\"Chapter 8 ■ PySparkSQL\\n200\\nHere is the output:\\n4\\n>>> filamentDataFrame.filter((filamentDataFrame.FilamentType == 'filamentA') \\n& (filamentDataFrame.BulbPower\\xa0\\xa0== '100W')).count()\\nHere is the output:\\n4\\nRecipe 8-3. Perform Aggregation Operations  \\non a DataFrame\\nProblem\\nYou want to perform data aggregation on a DataFrame.\\nSolution\\nTo get a summarized pattern of data, data scientists perform aggregation on a given \\ndataset. Summarized patterns are easy to understand. Sometimes the summarization is \\ndone based on the key. To perform aggregation based on the key, we first need to group \\nthe data by key.\\nIn PySparkSQL, grouping by key can be performed by using the groupBy() \\nfunction. This function returns the pyspark.sql.group.GroupedData object. After this \\nGroupedData object is created, we can apply many aggregation functions such as avg(), \\nsum(), count(), min(), max(), and sum() on GroupedData.\\nWe have a data file named adult.data. I obtained this data from the web site of the \\nBren School of Information and Computer Science at the University of California, Irvine \\n(UCI). This is a simple CSV file with 15 columns. Table\\xa08-1 describes all 15 columns.\\nYou want to do the following:\\n• \\nCreate a DataFrame from the adult.data file\\n• \\nCount the total number of records in the DataFrame\\n• \\nCount the number of times that a salary is greater than $50,000 \\nand the number of times it’s less than $50,000\\n• \\nPerform summary statistics on the numeric columns age, \\ncapital-gain, capital-loss, and hours-per-week\\n• \\nFind out the mean age of male and female workers from the data\\n• \\nFind out whether a salary greater than $50,000 is more frequent \\nfor males or females\\n• \\nFind the highest-paid job\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 217}, page_content='Chapter 8 ■ PySparkSQL\\n201\\nHow It Works\\nFirst we will download the required data, and then we will perform all the required \\nactions one by one.\\nTable 8-1.\\u2002 Description of adult.data File\\nColumns\\nDescription\\nage\\nAge of person, continuous\\nworkclass\\nfnlwgt\\neducation\\nEducation-num\\nMarital-status\\noccupation\\nrelationship\\nrace\\nsex\\nCapital-gain\\nContinuous\\nContinuous\\nContinuous\\nFemale, Male.\\nWhite, Asian-Pac-Islander, Amer-Indian-Eskimo, Other,\\nBlack.\\nRelationship: Wife, Own-child, Husband, Not-in-family,\\nOther-relative, Unmarried.\\nContinuous\\nContinuous\\nCapital-loss\\nHours-per-\\nweek\\nNative-country\\nClass (income)\\n>50K, <=50K\\nUnited-States, Cambodia, England, Puerto-Rico,\\nCanada, Germany, Outlying-US(Guam-USVI-etc),\\nIndia, Japan, Greece, South, China, Cuba, Iran,\\nHonduras, Philippines, Italy, Poland, Jamaica,\\nVietnam, Mexico, Portugal, Ireland, France,\\nDominican-Republic, Laos, Ecuador, Taiwan, Haiti,\\nColumbia, Hungary, Guatemala, Nicaragua, Scotland,\\nThailand, Yugoslavia, EI-Salvador, Trinadad&Tobago,\\nPeru, Hong, Holand-Netherlands.\\nTech-support, Craft-repair, Other-service, Sales, Exec-\\nmanagerial, Prof-sepcialty, Handlers-cleaners,\\nMachine-op-inspct, Adm-clerical, Farming-fishing,\\nTransport-moving, Priv-house-serv, Protective-serv,\\nArmed-Forces.\\nMarried-civ-spouse, Divorced, Never-married,\\nSeparated, Widowed, Married-spouse-absent,\\nMarried-AF-spouse.\\nBachelors, Some-college, 11th, HS-grad, Prof-school,\\nAssoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters,\\n1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\nPrivate, Self-emp-not-inc, Self-emp-inc, Federal-gov,\\nLocal-gov, State-gov, Without-pay, Never-worked.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 218}, page_content=\"Chapter 8 ■ PySparkSQL\\n202\\nStep 8-3-1. Creating a DataFrame from the adult.data File\\nLet’s download the file adult.data from the UCI Machine Learning Repository web site. \\nWe can fetch the data file by using the wget Linux command:\\n$wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\\nadult.data\\nWe have download the data file. Now we will read the data by using the spark.read.\\ncsv() function:\\n>>> censusDataFrame = spark.read.csv('file:///home/pysparkbook/\\npysparkBookData/adult.data',header=True, inferSchema = True)\\nIn the preceding code, the second argument of the spark.read.csv() function is \\nheader = True. This indicates that the adult.data file has a header. The inferSchema = \\nTrue argument is to infer the schema from the data itself. We are not providing an explicit \\nschema for our DataFrame.\\n>>> censusDataFrame.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- age: integer (nullable = true)\\n\\xa0|-- workclass: string (nullable = true)\\n\\xa0|-- fnlwgt: double (nullable = true)\\n\\xa0|-- education: string (nullable = true)\\n\\xa0|-- education-num: double (nullable = true)\\n\\xa0|-- marital-status: string (nullable = true)\\n\\xa0|-- occupation: string (nullable = true)\\n\\xa0|-- relationship: string (nullable = true)\\n\\xa0|-- race: string (nullable = true)\\n\\xa0|-- sex: string (nullable = true)\\n\\xa0|-- capital-gain: double (nullable = true)\\n\\xa0|-- capital-loss: double (nullable = true)\\n\\xa0|-- hours-per-week: double (nullable = true)\\n\\xa0|-- native-country: string (nullable = true)\\n\\xa0|-- income: string (nullable = true)\\nFrom the schema of our DataFrame, it is clear that there are 15 columns. Some \\ncolumns are numeric, and the rest are strings. Our dataset is a mixture of categorical and \\nnumerical fields.\\nStep 8-3-2. Counting the Total Number of Records in a DataFrame\\nLet’s count how many records we have in our DataFrame. Our simplest count() method \\nfulfills that requirement:\\n>>> censusDataFrame.count()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 219}, page_content=\"Chapter 8 ■ PySparkSQL\\n203\\nHere is the output:\\n32561\\nOur data frame has 32,561 records. That’s a large number of records.\\nStep 8-3-3. Counting the Frequency of Salaries Greater Than \\nand Less Than 50K\\nWe can achieve our goal of counting the frequency of certain salaries by first grouping our \\ndata by the income column and then counting by using our count() function:\\n>>> groupedByIncome = censusDataFrame.groupBy('income').count()\\n>>> groupedByIncome.show()\\nHere is the output:\\n+----------+---------+\\n|income\\xa0\\xa0\\xa0\\xa0|count\\xa0\\xa0\\xa0\\xa0|\\n+----------+---------+\\n|\\xa0\\xa0>50K\\xa0\\xa0\\xa0\\xa0| 7841\\xa0\\xa0\\xa0\\xa0|\\n| <=50K\\xa0\\xa0\\xa0\\xa0|24720\\xa0\\xa0\\xa0\\xa0|\\n+----------+---------+\\nIt is evident from this table that salaries greater than $50,000 are less frequent than \\nthose less than or equal to that amount.\\nStep 8-3-4. Performing Summary Statistics on Numeric \\nColumns\\nThe describe() function, shown here, is very useful:\\n>>> censusDataFrame.describe('age').show()\\nHere is the output:\\n+-------+------------------+\\n|summary|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0age|\\n+-------+------------------+\\n|\\xa0\\xa0count|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa032561|\\n|\\xa0\\xa0\\xa0mean| 38.58164675532078|\\n| stddev|13.640432553581356|\\n|\\xa0\\xa0\\xa0\\xa0min|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa017|\\n|\\xa0\\xa0\\xa0\\xa0max|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa090|\\n+-------+------------------+\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 220}, page_content=\"Chapter 8 ■ PySparkSQL\\n204\\nThe maximum age of a working person is 90 years old, and the minimum age is 17 \\nyears. The mean age of working people is 30.58 years. From this mean value, it can be \\ninferred that most working people are in their 30s.\\nSimilarly, we can find summary statistics for the capital-gain and capital-loss \\ndata:\\n>>> censusDataFrame.describe('capital-gain').show()\\n+-------+------------------+\\n|summary|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0capital-gain|\\n+-------+------------------+\\n|\\xa0\\xa0count|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa032561|\\n|\\xa0\\xa0\\xa0mean|1077.6488437087312|\\n| stddev| 7385.292084840354|\\n|\\xa0\\xa0\\xa0\\xa0min|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.0|\\n|\\xa0\\xa0\\xa0\\xa0max|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa099999.0|\\n+-------+------------------+\\n>>> censusDataFrame.describe('capital-loss').show()\\nHere is the output:\\n+-------+----------------+\\n|summary|\\xa0\\xa0\\xa0\\xa0capital-loss|\\n+-------+----------------+\\n|\\xa0\\xa0count|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa032561|\\n|\\xa0\\xa0\\xa0mean| 87.303829734959|\\n| stddev|402.960218649002|\\n|\\xa0\\xa0\\xa0\\xa0min|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.0|\\n|\\xa0\\xa0\\xa0\\xa0max|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa04356.0|\\n+-------+----------------+\\nLet’s see the distribution of hours per week for workers:\\n>>> censusDataFrame.describe('hours-per-week').show()\\nHere is the output:\\n+-------+------------------+\\n|summary|\\xa0\\xa0\\xa0\\xa0hours-per-week|\\n+-------+------------------+\\n|\\xa0\\xa0count|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa032561|\\n|\\xa0\\xa0\\xa0mean|40.437455852092995|\\n| stddev|12.347428681731838|\\n|\\xa0\\xa0\\xa0\\xa0min|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa01.0|\\n|\\xa0\\xa0\\xa0\\xa0max|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa099.0|\\nIt is clear that the maximum number of working hours per week is 99.0.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 221}, page_content=\"Chapter 8 ■ PySparkSQL\\n205\\nStep 8-3-5. Finding the Mean Age of Male and Female Workers \\nfrom the Data\\nThe average age of male and female workers can be found as follows:\\n>>> groupedByGender = censusDataFrame.groupBy('sex')\\n>>> type(groupedByGender)\\nHere is the output:\\n<class 'pyspark.sql.group.GroupedData'>\\n>>> groupedByGender.mean('age').show()\\nHere is the output:\\n+----------+------------------------+\\n\\xa0|\\xa0\\xa0\\xa0\\xa0sex\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0avg(age)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n+----------+------------------------+\\n\\xa0|\\xa0\\xa0\\xa0Male\\xa0\\xa0|\\xa0\\xa039.43354749885268\\xa0\\xa0\\xa0\\xa0\\xa0|\\n\\xa0| Female\\xa0\\xa0|\\xa0\\xa036.85823043357163\\xa0\\xa0\\xa0\\xa0\\xa0|\\n+----------+------------------------+\\nMale workers are, on average, older than their female worker counterparts.\\n>>> groupedByGender.mean('hours-per-week').show()\\nHere is the output:\\n+-----------+-------------------------+\\n\\xa0|\\xa0\\xa0\\xa0\\xa0sex\\xa0\\xa0\\xa0|\\xa0\\xa0avg(hours-per-week)\\xa0\\xa0\\xa0\\xa0|\\n+-----------+-------------------------+\\n\\xa0|\\xa0\\xa0\\xa0Male\\xa0\\xa0\\xa0|\\xa0\\xa042.42808627810923\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n\\xa0| Female\\xa0\\xa0 |\\xa0\\xa036.410361154953115 \\xa0\\xa0\\xa0\\xa0|\\n+-----------+-------------------------+\\nFemale workers, on average, work fewer hours than their male counterparts, \\naccording to our data.\\nStep 8-3-6. Finding Out Whether High Salaries are More \\nFrequent for Males or Females\\nSince our result depends on two fields, sex and income, we have to group our data by both:\\n>>> groupedByGenderIncome = censusDataFrame.groupBy(['income', 'sex'])\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 222}, page_content=\"Chapter 8 ■ PySparkSQL\\n206\\nNow, on the grouped data, we can apply the count() function to get our desired \\nresult:\\n>>> groupedByGenderIncome.count().show()\\nHere is the output:\\n+------+-------+-----+\\n|income|\\xa0\\xa0\\xa0\\xa0sex|count|\\n+------+-------+-----+\\n|\\xa0\\xa0>50K|\\xa0\\xa0\\xa0Male| 6662|\\n|\\xa0\\xa0>50K| Female| 1179|\\n| <=50K| Female| 9592|\\n| <=50K|\\xa0\\xa0\\xa0Male|15128|\\n+------+-------+-----+\\nWe can see that a salary greater than $50,000 is more frequent for males.\\nStep 8-3-7. Finding the Highest-Paid Job\\nTo find the job with highest income, we must group our DataFrame on the occupation \\nand income fields:\\n>>> groupedByOccupationIncome = censusDataFrame.groupBy(['occupation', \\n'income'])\\nOn the grouped data, we have to apply count(). We need the highest-paid \\noccupation, so we need to sort the data:\\n>>> groupedByOccupationIncome.count().sort(['income','count'],  \\nascending= 0).show(5)\\nHere is the output, showing only the top five rows:\\n+----------------+------+-----+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0occupation|income|count|\\n+----------------+------+-----+\\n| Exec-managerial|\\xa0\\xa0>50K| 1968|\\n|\\xa0\\xa0Prof-specialty|\\xa0\\xa0>50K| 1859|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Sales|\\xa0\\xa0>50K|\\xa0\\xa0983|\\n|\\xa0\\xa0\\xa0\\xa0Craft-repair|\\xa0\\xa0>50K|\\xa0\\xa0929|\\n|\\xa0\\xa0\\xa0\\xa0Adm-clerical|\\xa0\\xa0>50K|\\xa0\\xa0507|\\n+----------------+------+-----+\\nWe can see that a high frequency of executive/managerial people have salaries \\ngreater than $50,000.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 223}, page_content='Chapter 8 ■ PySparkSQL\\n207\\nRecipe 8-4. Execute SQL and HiveQL Queries  \\non a DataFrame\\nProblem\\nYou want to run SQL and HiveQL queries on a DataFrame.\\nSolution\\nWe can use createOrReplaceTempView(), which creates a temporary view. The \\nDataFrame class provides this function. The life of this view is the same as the \\nSparkSession that creates the DataFrame.\\nWe have another function, registerTempTable(), which creates a temporary table \\nin memory. Using SQLContext, we can run SQL commands, and using HiveContext, \\nwe can run HiveQL queries on these temporary tables. In new versions of PySpark, this \\nmethod is deprecated. But if you are working with older PySpark code, you might find \\nregisterTempTable().\\nIn the preceding recipe, we created a DataFrame named censusDataFrame. You want \\nto perform the following actions on the DataFrame:\\n• \\nCreate a temporary view\\n• \\nSelect the age and income columns by using SQL commands\\n• \\nCompute the average hours worked per week, based on  \\neducation level\\nHow It Works\\nWe will start with creating a temporary view of our DataFrame. After creating this \\ntemporary view, we will apply SQL commands to perform our tasks.\\nStep 8-4-1. Creating a Temporary View in Memory\\nLet’s create a temporary table first. Then we can run our SQL or HiveQL commands on \\nthat table.\\n>>> censusDataFrame.createOrReplaceTempView(\"censusDataTable\")\\nThis creates the temporary table censusDataTable.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 224}, page_content=\"Chapter 8 ■ PySparkSQL\\n208\\nStep 8-4-2. Selecting Age and Income Columns Using a SQL \\nCommand\\nThe select command is a highly used and very popular command in SQL. We need to \\nselect two columns, age and income:\\n>>> censusDataAgeIncome = spark.sql('select age, income from censusDataTable \\nlimit 5')\\nThe SQL command in the preceding code is a general SQL select command to \\nfetch two columns, age and income. The spark.sql() function can be used to run SQL \\ncommands.\\nIn the spark.sql() function, Spark is denoting a SparkSession object. Whenever \\nwe start the PySpark shell, we find the SparkSession available as spark. In the preceding \\nSQL command, limit 5 will return five records only.\\nRunning the SQL command by using the spark.sql() function will return a \\nDataFrame. In our case, we have the DataFrame censusDataAgeIncome, which has only \\ntwo columns, age and income. So we know that we can print our DataFrame columns by \\nusing the show() function. Let’s see the result:\\n>>> censusDataAgeIncome.show()\\nHere is the output:\\n+---+------+\\n|age|income|\\n+---+------+\\n| 39| <=50K|\\n| 50| <=50K|\\n| 38| <=50K|\\n| 53| <=50K|\\n| 28| <=50K|\\n+---+------+ \\nThe spark.sql() function returns a DataFrame. We can test this by using the type() \\nfunction. The following code line is for testing the data type of censusDataAgeIncome:\\n>>> type(censusDataAgeIncome)\\nHere is the output:\\n<class 'pyspark.sql.dataframe.DataFrame'>\\nThe type of censusDataAgeIncome is DataFrame.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 225}, page_content='Chapter 8 ■ PySparkSQL\\n209\\nStep 8-4-3. Computing Average Hours per Week Based on \\nEducation Level\\nComputing the average hours per week for workers, based on their education level, \\nrequires grouping data by education first. In SQL and HiveQL, we can use the group by \\nclause to get the grouped data:\\n>>> avgHoursPerWeekByEducation = spark.sql(\"select education, \\nround(avg(`hours-per-week`),2) as averageHoursPerWeek from censusDataTable \\ngroup by education\")\\nThe SQL avg() function will find the mean, or average, value. The round() function \\nhas been used to get data in a beautifully formatted fashion. We fetch the average hours \\nper week as an aliased name, averageHoursPerWeek.\\xa0\\xa0And at the end of the computation, \\nwe get the DataFrame avgHoursPerWeekByEducation, which consists of two columns. \\nThe first column is education, and the second column is averageHoursPerWeek. Let’s \\napply the show() function to see the content of the DataFrame:\\n>>> avgHoursPerWeekByEducation.show()\\nHere is the output:\\n+---------------+-----------------------------------+\\n|\\xa0\\xa0\\xa0\\xa0education\\xa0\\xa0|\\xa0\\xa0averageHoursPerWeek\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n+---------------+-----------------------------------+\\n|\\xa0\\xa0Prof-school\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa047.43\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa010th\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa037.05\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa07th-8th\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa039.37\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa05th-6th\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa038.90\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0Assoc-acdm\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa040.50\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0Assoc-voc\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa041.61\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Masters\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa043.84\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa012th\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa035.78\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n|\\xa0\\xa0\\xa0\\xa0Preschool\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa036.65\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa09th\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa038.04\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0Bachelors\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa042.61\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0Doctorate\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa046.97\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0HS-grad\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa040.58\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa011th\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa033.93\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n| Some-college\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa038.85\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa01st-4th\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa038.26\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n+---------------+-----------------------------------+'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 226}, page_content='Chapter 8 ■ PySparkSQL\\n210\\nRecipe 8-5. Perform Data Joining on DataFrames\\nProblem\\nYou want to perform join operations on two DataFrames.\\nSolution\\nOften we’re required to combine information from two or more DataFrames or tables. To \\ndo this, we perform a join of DataFrames. Basically, table joining is a SQL term, where we \\njoin two or more tables to get denormalized tables. Join operations on two tables are very \\ncommon in data science.\\nIn PySparkSQL, we can perform the following types of joins:\\n• \\nInner join\\n• \\nLeft outer join\\n• \\nRight outer join\\n• \\nFull outer join\\nIn Chapter 5, we discussed paired RDD joins. In this recipe, we’ll discuss joining \\nDataFrames. You want to perform the following:\\n• \\nRead a student data table from a PostgreSQL database\\n• \\nRead subject data from a JSON file\\n• \\nPerform an inner join on DataFrames\\n• \\nSave an inner-joined DataFrame as a JSON file\\n• \\nPerform a right outer join\\n• \\nSave a right-outer-joined DataFrame into PostgreSQL\\n• \\nPerform a left outer join\\n• \\nPerform a full outer join\\nIn order to join the two DataFrames, PySparkSQL provides the join() function, \\nwhich works on DataFrames.\\nHow It Works\\nLet’s start exploring the PostgreSQL database and the tables inside. We have to read data \\nfrom the PostgreSQL database. Let’s create a table in the PostgreSQL database and put \\nrecords into the table.\\nLet’s enter into the database server:\\n$ sudo -u postgres psql'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 227}, page_content='Chapter 8 ■ PySparkSQL\\n211\\nFor our recipe, we’ll create the database pysparkbookdb:\\npostgres=# create database pysparkbookdb;\\nHere is the output:\\nCREATE DATABASE\\nWe can see whether our database has been created successfully. The following SQL \\nselect command fetches all the databases that exist on our database server:\\npostgres=# SELECT datname FROM pg_database;\\n\\xa0\\xa0\\xa0\\nOutcome :\\n\\xa0datname\\xa0\\xa0\\xa0\\xa0\\n---------------\\n\\xa0template1\\n\\xa0template0\\n\\xa0postgres\\n\\xa0metastore\\n\\xa0pymetastore\\n\\xa0pysparkbookdb\\n(6 rows)\\nWe can see that our pysparkbookdb database has been created successfully.\\nAfter creating the pysparkbookdb database, we have to connect to that database. \\nWe can connect to a database by using the \\\\c command, which stands for connect. After \\nconnecting to the database, we are going to create a table, studentTable. Then we will \\ninsert our student data into the table.\\npostgres=# \\\\c pysparkbookdb\\nYou are now connected to the pysparkbookdb database as the user postgres. Let’s \\ncreate our required table too:\\npysparkbookdb=# create table studentTable(\\npysparkbookdb(# studentID char(50) not null,\\npysparkbookdb(# name char(50) not\\xa0\\xa0null,\\npysparkbookdb(# gender char(5) not null\\npysparkbookdb(# );\\nCREATE TABLE\\nWe have created a studentTable table in the RDBMS. The \\\\d, if used with the table \\nname, provides the schema of the table; but if the command is used without anything, it \\nprints all the tables in that particular database. In the following lines. we are printing the \\nschema of the studentTable table:\\npysparkbookdb=# \\\\d\\xa0\\xa0studentTable'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 228}, page_content='Chapter 8 ■ PySparkSQL\\n212\\nHere is the output:\\n\\xa0Table \"public.studenttable\"\\n\\xa0\\xa0Column\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0Type\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Modifiers\\n-----------+---------------+-----------\\n\\xa0studentid | character(50) | not null\\n\\xa0name\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| character(50) | not null\\n\\xa0gender\\xa0\\xa0\\xa0\\xa0| character(5)\\xa0\\xa0| not null\\nLet’s put some data into the table. We are going to put in five records of students:\\ninsert into studentTable values (\\'si1\\', \\'Robin\\', \\'M\\');\\ninsert into studentTable values (\\'si2\\', \\'Maria\\', \\'F\\');\\ninsert into studentTable values (\\'si3\\', \\'Julie\\',\\xa0\\xa0\\xa0\\'F\\');\\ninsert into studentTable values (\\'si4\\', \\'Bob\\',\\xa0\\xa0\\xa0\\'M\\');\\ninsert into studentTable values (\\'si6\\',\\'William\\',\\'M\\');\\nRecords have been inserted into the table. We can visualize table data by using the \\nSQL select command.\\npysparkbookdb=# select * from studentTable;\\nHere is the output:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0studentid\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0name\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0gender\\n------------------------+---------------------------------- +--------\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Robin\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0M\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Maria\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0F\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Julie\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0F\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Bob\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0M\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si6\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0William\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0M\\xa0\\xa0\\xa0\\xa0\\n(5 rows)\\nWe have created a table and inserted records.\\nStep 8-5-1. Reading Student Data Table from PostgreSQL \\nDatabase\\nWe know that we have our student data in a table in a PostgreSQL server. We need to read \\ndata from studentTable, which is in the pysparkbookdb database. In order to connect \\nPySpark to the PostgreSQL server, we need a database JDBC connector. We are going to \\nstart our PySpark shell by using the following command:\\npyspark --driver-class-path\\xa0\\xa0.ivy2/jars/org.postgresql_postgresql-9.4.1212.\\njar --packages org.postgresql:postgresql:9.4.1212'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 229}, page_content='Chapter 8 ■ PySparkSQL\\n213\\nAfter starting the PySpark shell, including our connector JARs, we can read the table \\ndata by using the spark.read function:\\n>>> dbURL=\"jdbc:postgresql://localhost/pysparkbookdb?user=postgres&passwo\\nrd=\\'\\'\"\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\nWe have created our database URL. We connect to our pysparkbookdb database by \\nusing the postgres user. There is no password for the postgres user. The PostgreSQL \\nserver is running on the localhost machine. We are going to read data from the \\nPostgreSQL server by using the spark.read function.\\n>>> studentsDataFrame = spark.read.format(\\'jdbc\\').options(\\nurl = dbURL,\\ndatabase=\\'pysparkbookdb\\',\\ndbtable=\\'studenttable\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0).load()\\nIn the options part, we provide the URL of the database, the database name for the \\ndatabase argument, and the table name for the dbtable argument. We read the table \\ndata, which has been transformed into the DataFrame studentsDataFrame. Let’s check \\nour studentsDataFrame:\\n>>> studentsDataFrame.show()\\nHere is the output:\\n+--------------------+--------------------+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0studentid|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0name|gender|\\n+--------------------+--------------------+------+\\n|si1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...|Robin\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...| M\\xa0\\xa0\\xa0\\xa0|\\n|si2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...|Maria\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...| F\\xa0\\xa0\\xa0\\xa0|\\n|si3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...|Julie\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...| F\\xa0\\xa0\\xa0\\xa0|\\n|si4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...|Bob\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...| M\\xa0\\xa0\\xa0\\xa0|\\n|si6\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...|William\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0...| M\\xa0\\xa0\\xa0\\xa0|\\n+--------------------+--------------------+------+\\nWe have our required student DataFrame. But do you see any problems with our \\nDataFrame? Have another look. Do you see the ellipses (. . .) in our DataFrame? We have \\nto remove them. We can do this by using the trim() function, applying it to the columns.\\nTo trim strings, we have to import the trim() function. This function is in the \\nsubmodule pyspark.sql.functions. After importing the trim() function, we can use it \\nto remove the dots from our columns as follows:\\n>>> from pyspark.sql.functions import trim\\n>>> studentsDataFrame = studentsDataFrame.select(trim(studentsData \\nFrame.studentid),trim(studentsDataFrame.name),studentsDataFrame.gender)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 230}, page_content=\"Chapter 8 ■ PySparkSQL\\n214\\nLet’s print and see whether we got rid of the problem:\\n>>> studentsDataFrame.show()\\nHere is the output:\\n+---------------+----------+------+\\n|trim(studentid)|trim(name)|gender|\\n+---------------+----------+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0\\xa0\\xa0\\xa0Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0\\xa0\\xa0\\xa0Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0\\xa0\\xa0Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si6|\\xa0\\xa0\\xa0William| M\\xa0\\xa0\\xa0\\xa0|\\n+---------------+----------+------+\\nWe got rid of the problem. But are you sure that we got rid of all the problems? How \\nabout the names of the DataFrame columns? Now we have to change the column names \\nto be clearer:\\n>>> studentsDataFrame = studentsDataFrame.withColumnRenamed \\n('trim(studentid)', 'studentID').withColumnRenamed('trim(name)','Name').\\nwithColumnRenamed('gender', 'Gender')\\nYou can see the changed column names by printing the schema:\\n>>> studentsDataFrame.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- studentID: string (nullable = false)\\n\\xa0|-- Name: string (nullable = false)\\n\\xa0|-- Gender: string (nullable = false)\\nWe have our column names in a readable format. We should check that everything is \\nappropriate by printing the DataFrame:\\n>>> studentsDataFrame.show()\\nHere is the output:\\n+---------+-------+------+\\n|studentID|\\xa0\\xa0\\xa0Name|Gender|\\n+---------+-------+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si6|William| M\\xa0\\xa0\\xa0\\xa0|\\n+---------+-------+------+\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 231}, page_content='Chapter 8 ■ PySparkSQL\\n215\\nNow we can move on to DataFrame joining.\\nStep 8-5-2. Reading Subject Data from a JSON File\\nLet’s read our subject data from the subjects.json file: \\n>>> subjectsDataFrame = sqlContext.read.format(\"json\").load(\\'/home/\\npysparkbook/pysparkBookData/subjects.json\\')\\nWe have another DataFrame, subjectsDataFrame. Let’s investigate our \\nsubjectsDataFrame by using the show() function:\\n>>> subjectsDataFrame.show()\\nHere is the output:\\n+---------+-------+\\n|studentID|subject|\\n+---------+-------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1| Python|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Java|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0\\xa0Java|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2| Python|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Ruby|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0C++|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si5|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0C|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4| Python|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0\\xa0Java|\\n+---------+-------+\\n>>> subjectsDataFrame.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- studentID: string (nullable = true)\\n\\xa0|-- subject: string (nullable = true)\\nStep 8-5-3. Performing an Inner Join on DataFrames\\nWe have two DataFrames, subjectsDataFrame and studentsDataFrame. In both \\nDataFrames, we have to perform a join on the studentID column. This column is \\ncommon in both DataFrames. An inner join returns records that have key values that \\nmatch. If we look at the values of the studentID column in both DataFrames, we will find \\nthat values si1, si2, si3, and si4 are common to both DataFrames. Therefore, an inner \\njoin will return records for only those values.\\n>>> joinedDataInner = subjectsDataFrame.join(studentsDataFrame, \\nsubjectsDataFrame.studentID==studentsDataFrame.studentID, how=\\'inner\\')\\n>>> joinedDataInner.show()'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 232}, page_content=\"Chapter 8 ■ PySparkSQL\\n216\\nHere is the output:\\n+---------+-------+---------+-----+------+\\n|studentID|subject|studentID| Name|Gender|\\n+---------+-------+---------+-----+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Ruby|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0C++|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n+---------+-------+---------+-----+------+\\nIn the resulting DataFrame joinedDataInner, it is easily observed that we have \\nstudent IDs si1, si2, si3, and si4.\\nStep 8-5-4. Saving an Inner-Joined DataFrame as a JSON File\\nAfter doing analysis, we generally save the results somewhere. Here we are going to \\nsave our DataFrame joinedDataInner as a JSON file. Let’s have a look at the columns in \\njoinedDataInner; we can see that the studentID column occurs twice. If we are saving \\ndata in the same format, it is going to throw a pyspark.sql.utils.AnalysisException \\nexception. Therefore, we first have to remove the duplicate column. For this, the select() \\nfunction is going to be the most useful. The following code removes the duplicate \\nstudentID column:\\n>>> joinedDataInner = joinedDataInner.select(subjectsDataFrame.\\nstudentID,'subject', 'Name', 'Gender')\\nThe columns of the DataFrame are as follows:\\n>>> joinedDataInner.columns\\n['studentID', 'subject', 'Name', 'Gender']\\nThe duplicate studentID column has been removed. The following code saves the \\nDataFrame as a JSON file inside the innerJoinedTable directory;\\n>>> joinedDataInner.write.format('json').save('/home/muser/\\ninnerJoinedTable')\\nWe should see what has been saved under our innerJoinedTable directory:\\ninnerJoinedTable$ ls\\nHere is the output:\\npart-r-00000-77838a67-4a1f-441a-bb42-4cd03be525a9.json\\xa0\\xa0_SUCCESS\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 233}, page_content='Chapter 8 ■ PySparkSQL\\n217\\nThe ls command shows two files inside the directory. The JSON file contains our \\ndata, and the second file tells us that we have written the data successfully. Now you want \\nto know what is inside the JSON file. The command cat is best for this job:\\ninnerJoinedTable$ cat part-r-00000-77838a67-4a1f-441a-bb42-4cd03be525a9.json\\nHere is the output:\\n{\"studentID\":\"si1\",\"subject\":\"Java\",\"Name\":\"Robin\",\"Gender\":\"M\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si1\",\"subject\":\"Python\",\"Name\":\"Robin\",\"Gender\":\"M\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si2\",\"subject\":\"Java\",\"Name\":\"Maria\",\"Gender\":\"F\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si2\",\"subject\":\"Python\",\"Name\":\"Maria\",\"Gender\":\"F\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si3\",\"subject\":\"Ruby\",\"Name\":\"Julie\",\"Gender\":\"F\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si3\",\"subject\":\"Java\",\"Name\":\"Julie\",\"Gender\":\"F\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si4\",\"subject\":\"Python\",\"Name\":\"Bob\",\"Gender\":\"M\\xa0\\xa0\\xa0\\xa0\"}\\n{\"studentID\":\"si4\",\"subject\":\"C++\",\"Name\":\"Bob\",\"Gender\":\"M\\xa0\\xa0\\xa0\\xa0\"}\\nWe have done one more job successfully.\\nStep 8-5-5. Performing a Left Outer Join\\nHere, we are going to perform a left outer join. In a left outer join, every value from the \\nstudentID column of the subjectsDataFrame DataFrame will be considered, even if it has \\na matching counterpart in the studentID column of the studentsDataFrame DataFrame. \\nFor the left outer join, we have to provide left_outer as the value of the how argument of \\nthe join() function.\\n>>> joinedDataLeftOuter = subjectsDataFrame.join(studentsDataFrame, \\nsubjectsDataFrame.studentID==studentsDataFrame.studentID, how=\\'left_outer\\')\\n>>> joinedDataLeftOuter.show()\\nHere is the output:\\n\\xa0+---------+-------+---------+-----+------+\\n|studentID|subject|studentID| Name|Gender|\\n+---------+-------+---------+-----+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si5|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0C|\\xa0\\xa0\\xa0\\xa0\\xa0null| null|\\xa0\\xa0null|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0C++|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Ruby|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|Robin| M\\xa0\\xa0\\xa0\\xa0|\\n+---------+-------+---------+-----+------+'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 234}, page_content=\"Chapter 8 ■ PySparkSQL\\n218\\nThe left-outer-joined table shows that si5, which is part of the studentID column of \\nsubjectsDataFrame, is part of our joined table.\\nStep 8-5-6. Saving a Left-Outer-Joined DataFrame into \\nPostgreSQL\\nSaving result data to a PostgreSQL database helps data analysts put results into safe \\nhands. Other users can use the result data for many purposes. Let’s save our results in a \\nPostgreSQL database.\\nAgain, we have to remove the duplicate column before saving the data to a \\nPostgreSQL database:\\n>>> joinedDataLeftOuter = joinedDataLeftOuter.select(subjectsDataFrame.\\nstudentID,'subject', 'Name', 'Gender')\\nIt is a good idea to check that the data has been saved properly in the database. The \\n\\\\d command will print all the existing tables in the PostgreSQL database. We have already \\ncreated the database pysparkbookdb on our server. In the same database, we are going to \\nsave our DataFrame:\\npysparkbookdb=# \\\\d\\nHere is the output:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0List of relations\\n\\xa0Schema |\\xa0\\xa0\\xa0\\xa0\\xa0Naame\\xa0\\xa0\\xa0\\xa0\\xa0| Type\\xa0\\xa0|\\xa0\\xa0Owner\\xa0\\xa0\\xa0\\n--------+---------------+-------+----------\\n\\xa0public | studenttable\\xa0 | table | postgres\\n(1 row)\\nThe \\\\d command shows that in the pysparkbookdb database, we have only one table, \\nstudenttable. Now let’s save the DataFrame:\\n>>> props = { 'user' : 'postgres', 'password' : '' }\\n>>> joinedDataLeftOuter.write.jdbc(\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0url\\xa0\\xa0\\xa0= dbURL,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0table = 'joineddataleftoutertable',\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0mode\\xa0\\xa0= 'overwrite',\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0properties = props\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\nThe preceding code saves the DataFrame to the joineddataleftoutertable table. \\nIt also defines the variable dbURL. The mode argument has the value overwrite, which \\nmeans it will overwrite the values if something before.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 235}, page_content=\"Chapter 8 ■ PySparkSQL\\n219\\nAfter saving data into PostgreSQL, we should check it once. Again, we are going to \\nuse the \\\\d command to see all the tables in our pysparkbookdb database:\\npysparkbookdb=# \\\\d\\nHere is the output:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0List of relations\\n\\xa0Schema |\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Name\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Type\\xa0\\xa0|\\xa0\\xa0Owner\\xa0\\xa0\\xa0\\n--------+--------------------------+-------+----------\\n\\xa0public | joineddataleftoutertable | table | postgres\\n\\xa0public | studenttable\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| table | postgres\\n(2 rows)\\nAnd we have our DataFrame saved in PostgreSQL as the joineddataleftoutertable \\ntable in the pysparkbookdb database. Let’s check the values in the table by using the \\nselect command:\\npysparkbookdb=# select * from joineddataleftoutertable;\\nHere is the output:\\n\\xa0studentID | subject | Name\\xa0\\xa0| Gender\\n-----------+---------+-------+--------\\n\\xa0si5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| C\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0|\\n\\xa0si2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Python\\xa0\\xa0| Maria | F\\xa0\\xa0\\xa0\\xa0\\n\\xa0si2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Java\\xa0\\xa0\\xa0\\xa0| Maria | F\\xa0\\xa0\\xa0\\xa0\\n\\xa0si4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| C++\\xa0\\xa0\\xa0\\xa0\\xa0| Bob\\xa0\\xa0\\xa0| M\\xa0\\xa0\\xa0\\xa0\\n\\xa0si4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Python\\xa0\\xa0| Bob\\xa0\\xa0\\xa0| M\\xa0\\xa0\\xa0\\xa0\\n\\xa0si3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Java\\xa0\\xa0\\xa0\\xa0| Julie | F\\xa0\\xa0\\xa0\\xa0\\n\\xa0si3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Ruby\\xa0\\xa0\\xa0\\xa0| Julie | F\\xa0\\xa0\\xa0\\xa0\\n\\xa0si1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Python\\xa0\\xa0| Robin | M\\xa0\\xa0\\xa0\\xa0\\n\\xa0si1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Java\\xa0\\xa0\\xa0\\xa0| Robin | M\\xa0\\xa0\\xa0\\xa0\\n(9 rows)\\nFor further use of the result data, we have already saved it in the PostgreSQL \\ndatabase. After the left outer join, we are going to perform a right outer join on our \\nDataFrames.\\nStep 8-5-7. Performing a Right Outer Join\\nIn a right outer join, every value of the studentID column of studentsDataFrame. \\n>>> joinedDataRightOuter = subjectsDataFrame.join(studentsDataFrame, \\nsubjectsDataFrame.studentID==studentsDataFrame.studentID, how='right_outer')\\n>>> joinedDataRightOuter.show()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 236}, page_content=\"Chapter 8 ■ PySparkSQL\\n220\\n+---------+-------+---------+-------+------+\\n|studentID|subject|studentID|\\xa0\\xa0\\xa0Name|Gender|\\n+---------+-------+---------+-------+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Ruby|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0C++|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0null|\\xa0\\xa0\\xa0null|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si6|William| M\\xa0\\xa0\\xa0\\xa0|\\n+---------+-------+---------+-------+------+\\nStep 8-5-8. Performing a Full Outer Join\\nAn outer join combines all values from the key columns:\\n>>> joinedDataOuter = subjectsDataFrame.join(studentsDataFrame, \\nsubjectsDataFrame.studentID==studentsDataFrame.studentID, how='outer')\\n>>> joinedDataOuter.show()\\nHere is the output:\\n+---------+-------+---------+-------+------+\\n|studentID|subject|studentID|\\xa0\\xa0\\xa0Name|Gender|\\n+---------+-------+---------+-------+------+\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si5|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0C|\\xa0\\xa0\\xa0\\xa0\\xa0null|\\xa0\\xa0\\xa0null|\\xa0\\xa0null|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si2|\\xa0\\xa0Maria| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0C++|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si4|\\xa0\\xa0\\xa0\\xa0Bob| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0\\xa0Ruby|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si3|\\xa0\\xa0Julie| F\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0null|\\xa0\\xa0\\xa0null|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si6|William| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1| Python|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0Robin| M\\xa0\\xa0\\xa0\\xa0|\\n|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0\\xa0Java|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0si1|\\xa0\\xa0Robin| M\\xa0\\xa0\\xa0\\xa0|\\n+---------+-------+---------+-------+------+\\nRecipe 8-6. Perform Breadth-First Search Using \\nGraphFrames\\nProblem\\nYou want to perform a breadth-first search using GraphFrames.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 237}, page_content='Chapter 8 ■ PySparkSQL\\n221\\nSolution\\nA breadth-first search is a very popular algorithm that can be used to find the shortest \\ndistance between two given nodes. Figure\\xa08-1 shows a graph that we have been given. It \\nhas seven nodes: A, B, C, D, E, F, and G.\\nA\\nB\\nG\\nF\\nE\\nD\\nC\\nFigure 8-1.\\u2002 A graph\\nIf we will look at the connection between nodes, we will find the following:\\nA – C – B\\nB – A – C – G – F\\nC – A – B – F – D\\nD – C – F – E\\nE – D – F\\nF – B – C – D -- E – G\\nG – B – F\\nLet me explain this structure. Take a look at the first line. This line, A – C – B, tells us \\nthat node A is connected to B and C.\\nYou want to perform a breadth-first search and find the shortest distance between \\nnodes. To do this, we are going to use an external library, GraphFrames. We can use \\nPySpark with GraphFrames very easily. GraphFrames provides DataFrame-based graphs.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 238}, page_content=\"Chapter 8 ■ PySparkSQL\\n222\\nHow It Works\\nTo use GraphFrames, we first have to include it as we did for the PostgreSQL connector \\nJAR file. We are going to start the PySpark shell by using the GraphFrames JAR. We are \\nusing GraphFrames version 0.4.0.\\n$ pyspark --packages graphframes:graphframes:0.4.0-spark2.0-s_2.11\\nThe GraphFrames package has been added. Now we are going to run a breadth-first \\nsearch using GraphFrames.\\nStep 8-6-1. Creating DataFrames of Vertices of a Given Graph\\nWe know that GraphFrames work on PySparkSQL DataFrames. Let’s create a DataFrame \\nof vertices. We already know how to create a DataFrame. We are going to perform the \\nsame steps as we did in Recipe 8-1:\\n>>> from pyspark.sql.types import *\\n>>> from pyspark.sql import Row\\n>>> verticesDataList = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\\n>>> verticesSchema = StructType([StructField('id',StringType(),True)])\\n>>> verticesRDD = sc.parallelize(verticesDataList, 4)\\n>>> verticesRDDRows = verticesRDD.map(lambda data : Row(data[0]))\\n>>> verticesDataFrame = sqlContext.createDataFrame(verticesRDDRows, \\nverticesSchema)\\n>>> verticesDataFrame.show(4)\\nHere is the output, showing only the top four rows:\\n+---+\\n| id|\\n+---+\\n|\\xa0\\xa0A|\\n|\\xa0\\xa0B|\\n|\\xa0\\xa0C|\\n|\\xa0\\xa0D|\\n+---+\\nWe have created our vertices DataFrame. I am sure that you have observed that the \\ncolumn name of our vertices DataFrame is id. Can you have another name as the column \\nname for the vertices DataFrame ? The answer is a simple no. It is mandatory to name the \\ncolumn id. Let’s create a DataFrame of edges.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 239}, page_content=\"Chapter 8 ■ PySparkSQL\\n223\\nStep 8-6-2. Creating DataFrames of Edges of a Given Graph\\nWe have to create a DataFrame of edges. We will first create a list of tuples; each tuple will \\nhave a source node and destination node of an edge:\\n>>> edgeDataList = \\x07[('A','C'),('A','B'),('B','A'),('B','C'),('B','G'), \\n('B','F'),('C','A'),\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\x07('C','B'),('C','F'),('C','D'),('D','C'),('D','F'), \\n('D','E'),('E','D'),\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\x07('E','F'),('F','B'),('F','C'),('F','D'),('F','E'), \\n('F','G'),('G','B'),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('G','F')]\\nAfter creating a list of edges, this list has to be parallelized by using the \\nparallelize() function. We are parallelizing this data into four partitions:\\n>>> edgeRDD = sc.parallelize(edgeDataList, 4)\\nWe have created an RDD of the edges list.\\n>>> edgeRDD.take(4)\\nHere is the output:\\n[('A', 'C'),\\n\\xa0('A', 'B'),\\n\\xa0('B', 'A'),\\n\\xa0('B', 'C')]\\nAfter creating the RDD of edges, the RDD of rows has to be created to create the \\nDataFrame. The following line of code creates an RDD of Row objects:\\n>>> edgeRDDRows = edgeRDD.map( lambda data : Row(data[0], data[1]))\\n>>> edgeRDDRows.take(4)\\nHere is the output:\\n[<Row(A, C)>,\\n\\xa0<Row(A, B)>,\\n\\xa0<Row(B, A)>,\\n\\xa0<Row(B, C)>]\\nA schema is required for our edge DataFrame. We have to create a column schema \\nfor the source node column and destination node column. Then, using the StructType() \\nfunction, we will create a schema for our edge DataFrame.\\n>>> sourceColumn = StructField('src', StringType(),True)\\n>>> destinationColumn = StructField('dst', StringType(), True)\\n>>> edgeSchema = StructType([sourceColumn, destinationColumn])\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 240}, page_content='Chapter 8 ■ PySparkSQL\\n224\\nHave you observed that for sourceColumn we have given the name as src, and for \\ndestinationColumn, we have given the name dst? This is also mandatory; it is required \\nsyntax for GraphFrames. The schema for the DataFrame has been created. The next step, \\nobviously, is to create the DataFrame:\\n>>> edgeDataFrame = sqlContext.createDataFrame(edgeRDDRows, edgeSchema)\\n>>> edgeDataFrame.show(5)\\nHere is the output, showing only the top five rows:\\n+---+---+\\n|src|dst|\\n+---+---+\\n|\\xa0\\xa0A|\\xa0\\xa0C|\\n|\\xa0\\xa0A|\\xa0\\xa0B|\\n|\\xa0\\xa0B|\\xa0\\xa0A|\\n|\\xa0\\xa0B|\\xa0\\xa0C|\\n|\\xa0\\xa0B|\\xa0\\xa0G|\\n+---+---+\\nStep 8-6-3. Creating a GraphFrames Object\\nAt this moment, we have verticesDataFrame, a DataFrame of vertices; and \\nedgeDataFrame, a DataFrames of edges. Using these two, we can create our graph.  \\nIn GraphFrames, we can create a graph by using the following code lines:\\n>>> import graphframes.graphframe\\xa0\\xa0as gfm\\n>>> ourGraph = gfm.GraphFrame(verticesDataFrame, edgeDataFrame)\\nThe GraphFrame Python class is defined under the graphframes.graphframe \\nsubmodule. GraphFrame() takes the vertices and edges DataFrames and returns a \\nGraphFrames object. We have our GraphFrames object, ourGraph. We can fetch all the \\nvertices as follows:\\n>>> ourGraph.vertices.show(5)\\nHere is the output, showing only the top five rows:\\n+---+\\n| id|\\n+---+\\n|\\xa0\\xa0A|\\n|\\xa0\\xa0B|\\n|\\xa0\\xa0C|\\n|\\xa0\\xa0D|\\n|\\xa0\\xa0E|\\n+---+'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 241}, page_content='Chapter 8 ■ PySparkSQL\\n225\\nWe can fetch edges also; here are the top five rows:\\n>>> ourGraph.edges.show(5)\\n+---+---+\\n|src|dst|\\n+---+---+\\n|\\xa0\\xa0A|\\xa0\\xa0C|\\n|\\xa0\\xa0A|\\xa0\\xa0B|\\n|\\xa0\\xa0B|\\xa0\\xa0A|\\n|\\xa0\\xa0B|\\xa0\\xa0C|\\n|\\xa0\\xa0B|\\xa0\\xa0G|\\n+---+---+\\nStep 8-6-4. Running a Breath-First Search Algorithm\\nWe have created a graph from the required data. Now we can run a breadth-first \\nalgorithm on the ourGraph graph. The bfs() function is a breadth-first search (BFS) \\nimplementation in GraphFrames. This function is defined on the GraphFrames object. \\nTherefore, we can run this BFS on our GraphFrames object ourGraph. We want to get \\nthe minimum path between node D and node G. The first argument, fromExpr, is an \\nexpression that tells us that we have to start our BFS from node D. The second argument \\nis toExpr, and the value of toExpr indicates that the destination node for our search is G.\\n>>> bfsPath = ourGraph.bfs(fromExpr=\"id=\\'D\\'\", toExpr = \"id=\\'G\\'\")\\n>>> bfsPath.show()\\nHere is the output:\\n+----+-----+---+-----+---+\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n|from|\\xa0\\xa0\\xa0e0| v1|\\xa0\\xa0\\xa0e1| to|\\n+----+-----+---+-----+---+\\n| [D]|[D,F]|[F]|[F,G]|[G]|\\n+----+-----+---+-----+---+\\nThe output of BFS is very clear. The [D] in the from column means that the start node \\nfor BFS is D. The [G] in the to column indicates that the destination node of BFS is G. The \\nshortest path between D and G is from D to F and from F to G.\\n■\\n■Note\\u2003  You can read more about GraphFrames in the following web pages:\\nhttps://graphframes.github.io/user-guide.html\\nhttps://graphframes.github.io/quick-start.html\\nhttps://graphframes.github.io/api/python/index.html'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 242}, page_content=\"Chapter 8 ■ PySparkSQL\\n226\\nRecipe 8-7. Calculate Page Rank Using \\nGraphFrames\\nProblem\\nYou want to perform a page-rank algorithm using GraphFrames.\\nSolution\\nWe already discussed page rank in Chapter 5. We have been given the same graph of web \\npages that we used in the previous chapter. You want to run the page-rank algorithm \\nusing DataFrames. Figure\\xa08-2 shows the network of web pages.\\na\\nb\\nd\\nc\\nFigure 8-2.\\u2002 Graph of web pages\\nHow It Works\\nFirst, we have to create a GraphFrames object for the given graph. We have to then create \\na DataFrame of vertices and a DataFrame of edges.\\nStep 8-7-1. Creating DataFrame of Vertices\\nWe have been given a graph of four nodes. Our four nodes are a, b, c, and d. First, we \\ncreate a list of these four nodes:\\n>>> verticesList\\xa0\\xa0= ['a', 'b', 'c', 'd']\\n>>> verticesListRDD = sc.parallelize(verticesList, 4)\\n>>> verticesListRowsRDD = verticesListRDD.map( lambda data : Row(data))\\n>>> verticesListRowsRDD.collect()\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 243}, page_content=\"Chapter 8 ■ PySparkSQL\\n227\\nHere is the output:\\n[<Row(a)>,\\n<Row(b)>,\\n<Row(c)>,\\n<Row(d)>]\\n>>> verticesSchema = StructType([StructField('id', StringType(), True)])\\n>>> verticesDataFrame = sqlContext.createDataFrame(verticesListRowsRDD, \\nverticesSchema)\\n>>> verticesDataFrame.show()\\n+---+\\n| id|\\n+---+\\n|\\xa0\\xa0a|\\n|\\xa0\\xa0b|\\n|\\xa0\\xa0c|\\n|\\xa0\\xa0d|\\n+---+\\nWe have created a DataFrame of vertices. Now we have to create a Data Frame of edges.\\nStep 8-7-2. Creating a DataFrame of Edges\\nTo create a DataFrame of edges, the steps are similar to those in many previous recipes. \\nFirst, we have to create a list of edges. Each edge in the list will be defined by a tuple. \\nThen, we have to create an RDD of edges. Thereafter, we have to transform our RDD to an \\nRDD of row objects. This will be followed by creating a schema and a DataFrame of edges. \\nLet’s perform the steps:\\n>>> edgeDataList = [('a','b'), ('a','c'), ('a','d'), ('b', 'c'),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0('b', 'd'),('c', 'b'), ('d', 'a'), ('d', 'c')]\\n>>> sourceColumn = StructField('src', StringType(),True)\\n>>> destinationColumn = StructField('dst', StringType(), True)\\n>>> edgeSchema = StructType([sourceColumn, destinationColumn])\\n>>> edgeRDD = sc.parallelize(edgeDataList, 4)\\n>>> edgeRDD.take(4)\\nHere is the output:\\n[('a', 'b'),\\n\\xa0('a', 'c'),\\n\\xa0('a', 'd'),\\n\\xa0('b', 'c')]\\n>>> edgeRDDRows = edgeRDD.map( lambda data : Row(data[0], data[1]))\\n>>> edgeRDDRows.take(4)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 244}, page_content='Chapter 8 ■ PySparkSQL\\n228\\nHere is the output:\\n[<Row(a, b)>,\\n<Row(a, c)>,\\n<Row(a, d)>,\\n<Row(b, c)>]\\n>>> edgeDataFrame = sqlContext.createDataFrame(edgeRDDRows, edgeSchema)\\n>>> edgeDataFrame.show(5)\\nHere is the output, showing only the top five rows:\\n+---+---+\\n|src|dst|\\n+---+---+\\n|\\xa0\\xa0a|\\xa0\\xa0b|\\n|\\xa0\\xa0a|\\xa0\\xa0c|\\n|\\xa0\\xa0a|\\xa0\\xa0d|\\n|\\xa0\\xa0b|\\xa0\\xa0c|\\n|\\xa0\\xa0b|\\xa0\\xa0d|\\n+---+---+\\nWe have created a DataFrame of edges. Let’s create a graph.\\nStep 8-7-3. Creating a Graph\\nThe process of creating a graph follows the same path as in the preceding recipe:\\n>>> import graphframes.graphframe\\xa0\\xa0as gfm\\n>>> ourGraph = gfm.GraphFrame(verticesDataFrame, edgeDataFrame)\\n>>> ourGraph.vertices.show(5)\\nHere is the output:\\n+---+\\n| id|\\n+---+\\n|\\xa0\\xa0a|\\n|\\xa0\\xa0b|\\n|\\xa0\\xa0c|\\n|\\xa0\\xa0d|\\n+---+\\n>>> ourGraph.edges.show(5)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 245}, page_content='Chapter 8 ■ PySparkSQL\\n229\\nHere is the output, showing only the top five rows:\\n+---+---+\\n|src|dst|\\n+---+---+\\n|\\xa0\\xa0a|\\xa0\\xa0b|\\n|\\xa0\\xa0a|\\xa0\\xa0c|\\n|\\xa0\\xa0a|\\xa0\\xa0d|\\n|\\xa0\\xa0b|\\xa0\\xa0c|\\n|\\xa0\\xa0b|\\xa0\\xa0d|\\n+---+---+\\nStep 8-7-4. Running a Page-Rank Algorithm\\nPage rank for pages can be found by using the pageRank() function, which is defined on \\nthe GraphFrames object:\\n>>> pageRanks = ourGraph.pageRank(resetProbability=0.15, tol=0.01)\\nYou might be wondering about the return type of the pageRank() function. Let’s see \\nby printing it:\\n>>> pageRanks\\nHere is the output:\\nGraphFrame(v:[id: string, pagerank: double], e:[src: string, dst:  \\nstring ... 1 more field])\\nThe pageRank() function returns a GraphFrame object. The returned GraphFrame object \\nhas vertices and edges. The vertices part of the returned GraphFrame object has the web \\npages and corresponding page ranks. Let’s explore what is inside the edges part of pageRanks:\\n>>> pageRanks.edges.show()\\nHere is the output:\\n+---+---+------------------+\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n|src|dst|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0weight|\\n+---+---+------------------+\\n|\\xa0\\xa0d|\\xa0\\xa0a|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.5|\\n|\\xa0\\xa0b|\\xa0\\xa0d|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.5|\\n|\\xa0\\xa0a|\\xa0\\xa0b|0.3333333333333333|\\n|\\xa0\\xa0a|\\xa0\\xa0d|0.3333333333333333|\\n|\\xa0\\xa0d|\\xa0\\xa0c|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.5|\\n|\\xa0\\xa0a|\\xa0\\xa0c|0.3333333333333333|\\n|\\xa0\\xa0b|\\xa0\\xa0c|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa00.5|\\n|\\xa0\\xa0c|\\xa0\\xa0b|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa01.0|\\n+---+---+------------------+'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 246}, page_content=\"Chapter 8 ■ PySparkSQL\\n230\\nIt can be observed that the edges part of the pageRanks object consists of edges and \\ncorresponding weights:\\n>>> pageRanks.vertices.select('id','pagerank')\\nHere is the output:\\nDataFrame[id: string, pagerank: double]\\n>>> pageRanks.vertices.select('id','pagerank').show()\\nHere is the output:\\n+---+------------------+\\n| id|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pagerank|\\n+---+------------------+\\n|\\xa0\\xa0a|0.4831888601952005|\\n|\\xa0\\xa0b| 1.238562817904233|\\n|\\xa0\\xa0d| 0.806940642367432|\\n|\\xa0\\xa0c|1.1401295025626326|\\n+---+------------------+\\nFinally, we have the page rank for the given pages.\\nRecipe 8-8. Read Data from Apache Hive\\nProblem\\nYou want to read table data from Apache Hive.\\nSolution\\nWe have a table, filamentdata, in Hive. This is the same filament data we have used in \\nmany recipes. We have to read this data by using PySparkSQL from Apache Hive. Let’s look \\nat the whole process. First we are going to create a table in Hive and upload data into it. Let’s \\nstart with creating the table filamentdata. We’ll create our table in the apress database of \\nHive. We created this database in Chapter 2, at the time of installation. But let’s check that \\nour creation still exists. We can display all the databases in Hive by using show:\\nhive> show databases;\\nHere is the output:\\nOK\\napress\\ndefault\\nTime taken: 3.275 seconds, Fetched: 2 row(s)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 247}, page_content=\"Chapter 8 ■ PySparkSQL\\n231\\nWe have the database apress. Therefore, we have to use this database by using the \\nuse command:\\nhive> use apress;\\nHere is the output:\\nOK\\nTime taken: 0.125 seconds\\nAfter using the database, we create a table named filamenttable by using the \\nfollowing command:\\nhive> create table filamenttable (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0filamenttype string,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0bulbpower string,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0>\\xa0\\xa0\\xa0lifeinhours float\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0>)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0> row format delimited\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0> fields terminated by ',';\\nWe have created a a Hive table with three columns. The first column is \\nfilamenttype, with values of the string type. The second column is bulbpower, with the \\ndata type string. The third column is lifeinhours, of the float type. And now we can \\ndisplay our table by using the show command:\\nhive> show tables;\\nHere is the output:\\nOK\\nfilamenttable\\nTime taken: 0.118 seconds, Fetched: 1 row(s)\\nThe required table has been created successfully. Let’s load the data into the table we \\nhave created. We’ll load the data into Hive from a local directory by using load. The local \\nclause in the following command tells Hive that the data is being loaded from a file in a \\nlocal directory, not from HDFS.\\nhive> load data local inpath '/home/pysparkbook/pysparkBookData/\\nfilamentData.csv' overwrite into table filamenttable;\\nHere is the output:\\nLoading data to table apress.filamenttable\\nOK\\nTime taken: 5.39 seconds\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 248}, page_content='Chapter 8 ■ PySparkSQL\\n232\\nAfter the data loads, we can query the table. We can display a row by using select \\nwith limit to limit the number of rows:\\nhive> select * from filamenttable limit 5;\\nOK\\nfilamentA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0605.0\\nfilamentB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0683.0\\nfilamentB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0100W\\xa0\\xa0\\xa0\\xa0691.0\\nfilamentB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0200W\\xa0\\xa0\\xa0\\xa0561.0\\nfilamentA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0200W\\xa0\\xa0\\xa0\\xa0530.0\\nTime\\ntaken: 0.532 seconds, Fetched: 5 row(s)\\nWe have displayed some of the rows of our filamenttable table. We have to read this \\ntable data by using PySparkSQL.\\nHow It Works\\nStep 8-8-1. Creating a HiveContext Object\\nThe HiveContext class has been defined inside the pyspark.sql submodule. We can \\ncreate the HiveContext object by using this class and providing sc as input to the \\nHiveContext constructor:\\n>>> from pyspark.sql import HiveContext\\n>>> ourHiveContext = HiveContext(sc)\\nWe have created the HiveContext object.\\nStep 8-8-2. Reading Table Data from Hive\\nWe can read the table by using the table() function, which is defined on the \\nHiveContext object. In the table() function, we have to provide the name of the table in \\nthe format <databaseName>.<tableName>. In our case, the database name is apress, and \\nthe table name is filamenttable. Therefore, the argument value for the table() function \\nwill be apress.filamenttable.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 249}, page_content=\"Chapter 8 ■ PySparkSQL\\n233\\n>>> FilamentDataFrame = ourHiveContext.table('apress.filamenttable')\\n>>> FilamentDataFrame.show(5)\\n+------------+---------+-----------+\\n|filamenttype|bulbpower|lifeinhours|\\n+------------+---------+-----------+\\n|\\xa0\\xa0\\xa0filamentA|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0605.0|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0683.0|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0100W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0691.0|\\n|\\xa0\\xa0\\xa0filamentB|\\xa0\\xa0\\xa0\\xa0\\xa0200W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0561.0|\\n|\\xa0\\xa0\\xa0filamentA|\\xa0\\xa0\\xa0\\xa0\\xa0200W|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0530.0|\\n+------------+---------+-----------+\\nonly showing top 5 rows\\nAnd, finally, we have created a DataFrame from the table in Apache Hive.\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 250}, page_content='235\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8_9\\nCHAPTER 9\\nPySpark MLlib and Linear \\nRegression\\nMachine learning has gone through many recent developments and is becoming more \\npopular day by day. People from all domains, including computer science, mathematics, \\nand management, are using machine learning in various projects to find hidden \\ninformation in data. Big data becomes more interesting when we start applying machine-\\nlearning algorithms to it.\\nPySpark MLlib is a machine-learning library. It is a wrapper over PySpark Core to \\ndo data analysis using machine-learning algorithms. It works on distributed systems and \\nis scalable. We can find implementations of classification, clustering, linear regression, \\nand other machine-learning algorithms in PySpark MLlib. We know that PySpark is good \\nfor iterative algorithms. Using iterative algorithms, many machine-learning algorithms \\nhave been implemented in PySpark MLlib. Apart from PySpark efficiency and scalability, \\nPySpark MLlib APIs are very user-friendly.\\nSoftware libraries, which are defined to provide solutions for various problems, come \\nwith their own data structures. These data structures are provided to solve a specific set \\nof problems with efficient options. PySpark MLlib comes with many data structures, \\nincluding dense vectors, sparse vectors, and a local and distributed matrix.\\nLinear regression is one of the most popular machine-learning algorithms. We \\ncreate a linear mathematical model between one or more independent variables and one \\ndependent variable.\\nIn this chapter, we will first move through the various data structures defined in \\nPySpark MLlib. Then we will explore linear regression with MLlib. We will also implement \\nlinear regression by using the ridge and lasso methods.\\nThis chapter covers the following recipes:\\nRecipe 9-1. Create a dense vector\\nRecipe 9-2. Create a sparse vector\\nRecipe 9-3. Create local matrices\\nRecipe 9-4. Create a RowMatrix\\nRecipe 9-5. Create a labeled point\\nRecipe 9-6. Apply linear regression'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 251}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n236\\nRecipe 9-7. Apply ridge regression\\nRecipe 9-8. Apply lasso regression\\nRecipe 9-1. Create a Dense Vector\\nProblem\\nYou want to create a dense vector.\\nSolution\\nDenseVector is a local vector. In PySpark, we can create a dense vector by using the \\nDenseVector constructor.\\nHow It Works\\nCreating a dense vector requires us to import the DenseVector class, which has been \\ndefined inside the pyspark.mllib.linalg submodule:\\n>>> from pyspark.mllib.linalg import DenseVector\\n>>> denseDataList = [1.0,3.4,4.5,3.2]\\nWe have created a list of four elements. This list, denseDataList, has floating-point \\ndata elements. We have already imported DenseVector, so we are going to create a dense \\nvector now:\\n>>> denseDataVector = DenseVector(denseDataList)\\n>>> print denseDataVector\\nHere is the output:\\n[1.0,3.4,4.5,3.2]\\nWe have created a dense vector named denseDataVector. We can index our \\nDenseVector elements. The elements of DenseVector are zero-indexed. In the following \\nlines of code, we’ll first fetch the second element of DenseVector, and then we’ll fetch the \\nfirst element with index 0. Last, we’ll fetch the third element of denseDataVector by using \\nthe index 2.\\n>>> denseDataVector[1]\\nHere is the output:\\n3.3999999999999999\\n>>> denseDataVector[0]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 252}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n237\\nHere is the output:\\n1.0\\n>>> denseDataVector[2]\\nHere is the output:\\n4.5\\n■\\n■Note\\u2003  We have already discussed NumPy. It is recommended to use a NumPy array over \\na Python list to create a dense vector, for efficiency. You can read more about DenseVector \\nand SparseVector at https://spark.apache.org/docs/2.0.0-preview/mllib-data-\\ntypes.html.\\nRecipe 9-2. Create a Sparse Vector\\nProblem\\nYou want to create a sparse vector.\\nSolution\\nSometimes we get a dataset in which the maximum value of data elements is 0. Can we \\nescape save zero? Yeah, we can do it. We can save a sparse vector as a SparseVector \\nobject in PySpark MLlib.\\nYou have been given a vector. Many elements of this vector have a value of 0, as you \\ncan see in Table\\xa09-1. The value at indices 1, 2, 3, 4, 5, and 7 is 0.0.\\nSo six out of eight values are 0. We are going to create a SparseVector out of this.\\nTable 9-1.\\u2002 A Sparse Dataset\\nData\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n3.2\\n0.0\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nIndex'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 253}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n238\\nHow It Works\\nIn this section, we are going to create a SparseVector. First, we have to import the \\nSparseVector class from our submodule pyspark.mllib.linalg:\\n>>> from pyspark.mllib.linalg import SparseVector\\n>>> sparseDataList = [1.0,3.2]\\n>>> sparseDataVector = SparseVector(8,[0,7],sparseDataList)\\nWe have created a sparse vector named sparseDataVector. Let me explain the \\narguments of SparseVector. The first argument, 8, indicates that we are have eight \\nelements in our SparseVector. The second element is a list, a list of indices that includes \\na nonzero element in our SparseVector. We have 1.0 at index 0, and 3.2 at index 7.\\nLet’s print our sparseDataVector and see the result:\\n>>> sparseDataVector\\nHere is the output:\\nSparseVector(8, {0: 1.0, 7: 3.2})\\nYou can see that it has our numbers at index 0 and index 7. We can also index our \\nSparseVector by using []:\\n>>> sparseDataVector[1]\\nHere is the output:\\n0.0\\n>>> sparseDataVector[7]\\nHere is the output:\\n3.2000000000000002\\nSo we have found that at index 1, we have 0.0; and at index 7, we have 3.2. The total \\ncount of nonzero elements can be fetched by using the function numNonzeros(). In the \\nfollowing code line, we have found that in our SparseVector, we have only two nonzero \\nelements:\\n>>> sparseDataVector.numNonzeros()\\nHere is the output: \\nsquared_distance( )\\n2'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 254}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n239\\nWe can do many operations on a SparseVector. The following code line shows how \\nto calculate the squared distance between two given SparseVectors. We calculate the \\nsquared distance by using the squared_distance() function:\\n>>> sparseDataList1 = [3.0,1.4,2.5,1.2]\\n>>> sparseDataVector1 = SparseVector(8,[0,3,4,6],sparseDataList1)\\n>>> squaredDistance = sparseDataVector.squared_distance(sparseDataVector1)\\n>>> squaredDistance\\nHere is the output:\\n23.890000000000001\\nWe have the squared distance.\\nRecipe 9-3. Create Local Matrices\\nProblem\\nYou want to create local matrices.\\nSolution\\nA local matrix is stored on a single machine, and, obviously, it is local to that machine. \\nThe indices of a local matrix are of the integer type. And the values of a local matrix are of \\nthe double type. A local matrix comes in two flavors: a dense matrix and a sparse matrix. \\nThe elements of a dense matrix are stored in a single array, in column major order. In \\na sparse matrix, nonzero values are stored in a compressed sparse column format. We \\ncan create a dense matrix by using the dense() method defined in the Matrices class. \\nThe dense() method is a static method, so it is not necessary to create an object of the \\nMatrices class. Similarly, we can create a sparse local matrix by using the sparse() \\nmethod defined in the Matrices class. This method is also a static method.\\nHow It Works\\nFirst, we will create a local dense matrix. Then we will create a local sparse matrix. Let’s \\nfirst create a Python list:\\n>>> denseDataList = [1.0,3.4,4.5,3.2]\\n>>> ourDenseMatrix = localMtrix.dense(numRows = 2, numCols = 2, values= \\ndenseDataList)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 255}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n240\\nWe have created a local dense matrix, ourDenseMatrix. We should discuss the \\narguments of the dense() method. We have provided three arguments to it. The first \\nargument defines the number of rows in our matrix. The second argument is the number \\nof columns. The third argument is a list of matrix elements. Now let’s print the matrix we \\nhave created and see how it looks:\\n>>> ourDenseMatrix\\nHere is the output:\\nDenseMatrix(2, 2, [1.0, 3.4, 4.5, 3.2], False)\\nA new thing appears—False—in the result. What is this? This tells us whether the \\nmatrix is transposed.\\nYou might be wondering how to visualize the structure of the dense matrix that we \\nhave created. The function to help us is toArray(). This will transform our dense matrix \\nto a NumPy array:\\n>>> ourDenseMatrix.toArray()\\nHere is the output:\\narray([[ 1. ,\\xa0\\xa04.5],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[ 3.4,\\xa0\\xa03.2]])\\nWe can see that our data points have been filled into the dense matrix by column. Let \\nme explain The first two values of denseDataList list are filled in first in the dense matrix, \\nin the first column. Then the remaining two elements are placed in the second column.\\nWe create a sparse matrix with the following line of code:\\n>>> sparseDataList = [1.0,3.2]\\nOur sparse matrix has only two nonzero elements. We want to create a matrix of 2 \\n× 2. Therefore, it is obvious that our matrix also will have two 0 elements. We know that \\nPySpark saves Spark local matrices in a compressed sparse column format. Therefore, \\nwe have to provide column pointers as an argument. We want to create a diagonal matrix \\nusing the numbers 1.0 and 3.2:\\n>>> ourSparseMatrix = localMtrix.sparse(numRows = 2,\\xa0\\xa0numCols = 2, colPtrs = \\n[0,1,2],\\xa0\\xa0rowIndices = [0,1], values = sparseDataList)\\n>>> ourSparseMatrix.toArray()\\nHere is the output:\\narray([[ 1. ,\\xa0\\xa00. ],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[ 0. ,\\xa0\\xa03.2]])'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 256}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n241\\n■\\n■Note\\u2003  You can read more about compressed sparse column format on Wikipedia, \\nhttps://en.wikipedia.org/wiki/Sparse_matrix.\\nRecipe 9-4. Create a Row Matrix\\nProblem\\nYou want to create a row matrix.\\nSolution\\nRowMatrix is a distributed matrix. The row indices of RowMatrix are meaningless. It is a \\nrow-oriented distributed matrix.\\nHow It Works\\nThe class RowMatrix is in the PySpark submodule pyspark.mllib.linalg.distributed. \\nLet’s first import this class. Then we will create a RowMatrix.\\n>>> from pyspark.mllib.linalg.distributed import RowMatrix as rm\\nTo create a RowMatrix, we first need an RDD of vectors. But even if you have an RDD \\nof lists, that will work. We have our data in a nested list, dataList:\\n>>> dataList\\xa0\\xa0= [[ 94.88,\\xa0\\xa082.04,\\xa0\\xa052.57],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[ 35.85,\\xa0\\xa026.9 ,\\xa0\\xa0\\xa03.63],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[ 41.76,\\xa0\\xa069.67,\\xa0\\xa050.62],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[ 90.45,\\xa0\\xa054.66,\\xa0\\xa064.07]]\\nThe nested list dataList has four rows and three columns. Now we have to create an \\nRDD out of this dataList:\\n>>> dataListRDD = sc.parallelize(dataList,4)\\n>>> ourRowMatrix = rm(rows = dataListRDD, numRows = 4 , numCols = 3)\\nWe have created our RowMatrix. The first argument of RowMatrix is the RDD of the \\nlist. The second and third arguments are the number of rows and the number of columns, \\nrespectively.\\n>>> ourRowMatrix.numRows()\\nHere is the output:\\n4L\\n>>> ourRowMatrix.numCols()'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 257}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n242\\nHere is the output:\\n3L\\nRecipe 9-5. Create a Labeled Point\\nProblem\\nYou want to create a labeled point.\\nSolution\\nA labeled point is a basic data structure for linear regression and classification algorithms. \\nIt consists of a sparse or dense vector associated with a label. Labels are of the double \\ndata type; therefore, a labeled point can be used in both regression and classification. The \\nLabeledPoint class stays inside the pyspark.mllib.regression submodule. Let’s create \\nLabeledPoint data.\\nHow It Works\\nFirst we define a nested list:\\n>>> from pyspark.mllib.regression import LabeledPoint\\n>>> labeledPointData = [[3.09,1.97,3.73,1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[2.96,2.15,4.16,1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[2.87,1.93,4.39,1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[3.02,1.55,4.43,1],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[1.8,3.65,2.08,2],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[1.36,4.43,1.95,2],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[1.71,4.35,1.94,2],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[1.03,3.75,2.12,2],\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[2.3,3.59,1.99,2]]\\nThe last data point in every list of our nested list labeledPointData is our label. Let’s \\nparallelize the data:\\n>>> labeledPointDataRDD = sc.parallelize(labeledPointData, 4)\\n>>> labeledPointDataRDD.take(4)\\nHere is the output:\\n[[3.09, 1.97, 3.73, 1],\\n\\xa0[2.96, 2.15, 4.16, 1],\\n\\xa0[2.87, 1.93, 4.39, 1],\\n\\xa0[3.02, 1.55, 4.43, 1]]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 258}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n243\\n>>> labeledPointRDD = labeledPointDataRDD.map(lambda data : LabeledPoint \\n(data[3],data[0:3]))\\n>>> labeledPointRDD.take(4)\\nHere is the output:\\n[LabeledPoint(1.0, [3.09,1.97,3.73]),\\n\\xa0LabeledPoint(1.0, [2.96,2.15,4.16]),\\n\\xa0LabeledPoint(1.0, [2.87,1.93,4.39]),\\n\\xa0LabeledPoint(1.0, [3.02,1.55,4.43])]\\nWe have created our labeledPointRDD. You can see that the labeled points have \\nbeen transformed into double data types.\\nhttps://spark.apache.org/docs/2.0.0/mllib-data-types.html\\nRecipe 9-6. Apply Linear Regression\\nProblem\\nYou want to apply linear regression.\\nSolution\\nLinear regression is a supervised machine-learning algorithm. Here we fit a line  \\n(a straight line or a curved line) that generates a linear relationship between dependent \\nand independent variables.\\nWe have been given a file, linearRegressionData.csv. This file consists of four \\ncolumns. If we visualize the file data, it looks as follows:\\n+-----+----+----+----+\\n| dvs1|ivs1|ivs2|ivs3|\\n+-----+----+----+----+\\n|34.63|5.53|5.58|5.41|\\n|40.89|3.89|6.48|6.97|\\n|37.25|5.07| 4.5| 6.5|\\n|45.09|5.81|5.71|8.59|\\n| 39.4|5.61|5.79|6.77|\\n+-----+----+----+----+.\\nThe column dvs1 is the dependent variable, which depends on the three \\nindependent variables ivs1, ivs2, and ivs3. We have to create a mathematical model \\nthat will show a linear relationship between the dependent and independent variables. \\nThe linear regression model’s mathematical formula is depicted in Figure\\xa09-1.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 259}, page_content=\"Chapter 9 ■ PySpark MLlib and Linear Regression\\n244\\nWe have to estimate the value of the intercept and weights.\\nHow It Works\\nLet’s create the model step-by-step.\\nStep 9-6-1. Reading CSV File Data\\nOur regression data is in linearRegressionData.csv. We have to read this data and \\ntransform it into labeled points in order to run linear regression analysis on it. I think \\nthat a better strategy is to transform our data to an RDD of labeled points that can be \\naccomplished by first reading the file by using the spark.read.csv() function. We know \\nthat the spark.read.csv() function will return a DataFrame. We have to transform that \\nDataFrame to an RDD of labeled points somehow. So first let’s read the file.\\n>>> regressionDataFrame = spark.read.csv('file:///home/pysparkbook/bData/\\nlinearRegressionData.csv',header=True, inferSchema = True)\\n>>> regressionDataFrame.show(5)\\nHere is the output, showing only the top five rows:\\n+-----+----+----+----+\\n| dvs1|ivs1|ivs2|ivs3|\\n+-----+----+----+----+\\n|34.63|5.53|5.58|5.41|\\n|40.89|3.89|6.48|6.97|\\n|37.25|5.07| 4.5| 6.5|\\n|45.09|5.81|5.71|8.59|\\n| 39.4|5.61|5.79|6.77|\\n+-----+----+----+----+.\\nAfter reading the file, we have the DataFrame. The following line of code transforms \\nour DataFrame to an RDD:\\n>>> regressionDataRDDDict = regressionDataFrame.rdd\\n>>> regressionDataRDDDict.take(5)\\nFigure 9-1.\\u2002 Mathematical formula for the linear regression model\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 260}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n245\\nHere is the output:\\n[Row(dvs1=34.63, ivs1=5.53, ivs2=5.58, ivs3=5.41),\\n\\xa0Row(dvs1=40.89, ivs1=3.89, ivs2=6.48, ivs3=6.97),\\n\\xa0Row(dvs1=37.25, ivs1=5.07, ivs2=4.5, ivs3=6.5),\\n\\xa0Row(dvs1=45.09, ivs1=5.81, ivs2=5.71, ivs3=8.59),\\n\\xa0Row(dvs1=39.4, ivs1=5.61, ivs2=5.79, ivs3=6.77)]\\nWe have seen the output of regressionDataRDDDict after transformation into an \\nRDD. This is an RDD of row objects. You might be thinking that you know about row \\nobjects. We use these while creating a DataFrame. But this is not our requirement. \\nYou can see that the row objects have data in key/value format. So we need more \\ntransformations to get only the values in our RDD:\\n>>> regressionDataRDD = regressionDataFrame.rdd.map(list)\\n>>> regressionDataRDD.take(5)\\nHere is the output:\\n[[34.63, 5.53, 5.58, 5.41],\\n\\xa0[40.89, 3.89, 6.48, 6.97],\\n\\xa0[37.25, 5.07, 4.5, 6.5],\\n\\xa0[45.09, 5.81, 5.71, 8.59],\\n\\xa0[39.4, 5.61, 5.79, 6.77]]\\nAdding a list as an argument of the RDD map() function has transformed our data \\ninto a format that we can use to easily create our labeled point RDD. In the following step, \\nwe are going to create a LabeledPoint RDD.\\nStep 9-6-2. Creating an RDD of the Labeled Point\\nIn order to run linear regression, we have transformed our data into labeled points.  \\nAs we discussed, the first column of our RDD is a dependent variable, which depends on \\nthe rest of the variables. Therefore, the first value of every RDD element is our label, and \\nthe rest are our features. Now we can create the LabeledPoint RDD. We know that to use \\nLabeledPoint, we have to import the class:\\n>>> from pyspark.mllib.regression import LabeledPoint\\n>>> regressionDataLabelPoint = regressionDataRDD.map(lambda data : LabeledPo\\nint(data[0],data[1:4]))\\nThe map() function can be used to transform our RDD to a LabeledPoint RDD. \\nThe first argument of LabeledPoint is the label, which we provide as data[0] in the \\npreceding code. We are going to take five elements out of the LabeledPoint RDD via \\nregressionDataLabelPoint. Surely, we are going to use the take() function with 5 as an \\nargument to it:\\n>>> regressionDataLabelPoint.take(5)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 261}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n246\\nHere is the output:\\n[LabeledPoint(34.63, [5.53,5.58,5.41]),\\n\\xa0LabeledPoint(40.89, [3.89,6.48,6.97]),\\n\\xa0LabeledPoint(37.25, [5.07,4.5,6.5]),\\n\\xa0LabeledPoint(45.09, [5.81,5.71,8.59]),\\n\\xa0LabeledPoint(39.4, [5.61,5.79,6.77])]\\nWe have created the required LabeledPoint RDD. As we discussed, linear regression \\nis a supervised machine-learning algorithm. Therefore, we first divide our given datasets \\ninto training and testing datasets. The training dataset will be used to create the model, \\nand then we’ll apply the testing data to check the accuracy of the linear regression model \\nwe have created. In the following step, we will divide our dataset into training and testing \\ndatasets.\\nStep 9-6-3. Dividing Training and Testing Data\\nPySpark provides the randomSplit() function, which we can use to divide our datasets \\ninto training and testing datasets:\\n>>> regressionLabelPointSplit = regressionDataLabelPoint.\\nrandomSplit([0.7,0.3])\\nWe providing the list [0.7 , 0.3] as an argument. This list indicates that we need \\n70 percent of our data points in our training dataset, and the rest in our testing dataset. \\nIn our dataset, we have a total of 30 records. Therefore, 22 records will go in the training \\ndataset, and 8 will go in the testing dataset.\\n>>> regressionLabelPointTrainData = regressionLabelPointSplit[0]\\n>>> regressionLabelPointTrainData.take(5)\\nHere is the output:\\n[LabeledPoint(34.63, [5.53,5.58,5.41]),\\n\\xa0LabeledPoint(45.09, [5.81,5.71,8.59]),\\n\\xa0LabeledPoint(39.4, [5.61,5.79,6.77]),\\n\\xa0LabeledPoint(33.25, [5.33,5.78,4.94]),\\n\\xa0LabeledPoint(44.17, [6.11,6.18,8.2])]\\nThe training dataset is ready:\\n>>> regressionLabelPointTrainData.count()\\nHere is the output:\\n22'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 262}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n247\\nAs we discussed, there are 22 data points are in our training dataset:\\n>>> regressionLabelPointTestData = regressionLabelPointSplit[1]\\n>>> regressionLabelPointTestData.take(5)\\nHere is the output:\\n[LabeledPoint(40.89, [3.89,6.48,6.97]),\\n\\xa0LabeledPoint(37.25, [5.07,4.5,6.5]),\\n\\xa0LabeledPoint(42.92, [5.39,6.59,7.56]),\\n\\xa0LabeledPoint(38.23, [4.19,6.47,6.52]),\\n\\xa0LabeledPoint(36.33, [5.65,5.78,5.47])]\\n>>> regressionLabelPointTestData.count()\\nHere is the output:\\n8\\nThe testing dataset has been created as regressionLabelPointTestData. The count() \\nfunction on the test data ensures that we have eight data points in our testing dataset.\\nNow that we have the training and testing datasets, we are ready to fit the regression \\nmodel to our training dataset. In the following step, we are going to create our linear \\nregression model.\\nStep 9-6-4. Creating a Linear Regression Model\\nPySpark uses stochastic gradient descent (SGD) to calculate the coefficients of the \\nlinear regression model. This is an optimization algorithm. The PySpark class \\nLinearRegressionWithSGD is used to do the operation related to linear regression.\\nLet’s import LinearRegressionWithSGD; we’ll import it as lrSGD:\\n>>> from pyspark.mllib.regression import LinearRegressionWithSGD as lrSGD\\n>>> ourModelWithLinearRegression\\xa0\\xa0= lrSGD.train(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = regressionLabelPointTrainData,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0iterations = 200,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0step = 0.02,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0intercept = True)\\nThere is a static method, train(), defined in the LinearRegressionWithSGD class. \\nThe train() method is used to create a linear regression model. The first argument of \\nthe train() method is the data that is our training data. SGD is an iterative algorithm, so \\nwe provide the number of iterations as the second argument to our train() method. The \\nthird parameter, step, defines the size of the movement in the SGD algorithm. If a linear \\nmodel has an intercept, we have to set that intercept to True. Finally, this creates our \\nlinear regression model named ourModelWithLinearRegression.\\n>>> ourModelWithLinearRegression.intercept'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 263}, page_content=\"Chapter 9 ■ PySpark MLlib and Linear Regression\\n248\\nHere is the output:\\n1.3475409224629387\\n>>> ourModelWithLinearRegression.weights\\nHere is the output:\\nDenseVector([1.7083, 2.1529, 2.3226])\\nWe can create our regression model by using the intercept and weights. The created \\nmodel is shown in in Figure\\xa09-2.\\n■\\n■Note\\u2003  You can read more about stochastic gradient descent on Wikipedia, https://\\nen.wikipedia.org/wiki/Stochastic_gradient_descent.\\nStep 9-6-5. Saving the Created Model\\nSometimes we have to save the created model and use it in the future. We can save our \\nmodel by using the save() method:\\n>>> ourModelWithLinearRegression.save(sc, '/home/pysparkbook/\\nourModelWithLinearRegression')\\nThe first argument of the save() method is SparkContext. The second argument is \\nthe path of the directory where you want to save your model. We have saved our model, \\nbut how do we read it? Reading the saved model requires the load() method. This \\nmethod is inside the LinearRegressionModel class.\\n>>> from pyspark.mllib.regression import LinearRegressionModel as \\nlinearRegressModel\\n>>> ourModelWithLinearRegressionReloaded = linearRegressModel.load(sc, '/\\nhome/pysparkbook/ourModelWithLinearRegression')\\nThe saved model has been reloaded, so now we can use it. Since we have reloaded \\nthe model, let’s check whether we can get the intercept and weight values from the \\nreloaded model:\\n>>> ourModelWithLinearRegressionReloaded.intercept\\nFigure 9-2.\\u2002 Our regression model\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 264}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n249\\nHere is the output:\\n1.34754092246\\n>>> ourModelWithLinearRegressionReloaded.weights\\nHere is the output:\\nDenseVector([1.7083, 2.1529, 2.3226])\\nIt is clear that we have reloaded the model.\\nStep 9-6-6. Predicting Data by Using the Model\\nWhenever any mathematical model is created, we can check the credibility of the model. \\nTo check our model’s credibility, we will make an RDD of the actual and predicted data of \\nour test data:\\n>>> actualDataandLinearRegressionPredictedData = regressionLabelPointTest \\nData.map(lambda data : (float(data.label) , float(ourModelWithLinearRegressi\\non.predict(data.features))))\\nThe predict() method will take the features data (which are independent variables) \\nand return the predicted values for the dependent variable. Now we’ll have an RDD of the \\nactual and predicted data.\\n>>> actualDataandLinearRegressionPredictedData.take(5)\\nHere is the output:\\n[(40.89, 38.1322613341641),\\n\\xa0(37.25, 34.79375295252528),\\n\\xa0(42.92, 42.30191258048799),\\n\\xa0(38.23, 37.57804867136557),\\n\\xa0(36.33, 36.14796052442989)]\\nStep 9-6-7. Evaluating the Model We Created\\nAfter getting the RDD of actual and predicted data, we will calculate evaluation metrics. \\nWe will first calculate the root-mean-square error. The mathematical formula for a root-\\nmean-square error is given in Figure\\xa09-3.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 265}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n250\\nWe have to import the RegressionMetrics class to evaluate the model:\\n>>> from pyspark.mllib.evaluation import RegressionMetrics as rmtrcs\\n>>> ourLinearRegressionModelMetrics = rmtrcs(actualDataandLinearRegressionP\\nredictedData)\\n>>> ourLinearRegressionModelMetrics.rootMeanSquaredError\\nHere is the output:\\n1.8446573587605941\\nThe value of our root-mean-square error is 1.844657. Similarly, we calculate the \\nvalue of R2:\\n>>> ourLinearRegressionModelMetrics.r2\\nHere is the output:\\n0.47423120771913974\\nThe value of R2 is less, even less than 0.5. So is this a good model? I say no. So another \\nquestion is, can we improve the efficiency of our model? And the answer is yes; I have \\ndone it just by playing with the learning step size and number of iterations:\\n>>> ourModelWithLinearRegression\\xa0\\xa0= \\x07lrSGD.train(data = regressionLabelPoint \\nTrainData,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0iterations = 100,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0step = 0.05,\\n...\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0intercept = True)\\n>>> actualDataandLinearRegressionPredictedData = \\nregressionLabelPointTestData.map(lambda data : (float(data.label) ,  \\nfloat(ourModelWithLinearRegression.predict(data.features))))\\nFigure 9-3.\\u2002 Mathematical formula for a root-mean-square error\\ndvs1 is the actual value of the dependent variable and dvs1pred is the predicted \\nvalue of the dependent variable. We have n data points.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 266}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n251\\n>>> from pyspark.mllib.evaluation import RegressionMetrics as rmtrcs\\n>>> ourLinearRegressionModelMetrics = rmtrcs(actualDataandLinearRegressionP\\nredictedData)\\n>>> ourLinearRegressionModelMetrics.rootMeanSquaredError\\nHere is the output:\\n1.7856232547826518\\n>>> ourLinearRegressionModelMetrics.r2\\nHere is the output:\\n0.6377723547885376\\nSo, finally, we have increased the value of R2.\\n■\\n■Note\\u2003  You can read more about R2 at https://en.wikipedia.org/wiki/\\nCoefficient_of_determination.\\nRecipe 9-7. Apply Ridge Regression\\nProblem\\nYou want to apply ridge regression.\\nSolution\\nYou have been given a dataset in the CSV file autoMPGDataModified.csv. This dataset \\nhas five columns. We have to fit a linear regression model to this data by using ridge \\nregularization. The first column is miles per gallon, which is the dependent variable in \\nthis case.\\n+----+------------+----------+------+------------+\\n| mpg|displacement|horsepower|weight|acceleration|\\n+----+------------+----------+------+------------+\\n|18.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0307.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa018|\\xa0\\xa03504|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa012.0|\\n|15.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0350.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa036|\\xa0\\xa03693|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa011.5|\\n|18.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0318.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa030|\\xa0\\xa03436|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa011.0|\\n|16.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0304.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa030|\\xa0\\xa03433|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa012.0|\\n|17.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0302.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa025|\\xa0\\xa03449|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa010.5|\\n+----+------------+----------+------+------------+'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 267}, page_content=\"Chapter 9 ■ PySpark MLlib and Linear Regression\\n252\\nI have taken this dataset from the UCI Machine Learning Repository (https://\\narchive.ics.uci.edu/ml/datasets/auto+mpg) and removed some columns. According \\nto the web page, the dataset was taken from the StatLib library maintained at Carnegie \\nMellon University and was used in the 1983 American Statistical Association Exposition.\\nYou might be wondering about the difference between linear regression and linear \\nregression with the ridge parameter. We know that we do optimization of error part using \\nSGD. So in the error part, an extra term is added, as shown in Figure\\xa09-4.\\nLet’s perform ridge regression on the given dataset.\\n■\\n■Note\\u2003  You can read more about the auto-mpg data on the following sites:\\nhttps://archive.ics.uci.edu/ml/datasets/auto+mpg\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/\\nYou can read more about ridge regression on the following sites:\\nwww.quora.com/Why-is-it-that-the-lasso-unlike-ridge-regression-results-in-\\ncoefficient-estimates-that-are-exactly-equal-to-zero\\nhttps://en.wikipedia.org/wiki/Tikhonov_regularization\\nhttps://en.wikipedia.org/wiki/Regularization_(mathematics)\\nHow It Works\\nStep 9-7-1. Reading the CSV File Data\\nWe have to read the data and transform it to RDD, as we have done in previous recipes:\\n>>> autoDataFrame = spark.read.csv('file:///home/pysparkbook/bData/\\nautoMPGDataModified.csv',header=True, inferSchema = True)\\n>>> autoDataFrame.show(5)\\nFigure 9-4.\\u2002 Extra error term in ridge regression\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 268}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n253\\nHere is the output, showing only the top five rows:\\n+----+------------+----------+------+------------+\\n| mpg|displacement|horsepower|weight|acceleration|\\n+----+------------+----------+------+------------+\\n|18.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0307.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa018|\\xa0\\xa03504|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa012.0|\\n|15.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0350.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa036|\\xa0\\xa03693|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa011.5|\\n|18.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0318.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa030|\\xa0\\xa03436|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa011.0|\\n|16.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0304.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa030|\\xa0\\xa03433|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa012.0|\\n|17.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0302.0|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa025|\\xa0\\xa03449|\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa010.5|\\n+----+------------+----------+------+------------+\\n>>> autoDataFrame.printSchema()\\nHere is the output:\\nroot\\n\\xa0|-- mpg: double (nullable = true)\\n\\xa0|-- displacement: double (nullable = true)\\n\\xa0|-- horsepower: integer (nullable = true)\\n\\xa0|-- weight: integer (nullable = true)\\n\\xa0|-- acceleration: double (nullable = true)\\n>>> autoDataRDDDict = autoDataFrame.rdd\\n>>> autoDataRDDDict.take(5)\\nHere is the output:\\n[Row(mpg=18.0, displacement=307.0, horsepower=18, weight=3504, \\nacceleration=12.0),\\nRow(mpg=15.0, displacement=350.0, horsepower=36, weight=3693, \\nacceleration=11.5),\\nRow(mpg=18.0, displacement=318.0, horsepower=30, weight=3436, \\nacceleration=11.0),\\nRow(mpg=16.0, displacement=304.0, horsepower=30, weight=3433, \\nacceleration=12.0),\\nRow(mpg=17.0, displacement=302.0, horsepower=25, weight=3449, \\nacceleration=10.5)]\\nWe transform our DataFrame to an RDD so that we can transform it further into the \\nLabeledPoint RDD:\\n>>> autoDataRDD = autoDataFrame.rdd.map(list)\\n>>> autoDataRDD.take(5)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 269}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n254\\nHere is the output:\\n[[18.0, 307.0, 18, 3504, 12.0],\\n\\xa0[15.0, 350.0, 36, 3693, 11.5],\\n\\xa0[18.0, 318.0, 30, 3436, 11.0],\\n\\xa0[16.0, 304.0, 30, 3433, 12.0],\\n\\xa0[17.0, 302.0, 25, 3449, 10.5]]\\nStep 9-7-2. Creating an RDD of the Labeled Points\\nAfter getting the RDD, we have to transform the RDD to the LabeledPoint RDD:\\n>>> from pyspark.mllib.regression import LabeledPoint\\n>>> autoDataLabelPoint = autoDataRDD.map(lambda data : LabeledPoint(data[0],\\n[data[1]/10,data[2],float(data[3])/100,data[4]]))\\nIn the dataset, we can see that it is better to normalize the data. Therefore, we divide \\nthe displacement by 10 and the weight by 100:\\n>>> autoDataLabelPoint.take(5)\\nHere is the output:\\n[LabeledPoint(18.0, [30.7,18.0,35.04,12.0]),\\n\\xa0LabeledPoint(15.0, [35.0,36.0,36.93,11.5]),\\n\\xa0LabeledPoint(18.0, [31.8,30.0,34.36,11.0]),\\n\\xa0LabeledPoint(16.0, [30.4,30.0,34.33,12.0]),\\n\\xa0LabeledPoint(17.0, [30.2,25.0,34.49,10.5])]\\nStep 9-7-3. Dividing Training and Testing Data\\nIt is time to divide our dataset into training and testing datasets:\\n>>> autoDataLabelPointSplit = autoDataLabelPoint.randomSplit([0.7,0.3])\\n>>> autoDataLabelPointTrain = autoDataLabelPointSplit[0]\\n>>> autoDataLabelPointTest = autoDataLabelPointSplit[1]\\n>>> autoDataLabelPointTrain.take(5)\\nHere is the output:\\n[LabeledPoint(18.0, [30.7,18.0,35.04,12.0]),\\n\\xa0LabeledPoint(15.0, [35.0,36.0,36.93,11.5]),\\n\\xa0LabeledPoint(18.0, [31.8,30.0,34.36,11.0]),\\n\\xa0LabeledPoint(16.0, [30.4,30.0,34.33,12.0]),\\n\\xa0LabeledPoint(17.0, [30.2,25.0,34.49,10.5])]\\n>>> autoDataLabelPointTest.take(5)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 270}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n255\\nHere is the output:\\n[LabeledPoint(14.0, [45.5,48.0,44.25,10.0]),\\n\\xa0LabeledPoint(15.0, [39.0,41.0,38.5,8.5]),\\n\\xa0LabeledPoint(15.0, [40.0,30.0,37.61,9.5]),\\n\\xa0LabeledPoint(24.0, [11.3,92.0,23.72,15.0]),\\n\\xa0LabeledPoint(26.0, [9.7,51.0,18.35,20.5])]\\n>>> autoDataLabelPointTest.count()\\nHere is the output:\\n122\\n>>> autoDataLabelPointTrain.count()\\nHere is the output:\\n269\\nStep 9-7-4. Creating a Linear Regression Model\\nWe can create our model by using the train() method of the RidgeRegressionWithSGD \\nclass. Therefore, we first have to import the RidgeRegressionWithSGD class and then run \\nthe train() method:\\n>>> from pyspark.mllib.regression import RidgeRegressionWithSGD\\xa0\\xa0as ridgeSGD\\n>>> ourModelWithRidge\\xa0\\xa0= ridgeSGD.train(\\ndata = autoDataLabelPointTrain,\\niterations = 400,\\nstep = 0.0005,\\nregParam = 0.05,\\nintercept = True\\n)\\nIn our train() method, there is one more argument, regParam, than in our previous \\nrecipe. The regParam argument is a regularization parameter, alpha, shown previously in \\nFigure\\xa09-4.\\n>>> ourModelWithRidge.intercept\\nHere is the output:\\n1.0192595005891258\\n>>> ourModelWithRidge.weights'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 271}, page_content=\"Chapter 9 ■ PySpark MLlib and Linear Regression\\n256\\nHere is the output:\\nDenseVector([-0.0575, 0.2025, 0.1961, 0.3503])\\nWe have created our model and have the intercept and coefficients.\\nStep 9-7-5. Saving the Created Model\\nWe can save our model and reload it as we did in the previous recipe. The following code \\nfirst saves the model and then reloads it. After reloading the model, we will check whether \\nit is working correctly.\\n>>> ourModelWithRidge.save(sc, '/home/pysparkbook/ourModelWithRidge')\\n>>> from\\xa0\\xa0pyspark.mllib.regression import RidgeRegressionModel as \\nridgeRegModel\\n>>> ourModelWithRidgeReloaded = ridgeRegModel.load(sc, '/home/pysparkbook/\\nourModelWithRidge')\\n>>> ourModelWithRidgeReloaded.intercept\\nHere is the output:\\n1.01925950059\\n>>> ourModelWithRidgeReloaded.weights\\nHere is the output:\\nDenseVector([-0.0575, 0.2025, 0.1961, 0.3503])\\nOur saved model is working correctly.\\nStep 9-7-6. Predicting the Data by Using the Model\\nIn this step, we are going to create an RDD of actual and predicted data. The predicted \\ndata will be calculated by using the predict() function.\\n>>> actualDataandRidgePredictedData = autoDataLabelPointTest.map(lambda data \\n: [float(data.label) , float(ourModelWithRidge.predict(data.features))])\\n>>> actualDataandRidgePredictedData.take(5)\"),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 272}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n257\\nHere is the output:\\n[[18.0, 15.857286660271024],\\n\\xa0[16.0, 16.28216643081738],\\n\\xa0[17.0, 14.787196092732607],\\n\\xa0[15.0, 17.60672713589945],\\n\\xa0[14.0, 17.67800889949583]]\\nStep 9-7-7. Evaluating the Model We Have Created\\nWe have to again find the root-mean-square error:\\n>>> ourRidgeModelMetrics = rmtrcs(actualDataandRidgePredictedData)\\n>>> ourRidgeModelMetrics.rootMeanSquaredError\\nHere is the output:\\n8.149263319131556\\nThis is the error value. The higher the value, the less accurate the model is. We have \\ncalculated the root-mean-square error, and we have checked the credibility of the model.\\nRecipe 9-8. Apply Lasso Regression\\nProblem\\nYou want to apply lasso regression.\\nSolution\\nLinear regression with lasso regularization is used for models that are not properly fitted. In \\nthe case of lasso, at the time of error minimization, we add the term shown in Figure\\xa09-5.\\nFigure 9-5.\\u2002 Extra error term in lasso regression'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 273}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n258\\nHow It Works\\nStep 9-8-1. Creating a Linear Regression Model with Lasso\\nWe have already created LabeledPoint, containing auto data. We can apply the train() \\nmethod defined in the LassoWithSGD class:\\n>>> from pyspark.mllib.regression import LassoWithSGD\\xa0\\xa0as lassoSGD\\n>>> ourModelWithLasso\\xa0\\xa0= lassoSGD.train(data = autoDataLabelPointTrain, \\niterations = 400, step = 0.0005,regParam = 0.05, intercept = True)\\nWe have created our model.\\n>>> ourModelWithLasso.intercept\\nHere is the output:\\n1.020329086499831\\n>>> ourModelWithLasso.weights\\nHere is the output:\\nDenseVector([-0.063, 0.2046, 0.198, 0.3719])\\nWe have the intercept and weight of the model.\\nStep 9-8-2. Predicting the Data Using the Lasso Model\\nIn order to get the RDD of actual and predicted data, we are going to use the same \\nstrategy used in previous recipes:\\n>>> actualDataandLassoPredictedData = autoDataLabelPointTest.map(lambda data \\n: (float(data.label) , float(ourModelWithLasso.predict(data.features))))\\n>>> actualDataandLassoPredictedData.take(5)\\nHere is the output:\\n[(15.0, 17.768596038896607),\\n\\xa0(16.0, 16.5021818747879),\\n\\xa0(17.0, 14.965800201626084),\\n\\xa0(15.0, 17.734571412337576),\\n\\xa0(15.0, 17.154509770352835)]'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 274}, page_content='Chapter 9 ■ PySpark MLlib and Linear Regression\\n259\\nStep 9-8-3. Evaluating the Model We Have Created\\nNow we have to test the model—and though there’s no need to say it, we are going to use \\nthe same strategy as before:\\n>>> from pyspark.mllib.evaluation import RegressionMetrics as rmtrcs\\n>>> ourLassoModelMetrics = rmtrcs(actualDataandLassoPredictedData)\\n>>> ourLassoModelMetrics.rootMeanSquaredError\\nHere is the output:\\n7.030519540791776\\nWe have found the root-mean-square error.'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 275}, page_content='261\\n© Raju Kumar Mishra 2018 \\nR. K. Mishra, PySpark Recipes, https://doi.org/10.1007/978-1-4842-3141-8\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 A\\nAnonymous function, 66\\nApache Hadoop, 1\\nApache HBase, 42–44\\nApache Hive, 6–7, 230\\nApache Kafka, 8, 178\\nApache License, 7\\nApache Mahout, 5\\nApache Mesos, 38–42\\nApache Pig, 7\\nApache Spark, 9\\nApache Storm, 2\\nApache Tez, 2\\nAtomicity, Consistency, Isolation, and \\nDurability (ACID), 12\\navg() function, 209\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 B\\nbfs() function, 225\\nBig data\\ncharacteristics, 2\\nvariety, 3\\nvelocity, 3\\nveracity, 3\\nvolume, 2\\nBreadth-first search algorithm, 220, 225\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 C\\nCentOS operating system, 15\\nCluster managers, 10–11\\ncount() function, 140, 198, 247\\nCount of records, 195\\ncreateCSV() function, 152\\ncreateDataFrame() function, 191\\ncreateJSON() function, 157, 158\\ncreateOrReplaceTempView()  \\nfunction, 207\\ncreateStream() function, 181\\nCSV file\\nreading, 150\\npaired RDD, 152\\nparseCSV() function, 151\\nwriting RDD to, 152\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 D\\nData aggregation, 200\\nfilament data, 119\\nmean, 121–123, 125–126\\npaired RDD, 121\\nRDD, 120\\nDataFrame, 188\\nchanging data type of column, 192\\ncompound logical expression, 194\\ncreation, 191, 196\\ndata aggregation, 200\\ndata joining, 210\\nfull outer join, 220\\ninner join, 215\\nleft outer join, 217\\nreading student data table, \\nPostgreSQL database, 212\\nreading subject data, JSON file, 215\\nright outer join, 219\\nexploratory data analysis, 195\\nfilament data nested list creation, 188\\nfilter() and count() functions, 193, 198\\nRDD of row objects, creation, 190\\nschema creation, 189\\nschema definition, 196\\nschema printing, 192\\nIndex'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 276}, page_content='■ INDEX\\n262\\nSQL and HiveQL queries,  \\nexecution of, 207\\nsummary statistics, 197\\nDataFrame abstraction, 187\\nData joining, 210\\nfull outer join, 220\\ninner join, 215\\nleft outer join, 217\\nreading student data table, \\nPostgreSQL database, 212\\nreading subject data, JSON file, 215\\nright outer join, 219\\nDataNodes, 4\\nDataset interface, 187\\nData structure, labeled point, 242\\nDense vector creation, 236\\ndescribe() function, 197\\nDistributed systems, 1\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 E\\nE-commerce companies, 1\\nExtract, transform, and load (ETL), 7\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 F\\nfilter() function, 193, 198\\nFull outer join, 220\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 G\\nGoogle file system (GFS), 4\\nGraphFrames library, 10, 187\\nGraphFrames object creation, 224\\ngroupBy() function, 200\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 H\\nHadoop distributed file system  \\n(HDFS), 4–5, 15\\nreading data from, 145\\nsaving RDD data to, 146\\nHadoop installation\\n.bashrc file, 21\\nCentOS User, 16–17\\ndownloading, 19\\nenvironment file, 20\\ninstallation directory, 19–20\\nJava, 17\\njps command, 23\\nNameNode format, 22\\npasswordless login, 18–19\\nproblem, 16\\nproperties files, 20–21\\nsolution, 16\\nstarting script, 22\\nHBase, 2, 12–14\\nHive installation, 27–29\\nHive property, 37\\nHiveQL and SQL queries, execution of, 207\\nHiveQL commands, 7\\nHive query language (HQL), 6\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 I\\nInner join, 215\\nI/O operations. See PySpark, input/output \\n(I/O) operations\\nIPython\\nintegration, 79\\nNotebook, 81–83\\npip, 80\\nPySpark, 81\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 J\\nJava database connectivity (JDBC), 12\\nJavaScript object notation (JSON)\\nreading file, 154\\nreading subject data from, 215\\nwriting RDD to file, 156\\njsonParse() function, 155–156\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 K\\nK-nearest neighbors (KNN) algorithm, \\nPySpark, 166\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 L\\nLabeled point, 242, 245, 254\\nLasso regression, 257\\nLeft outer join, 217\\nLen() function, 140\\nLinear regression, 235, 243\\nLocal matrix creation, 239\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 M\\nMachine learning, 235\\nmap() function, 154, 190, 245\\nDataFrame (cont.)'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 277}, page_content='■ INDEX\\n263\\nMap-reduce model, 5\\nMatrices\\nlocal matrix creation, 239\\nrow matrix creation, 241\\nMLlib, 10\\nMutable collection, 56\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 N, O\\nNameNode, 4\\nnc command, 175\\nNetcat, 174\\nnewAPIHadoopRDD() function,  \\n159–160\\nNoSQL databases, 2, 15\\nNumPy\\narray(), 73\\ndtype, 74–75\\nmean, 78\\nmean temperature, 77\\nmedians, 78\\nmin() and max(), 76\\nndarray, 72\\npip, 72\\nshape, 75\\nstandard deviation, 77\\ntemperature readings, 71\\nvariance, 77–78\\nvstack(), 73–74\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 P, Q\\nPage-rank algorithm, 226\\ndamping factor, 133\\nfunction, 134\\nloop, 135\\nnested lists, 134\\noptimization, 164\\npaired RDDs, 135\\nweb-page system, 132\\nPaired RDD\\naggregate data (see Data aggregation)\\ncreation\\nconsonants, 117\\nelements, 116–117\\nkeys(), 118\\nmap(), 116, 118\\nvalues, 118\\njoin data\\ncreation, 128–129\\nfull outer, 131\\ninner, 129\\nleft outer, 130\\nnested list, 128\\nright outer, 131\\nkey/value-pair architecture, 115\\npage rank (see Page-rank algorithm)\\nplayDataLineLength RDD, 142\\nPostgreSQL database, 12, 30–35,  \\n37, 212\\npredict() function, 256\\nprintSchema() function, 192\\nProcedural language/PostgreSQL  \\n(PL/pgSQL), 12\\nPySpark, 15, 37\\nk-nearest neighbors (KNN)  \\nalgorithm, 166\\npage-rank algorithm optimization, \\n164\\nscript execution\\nin local mode, 182\\nStandalone and Mesos cluster \\nmanagers, 184\\nPySpark, input/output (I/O) operations\\nreading CSV file, 150\\npaired RDD, 152\\nparseCSV() function, 151\\nreading data\\nHDFS, 145\\nsequential file, 147\\nreading directory, 143\\ntextFile() function, 144\\nwholeTextFiles() function, 144\\nreading JSON file, 154\\nreading table data, HBase, 159\\nreading text file\\ncount() function, 140\\nLen() function, 140\\ntextFile() function, 138\\nwholeTextFiles() function, 139\\nsaving RDD data to HDFS, 146\\nwriting data to sequential file, 148\\nwriting RDD\\nCSV file, 152\\nJSON file, 156\\ntext file, 141\\nPySpark MLlib, 235\\ndense vector creation, 236\\nlabeled point creation, 242\\nlocal matrix creation, 239\\nrow matrix creation, 241\\nsparse vector creation, 237'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 278}, page_content='■ INDEX\\n264\\nPySparkSQL, 7, 9\\nbreadth-first search algorithm,  \\n220, 225\\nDataFrame, 188\\nchanging data type of column, 192\\ncompound logical expression, 194\\ncreation, 191, 196\\ndata aggregation, 200\\ndata joining, 210\\nexploratory data analysis, 195\\nfilament data nested list  \\ncreation, 188\\nfilter() and count()  \\nfunctions, 193, 198\\nschema creation, 189\\nschema definition, 196\\nschema printing, 192\\nSQL and HiveQL queries, \\nexecution of, 207\\nsummary statistics, 197\\nRDD of row objects, creation, 190\\nGraphFrames object creation, 224\\npage-rank algorithm, 226\\nreading table data, Apache Hive, 230\\nPySpark shell\\nproblem, 25\\nPython programmers, 26\\nsolution, 25\\nPySpark streaming, 163\\nintegration, Apache Kafka, 178\\nreading data, console, 174\\nPython\\nconditionals, 67–68\\ndata and data type, 46–48\\ndictionary, 62–64\\nfor and while loops, 69–70\\nfunctions, 65\\nlambda function, 66–67\\nlist, 54–58\\nNumPy (see NumPy)\\nset, 60–61\\nstring, 48–51\\ntuple, 58–60\\ntypecasting, 51–53\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 R\\nrandomSplit() function, 246\\nregisterTempTable() function, 207\\nRegression\\nlasso, 257\\nlinear, 243\\nridge, 251\\nRelational database management system \\n(RDBMS), 2, 6, 15\\nResilient distributed dataset (RDD)\\naction, 87–88\\ncreation\\nfirst(), 90\\ngetNumPartitions(), 91\\nlist, 89\\nparallelized(), 89\\ntake(), 90\\ndata manipulation\\ncollect(), 98\\nfilter(), 98\\nlist, 95\\nmap(), 95–96\\nsortBy(), 97\\ntake(), 96\\nMesos cluster manager, 113–114\\nrun set operations, 99–103\\nSparkContext, 86\\nStandalone Cluster Manager, 109–113\\nsummary statistics, 103–108\\ntemperature data, 91–94\\ntransformation, 87–88\\nRidge regression, 251\\nRight outer join, 219\\nround() function, 209\\nRow matrix creation, 241\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 S\\nsave() method, 248\\nsaveAsTextFile() function, 141\\nselect command, 208\\nsequenceFile() function, 148\\nsequenceFile() method, 148\\nSequential file\\nreading data from, 147\\nwriting data to, 148\\nshow() function, 191, 209, 215\\nShuffling, 163\\nsocketTextStream() function, 175–176\\nSoftware libraries, 235\\nSpark, 163\\nSpark architecture\\ndriver, 86\\nexecutors, 86'),\n",
       " Document(metadata={'producer': 'www.allitebooks.com', 'creator': 'www.allitebooks.com', 'creationdate': '2017-11-30T19:37:38+05:30', 'source': '..\\\\data\\\\pdf\\\\sample.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sample.pdf', 'total_pages': 280, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': 'www.allitebooks.com', 'keywords': '', 'moddate': '2017-12-11T10:32:49-08:00', 'trapped': '', 'modDate': \"D:20171211103249-08'00'\", 'creationDate': \"D:20171130193738+05'30'\", 'page': 279}, page_content='■ INDEX\\n265\\nSpark installation\\nallPySpark location, 24\\n.bashrc File, 24\\ndownloading, 23\\nenvironment file, 24\\nproblem, 23\\nPySpark, 25\\nsolution, 23\\n.tgz file, 23\\nspark.read.csv() function, 244\\nspark.sql() function, 208\\nSparse vector creation, 237\\nsplit() function, 176\\nSQL and HiveQL queries, execution of, 207\\nStochastic gradient descent (SGD), 247\\nstringToNumberSum() function, 176\\nstrip() function, 176\\nStructField(), 189\\nStructType() function, 223\\nStructured query language (SQL), 6\\nsummary() function, 195\\nSupervised machine-learning  \\nalgorithm, 243\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 T\\nTable joining, 210\\ntake() function, 245\\ntextFile() function, 138, 143–144\\ntrain() method, 247\\ntype() function, 208\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 U\\nUnix, 4\\nUser-defined functions (UDFs), 7\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 V\\nVectors\\ndense vector, 236\\nsparse vector, 237\\n\\x03\\n\\u200a\\n\\u200a\\n\\u200a\\n\\x84 W, X, Y, Z\\nwholeTextFiles() function,  \\n139, 143–144')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###directory loader for pdf\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",###To match files\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a9570",
   "metadata": {},
   "source": [
    "code is till here \n",
    "later delete the below part\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b708681",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e88878",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01df08c",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd9765d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer ###embiding by hugging_face\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid ### for vectordb id \n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d395643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding model:all-MiniLM-L6-v2\n",
      "Model Loaded successfully.\n",
      "embedding diamension:384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x27f0c8baad0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\" \n",
    "        Initialize the embading manager\n",
    "        \"\"\"\n",
    "        self.model_name=model_name\n",
    "        self.model =None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print (f\"loading embedding model:{self.model_name}\")\n",
    "            self.model= SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded successfully.\\nembedding diamension:{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading model{self.model_name}:{e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self,texts:List[str])->np.ndarray:\n",
    "        \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print(f\"Generating embedding for {len(texts)} texts...\")\n",
    "        embeddings=self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape:{embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "\n",
    "### intialize embedding manager\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bd9fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store initialized.\n",
      "Collection:pdf_documents\n",
      "existing document in colletion:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x27f0f6865d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###vector storage\n",
    "class VectorStore:\n",
    "\n",
    "    def __init__(self,collection_name:str=\"pdf_documents\",persist_directory: str=\"../data/vector_store\"):\n",
    "        self.collection_name=collection_name \n",
    "        self.persist_directory=persist_directory\n",
    "        self.client =None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "        \n",
    "    def _initialize_store(self):\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF document embedding for RAG\"}\n",
    "            )\n",
    "            print(f\"vector store initialized.\\nCollection:{self.collection_name}\")\n",
    "            print(f\"existing document in colletion:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store:{e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"adding{len(documents)}document to vector store...\")\n",
    "\n",
    "        ###prepare data for chromadb\n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_text=[]\n",
    "        embedding_list=[]\n",
    "\n",
    "        for i, (doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            ### prepare meta data\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index']=i\n",
    "            metadata['content_length']=len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            ###document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            ###embedding\n",
    "            embedding_list.append(embedding.tolist())\n",
    "\n",
    "        ###add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embedding_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error adding documents to vector store:{e}\")\n",
    "            raise\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag 1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
